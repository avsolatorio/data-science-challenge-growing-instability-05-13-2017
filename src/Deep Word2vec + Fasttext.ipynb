{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from growing_instability_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('../data/sampleSubmission.csv')\n",
    "topics = sorted(set(sample_sub.columns.difference(['id'])))\n",
    "\n",
    "topic2actual = {}\n",
    "for i in sample_sub.columns:\n",
    "    if 'id' == i:\n",
    "        continue\n",
    "    topic2actual[i] = segment(i)\n",
    "    \n",
    "target_columns = sorted(topics)\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.73 s, sys: 1.4 s, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'wvec_trainingX')\n",
    "# fvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'fvec_trainingX')\n",
    "\n",
    "wvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_wvec_trainingX')\n",
    "fvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_fvec_trainingX')\n",
    "\n",
    "word2idx_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'word2idx_trainingX')\n",
    "_word2idx = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', '_word2idx')\n",
    "trainingY = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'trainingY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "\n",
    "top_tokens = Counter()\n",
    "for i in word2idx_trainingX:\n",
    "    top_tokens.update(set(i[:maxlen]))\n",
    "\n",
    "top_tokens = pd.DataFrame(top_tokens.most_common(), columns=['token', 'freq'])\n",
    "top_doc_tokens = top_tokens[top_tokens.freq > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_token2ind = {}\n",
    "for i, j in enumerate(top_doc_tokens.token):\n",
    "    top_token2ind[j] = i + 1  # Add 1 to start with 1 since 0 is special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 s, sys: 140 ms, total: 28 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2ind = top_token2ind\n",
    "\n",
    "ind2class = dict(enumerate(topics))\n",
    "class2ind = {j: i for i, j in ind2class.items()}\n",
    "\n",
    "num_samples = trainingY.shape[0]\n",
    "\n",
    "training_X = word2idx_trainingX.head(num_samples)\n",
    "\n",
    "training_Y = pd.DataFrame(zip(*np.where(trainingY.head(num_samples) == 1)), columns=['iloc', 'topics'])\n",
    "training_WV = wvec_trainingX.head(num_samples)\n",
    "training_FS = fvec_trainingX.head(num_samples)\n",
    "\n",
    "training_Y = training_Y.groupby('iloc')['topics'].apply(list)\n",
    "training_Y.index = trainingY.head(num_samples).index\n",
    "\n",
    "# indices = sorted(training_Y.index.copy())\n",
    "indices = sorted(training_Y.index[training_Y.index.str.contains('^201[0-9]')])\n",
    "# np.random.shuffle(indices)\n",
    "indices = pd.Index(indices)\n",
    "\n",
    "training_X = training_X.ix[indices]\n",
    "training_WV = training_WV.ix[indices]\n",
    "training_FS = training_FS.ix[indices]\n",
    "training_Y = training_Y.ix[indices]\n",
    "\n",
    "# Transform index to top index\n",
    "training_X = training_X.map(lambda x: [top_token2ind.get(i, 0) for i in x])\n",
    "\n",
    "dataset = zip(training_X, training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010a_TrainingData_00008    0.2\n",
       "2010a_TrainingData_00022    0.2\n",
       "2010a_TrainingData_00027    0.2\n",
       "2010a_TrainingData_00034    0.2\n",
       "2010a_TrainingData_00036    0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample_weights = training_Y.index.map(lambda x: (int(re.findall('^201([0-9])', x)[0]) + 1.))\n",
    "training_sample_weights = pd.Series(training_sample_weights / training_sample_weights.max(), index=training_Y.index)\n",
    "training_sample_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_sc = StandardScaler()\n",
    "fs_sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "def build_target(y, size):\n",
    "    e = np.zeros(size)\n",
    "    e[y] = 1\n",
    "    return e\n",
    "\n",
    "def build_input_output_data(X, WV, FS, Y, maxlen):\n",
    "\n",
    "    x = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    y = np.vstack(Y.map(lambda x: build_target(x, len(topics))))\n",
    "    wv = np.vstack(WV)\n",
    "    fs = np.vstack(FS)\n",
    "    \n",
    "    return x, wv, fs, y\n",
    "\n",
    "\n",
    "test_ix = training_Y.index.str.contains('^201[0-4]')\n",
    "val_ix = training_Y.index.str.contains('^2014[b]')\n",
    "\n",
    "\n",
    "x_train, wv_train, fs_train, y_train = build_input_output_data(\n",
    "    training_X.ix[test_ix],\n",
    "    training_WV.ix[test_ix],\n",
    "    training_FS.ix[test_ix],\n",
    "    training_Y.ix[test_ix],\n",
    "    maxlen=maxlen\n",
    ")\n",
    "\n",
    "\n",
    "x_val, wv_val, fs_val, y_val = build_input_output_data(\n",
    "    training_X.ix[val_ix],\n",
    "    training_WV.ix[val_ix],\n",
    "    training_FS.ix[val_ix],\n",
    "    training_Y.ix[val_ix],\n",
    "    maxlen=maxlen\n",
    ")\n",
    "\n",
    "wv_train = wv_sc.fit_transform(wv_train)\n",
    "fs_train = fs_sc.fit_transform(fs_train)\n",
    "\n",
    "wv_val = wv_sc.transform(wv_val)\n",
    "fs_val = fs_sc.transform(fs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94731,), (9424,), (94731,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_Y.shape, training_Y.ix[training_Y.index.str.contains('^2014[b]')].shape, training_sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Setup model\n",
    "# model_lstm = keras.models.Sequential()\n",
    "# model_lstm.add(keras.layers.Embedding(len(word2ind) + 1, 256))\n",
    "# # model_lstm.add(keras.layers.LSTM(32, return_sequences=False, input_shape=(None, len(word2ind) + 1)))\n",
    "# # model_lstm.add(keras.layers.Dropout(0.2))\n",
    "# model_lstm.add(keras.layers.LSTM(16, return_sequences=False))\n",
    "# model_lstm.add(keras.layers.Dense(128))\n",
    "# model_lstm.add(keras.layers.Activation('relu'))\n",
    "# model_lstm.add(keras.layers.Dropout(0.2))\n",
    "# model_lstm.add(keras.layers.Dense(len(class2ind)))\n",
    "# model_lstm.add(keras.layers.Activation('sigmoid'))\n",
    "# model_lstm.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # for i in range(6):\n",
    "# #     model_lstm.fit_generator(id_lstm_gen, steps_per_epoch=len(dataset), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86772972,  0.86588758,  1.0798322 , ...,  0.4315823 ,\n",
       "        0.35406384,  0.3516916 ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_train.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Convolution1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "num_filters = 10\n",
    "sz = 3\n",
    "\n",
    "wv_input = Input(shape=(300,), name='wv_input')\n",
    "fs_input = Input(shape=(300,), name='fs_input')\n",
    "\n",
    "wv_x = Dense(128, activation='relu')(wv_input)\n",
    "wv_x = Dropout(0.3)(wv_x)\n",
    "wv_x = Dense(512, activation='relu')(wv_x)\n",
    "wv_x = Dropout(0.3)(wv_x)\n",
    "\n",
    "fs_x = Dense(128, activation='relu')(fs_input)\n",
    "fs_x = Dropout(0.3)(fs_x)\n",
    "fs_x = Dense(512, activation='relu')(fs_x)\n",
    "fs_x = Dropout(0.3)(fs_x)\n",
    "\n",
    "x_mult = keras.layers.dot([wv_x, fs_x], 1)\n",
    "x = keras.layers.concatenate([wv_x, fs_x, x_mult])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(len(class2ind), activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=[wv_input, fs_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as K\n",
    "\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    TP = K.metrics.true_positives(y_true, K.round(y_pred))\n",
    "    FP = K.metrics.false_positives(y_true, K.round(y_pred))\n",
    "    FN = K.metrics.false_negatives(y_true, K.round(y_pred))\n",
    "    \n",
    "    p = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FP))\n",
    "    r = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FN))\n",
    "    \n",
    "    return (2.0 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "import keras.backend as KB\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = KB.sum(KB.round(KB.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = KB.sum(KB.round(KB.clip(y_pred, 0, 1)))\n",
    "    c3 = KB.sum(KB.round(KB.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "wv_input (InputLayer)            (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "fs_input (InputLayer)            (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 128)           38528                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 128)           38528                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 512)           66048                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 512)           66048                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 1025)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 128)           131328                                       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 256)           33024                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 128)           32896                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 160)           20640                                        \n",
      "====================================================================================================\n",
      "Total params: 427,040\n",
      "Trainable params: 427,040\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss={'main_output': 'categorical_crossentropy'},\n",
    "              loss_weights={'main_output': 1.}, metrics=['accuracy', f1_micro])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "# model.fit(X, y, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.train_on_batch(\n",
    "#     {'main_input': x_train[:10], 'wv_input': np.vstack(training_WV)[:10], 'fs_input': np.vstack(training_FS)[:10]},\n",
    "#     {'main_output': y_train[:10], 'aux_output': y_train[:10]}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.9674 - acc: 0.5478 - f1_micro: 0.2535 - val_loss: 2.7046 - val_acc: 0.6051 - val_f1_micro: 0.2645\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.9033 - acc: 0.5547 - f1_micro: 0.2746 - val_loss: 2.6373 - val_acc: 0.6115 - val_f1_micro: 0.2845\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.8473 - acc: 0.5614 - f1_micro: 0.2936 - val_loss: 2.5996 - val_acc: 0.6164 - val_f1_micro: 0.3020\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.8051 - acc: 0.5664 - f1_micro: 0.3099 - val_loss: 2.5656 - val_acc: 0.6161 - val_f1_micro: 0.3175\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.7725 - acc: 0.5720 - f1_micro: 0.3247 - val_loss: 2.5370 - val_acc: 0.6237 - val_f1_micro: 0.3315\n",
      "CPU times: user 7.11 s, sys: 464 ms, total: 7.57 s\n",
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# And trained it via:\n",
    "batch_size = 500\n",
    "model.fit(\n",
    "    {'wv_input': wv_train, 'fs_input': fs_train},\n",
    "    {'main_output': y_train},\n",
    "    epochs=5, batch_size=batch_size,   # 500\n",
    "    validation_split=0.2,\n",
    "    validation_data=(\n",
    "        {'wv_input': wv_val, 'fs_input': fs_val},\n",
    "        {'main_output': y_val}\n",
    "    ),\n",
    "#     sample_weight=training_sample_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9868 - acc: 0.6615 - f1_micro: 0.5943 - val_loss: 1.6196 - val_acc: 0.7321 - val_f1_micro: 0.5944\n",
      "Epoch 2/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9869 - acc: 0.6626 - f1_micro: 0.5944 - val_loss: 1.6181 - val_acc: 0.7315 - val_f1_micro: 0.5944\n",
      "Epoch 3/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9812 - acc: 0.6634 - f1_micro: 0.5944 - val_loss: 1.6202 - val_acc: 0.7328 - val_f1_micro: 0.5945\n",
      "Epoch 4/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9818 - acc: 0.6649 - f1_micro: 0.5945 - val_loss: 1.6139 - val_acc: 0.7346 - val_f1_micro: 0.5945\n",
      "Epoch 5/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9898 - acc: 0.6605 - f1_micro: 0.5946 - val_loss: 1.6152 - val_acc: 0.7351 - val_f1_micro: 0.5946\n",
      "Epoch 6/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9944 - acc: 0.6638 - f1_micro: 0.5946 - val_loss: 1.6137 - val_acc: 0.7353 - val_f1_micro: 0.5946\n",
      "Epoch 7/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9840 - acc: 0.6622 - f1_micro: 0.5947 - val_loss: 1.6180 - val_acc: 0.7324 - val_f1_micro: 0.5947\n",
      "Epoch 8/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9886 - acc: 0.6622 - f1_micro: 0.5947 - val_loss: 1.6167 - val_acc: 0.7373 - val_f1_micro: 0.5948\n",
      "Epoch 9/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9818 - acc: 0.6628 - f1_micro: 0.5948 - val_loss: 1.6164 - val_acc: 0.7388 - val_f1_micro: 0.5948\n",
      "Epoch 10/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9925 - acc: 0.6621 - f1_micro: 0.5949 - val_loss: 1.6104 - val_acc: 0.7337 - val_f1_micro: 0.5949\n",
      "Epoch 11/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9822 - acc: 0.6628 - f1_micro: 0.5949 - val_loss: 1.6176 - val_acc: 0.7321 - val_f1_micro: 0.5949\n",
      "Epoch 12/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9833 - acc: 0.6614 - f1_micro: 0.5950 - val_loss: 1.6153 - val_acc: 0.7340 - val_f1_micro: 0.5950\n",
      "Epoch 13/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9843 - acc: 0.6640 - f1_micro: 0.5950 - val_loss: 1.6161 - val_acc: 0.7345 - val_f1_micro: 0.5951\n",
      "Epoch 14/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9869 - acc: 0.6632 - f1_micro: 0.5951 - val_loss: 1.6168 - val_acc: 0.7279 - val_f1_micro: 0.5951\n",
      "Epoch 15/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9912 - acc: 0.6641 - f1_micro: 0.5951 - val_loss: 1.6211 - val_acc: 0.7313 - val_f1_micro: 0.5952\n",
      "Epoch 16/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9930 - acc: 0.6647 - f1_micro: 0.5952 - val_loss: 1.6175 - val_acc: 0.7375 - val_f1_micro: 0.5952\n",
      "Epoch 17/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9808 - acc: 0.6647 - f1_micro: 0.5953 - val_loss: 1.6162 - val_acc: 0.7299 - val_f1_micro: 0.5953\n",
      "Epoch 18/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9801 - acc: 0.6631 - f1_micro: 0.5953 - val_loss: 1.6235 - val_acc: 0.7288 - val_f1_micro: 0.5953\n",
      "Epoch 19/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9753 - acc: 0.6629 - f1_micro: 0.5954 - val_loss: 1.6264 - val_acc: 0.7291 - val_f1_micro: 0.5954\n",
      "Epoch 20/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9882 - acc: 0.6639 - f1_micro: 0.5954 - val_loss: 1.6209 - val_acc: 0.7277 - val_f1_micro: 0.5955\n",
      "Epoch 21/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9818 - acc: 0.6630 - f1_micro: 0.5955 - val_loss: 1.6136 - val_acc: 0.7311 - val_f1_micro: 0.5955\n",
      "Epoch 22/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9752 - acc: 0.6635 - f1_micro: 0.5956 - val_loss: 1.6096 - val_acc: 0.7358 - val_f1_micro: 0.5956\n",
      "Epoch 23/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9854 - acc: 0.6621 - f1_micro: 0.5956 - val_loss: 1.6143 - val_acc: 0.7312 - val_f1_micro: 0.5956\n",
      "Epoch 24/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9830 - acc: 0.6626 - f1_micro: 0.5957 - val_loss: 1.6137 - val_acc: 0.7329 - val_f1_micro: 0.5957\n",
      "Epoch 25/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9827 - acc: 0.6628 - f1_micro: 0.5957 - val_loss: 1.6215 - val_acc: 0.7306 - val_f1_micro: 0.5958\n",
      "Epoch 26/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9860 - acc: 0.6621 - f1_micro: 0.5958 - val_loss: 1.6103 - val_acc: 0.7339 - val_f1_micro: 0.5958\n",
      "Epoch 27/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9834 - acc: 0.6624 - f1_micro: 0.5959 - val_loss: 1.6174 - val_acc: 0.7316 - val_f1_micro: 0.5959\n",
      "Epoch 28/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9807 - acc: 0.6628 - f1_micro: 0.5959 - val_loss: 1.6120 - val_acc: 0.7374 - val_f1_micro: 0.5959\n",
      "Epoch 29/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9909 - acc: 0.6636 - f1_micro: 0.5960 - val_loss: 1.6085 - val_acc: 0.7261 - val_f1_micro: 0.5960\n",
      "Epoch 30/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9807 - acc: 0.6623 - f1_micro: 0.5960 - val_loss: 1.6158 - val_acc: 0.7297 - val_f1_micro: 0.5961\n",
      "Epoch 31/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9801 - acc: 0.6616 - f1_micro: 0.5961 - val_loss: 1.6112 - val_acc: 0.7302 - val_f1_micro: 0.5961\n",
      "Epoch 32/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9829 - acc: 0.6616 - f1_micro: 0.5962 - val_loss: 1.6074 - val_acc: 0.7271 - val_f1_micro: 0.5962\n",
      "Epoch 33/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9763 - acc: 0.6630 - f1_micro: 0.5962 - val_loss: 1.6085 - val_acc: 0.7272 - val_f1_micro: 0.5962\n",
      "Epoch 34/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9830 - acc: 0.6631 - f1_micro: 0.5963 - val_loss: 1.6080 - val_acc: 0.7273 - val_f1_micro: 0.5963\n",
      "Epoch 35/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9779 - acc: 0.6622 - f1_micro: 0.5963 - val_loss: 1.6125 - val_acc: 0.7313 - val_f1_micro: 0.5963\n",
      "Epoch 36/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9872 - acc: 0.6612 - f1_micro: 0.5964 - val_loss: 1.6128 - val_acc: 0.7348 - val_f1_micro: 0.5964\n",
      "Epoch 37/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9827 - acc: 0.6603 - f1_micro: 0.5964 - val_loss: 1.6087 - val_acc: 0.7296 - val_f1_micro: 0.5964\n",
      "Epoch 38/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9828 - acc: 0.6642 - f1_micro: 0.5965 - val_loss: 1.6110 - val_acc: 0.7307 - val_f1_micro: 0.5965\n",
      "Epoch 39/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9843 - acc: 0.6641 - f1_micro: 0.5965 - val_loss: 1.6126 - val_acc: 0.7316 - val_f1_micro: 0.5965\n",
      "Epoch 40/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9805 - acc: 0.6634 - f1_micro: 0.5966 - val_loss: 1.6104 - val_acc: 0.7345 - val_f1_micro: 0.5966\n",
      "Epoch 41/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9792 - acc: 0.6640 - f1_micro: 0.5966 - val_loss: 1.6065 - val_acc: 0.7320 - val_f1_micro: 0.5966\n",
      "Epoch 42/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9782 - acc: 0.6638 - f1_micro: 0.5967 - val_loss: 1.6086 - val_acc: 0.7324 - val_f1_micro: 0.5967\n",
      "Epoch 43/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9792 - acc: 0.6628 - f1_micro: 0.5967 - val_loss: 1.6098 - val_acc: 0.7265 - val_f1_micro: 0.5967\n",
      "Epoch 44/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9767 - acc: 0.6625 - f1_micro: 0.5968 - val_loss: 1.6102 - val_acc: 0.7320 - val_f1_micro: 0.5968\n",
      "Epoch 45/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9766 - acc: 0.6630 - f1_micro: 0.5968 - val_loss: 1.6119 - val_acc: 0.7355 - val_f1_micro: 0.5968\n",
      "Epoch 46/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9895 - acc: 0.6620 - f1_micro: 0.5969 - val_loss: 1.6143 - val_acc: 0.7306 - val_f1_micro: 0.5969\n",
      "Epoch 47/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9786 - acc: 0.6619 - f1_micro: 0.5969 - val_loss: 1.6143 - val_acc: 0.7312 - val_f1_micro: 0.5969\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9776 - acc: 0.6637 - f1_micro: 0.5970 - val_loss: 1.6106 - val_acc: 0.7311 - val_f1_micro: 0.5970\n",
      "Epoch 49/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9810 - acc: 0.6636 - f1_micro: 0.5970 - val_loss: 1.6027 - val_acc: 0.7252 - val_f1_micro: 0.5970\n",
      "Epoch 50/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9758 - acc: 0.6643 - f1_micro: 0.5971 - val_loss: 1.6129 - val_acc: 0.7267 - val_f1_micro: 0.5971\n",
      "Epoch 51/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9773 - acc: 0.6648 - f1_micro: 0.5971 - val_loss: 1.6083 - val_acc: 0.7316 - val_f1_micro: 0.5971\n",
      "Epoch 52/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9785 - acc: 0.6655 - f1_micro: 0.5972 - val_loss: 1.6087 - val_acc: 0.7234 - val_f1_micro: 0.5972\n",
      "Epoch 53/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9745 - acc: 0.6643 - f1_micro: 0.5972 - val_loss: 1.6122 - val_acc: 0.7229 - val_f1_micro: 0.5973\n",
      "Epoch 54/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9812 - acc: 0.6640 - f1_micro: 0.5973 - val_loss: 1.6089 - val_acc: 0.7272 - val_f1_micro: 0.5973\n",
      "Epoch 55/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9796 - acc: 0.6635 - f1_micro: 0.5973 - val_loss: 1.6110 - val_acc: 0.7229 - val_f1_micro: 0.5974\n",
      "Epoch 56/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9742 - acc: 0.6647 - f1_micro: 0.5974 - val_loss: 1.6066 - val_acc: 0.7325 - val_f1_micro: 0.5974\n",
      "Epoch 57/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9810 - acc: 0.6629 - f1_micro: 0.5974 - val_loss: 1.6055 - val_acc: 0.7293 - val_f1_micro: 0.5975\n",
      "Epoch 58/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9807 - acc: 0.6615 - f1_micro: 0.5975 - val_loss: 1.6091 - val_acc: 0.7376 - val_f1_micro: 0.5975\n",
      "Epoch 59/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9785 - acc: 0.6645 - f1_micro: 0.5975 - val_loss: 1.6119 - val_acc: 0.7288 - val_f1_micro: 0.5975\n",
      "Epoch 60/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9749 - acc: 0.6646 - f1_micro: 0.5976 - val_loss: 1.6103 - val_acc: 0.7325 - val_f1_micro: 0.5976\n",
      "Epoch 61/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9727 - acc: 0.6628 - f1_micro: 0.5976 - val_loss: 1.5995 - val_acc: 0.7373 - val_f1_micro: 0.5977\n",
      "Epoch 62/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9784 - acc: 0.6641 - f1_micro: 0.5977 - val_loss: 1.6049 - val_acc: 0.7326 - val_f1_micro: 0.5977\n",
      "Epoch 63/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9801 - acc: 0.6638 - f1_micro: 0.5977 - val_loss: 1.6085 - val_acc: 0.7373 - val_f1_micro: 0.5978\n",
      "Epoch 64/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9723 - acc: 0.6633 - f1_micro: 0.5978 - val_loss: 1.6033 - val_acc: 0.7322 - val_f1_micro: 0.5978\n",
      "Epoch 65/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9785 - acc: 0.6643 - f1_micro: 0.5979 - val_loss: 1.6008 - val_acc: 0.7351 - val_f1_micro: 0.5979\n",
      "Epoch 66/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9800 - acc: 0.6637 - f1_micro: 0.5979 - val_loss: 1.6045 - val_acc: 0.7241 - val_f1_micro: 0.5979\n",
      "Epoch 67/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9758 - acc: 0.6640 - f1_micro: 0.5980 - val_loss: 1.6050 - val_acc: 0.7357 - val_f1_micro: 0.5980\n",
      "Epoch 68/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9735 - acc: 0.6650 - f1_micro: 0.5980 - val_loss: 1.6056 - val_acc: 0.7267 - val_f1_micro: 0.5980\n",
      "Epoch 69/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9758 - acc: 0.6627 - f1_micro: 0.5981 - val_loss: 1.6064 - val_acc: 0.7295 - val_f1_micro: 0.5981\n",
      "Epoch 70/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9757 - acc: 0.6651 - f1_micro: 0.5981 - val_loss: 1.6066 - val_acc: 0.7271 - val_f1_micro: 0.5981\n",
      "Epoch 71/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9758 - acc: 0.6671 - f1_micro: 0.5982 - val_loss: 1.6023 - val_acc: 0.7320 - val_f1_micro: 0.5982\n",
      "Epoch 72/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9685 - acc: 0.6638 - f1_micro: 0.5982 - val_loss: 1.5999 - val_acc: 0.7308 - val_f1_micro: 0.5982\n",
      "Epoch 73/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9695 - acc: 0.6634 - f1_micro: 0.5983 - val_loss: 1.6009 - val_acc: 0.7308 - val_f1_micro: 0.5983\n",
      "Epoch 74/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9749 - acc: 0.6642 - f1_micro: 0.5983 - val_loss: 1.6073 - val_acc: 0.7309 - val_f1_micro: 0.5983\n",
      "Epoch 75/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9768 - acc: 0.6646 - f1_micro: 0.5984 - val_loss: 1.6058 - val_acc: 0.7301 - val_f1_micro: 0.5984\n",
      "Epoch 76/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9756 - acc: 0.6651 - f1_micro: 0.5984 - val_loss: 1.5979 - val_acc: 0.7409 - val_f1_micro: 0.5984\n",
      "Epoch 77/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9702 - acc: 0.6626 - f1_micro: 0.5985 - val_loss: 1.5995 - val_acc: 0.7296 - val_f1_micro: 0.5985\n",
      "Epoch 78/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9723 - acc: 0.6648 - f1_micro: 0.5985 - val_loss: 1.5978 - val_acc: 0.7331 - val_f1_micro: 0.5985\n",
      "Epoch 79/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9789 - acc: 0.6637 - f1_micro: 0.5986 - val_loss: 1.6027 - val_acc: 0.7333 - val_f1_micro: 0.5986\n",
      "Epoch 80/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9764 - acc: 0.6639 - f1_micro: 0.5986 - val_loss: 1.5909 - val_acc: 0.7323 - val_f1_micro: 0.5986\n",
      "Epoch 81/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9685 - acc: 0.6652 - f1_micro: 0.5987 - val_loss: 1.6113 - val_acc: 0.7316 - val_f1_micro: 0.5987\n",
      "Epoch 82/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9737 - acc: 0.6656 - f1_micro: 0.5987 - val_loss: 1.6034 - val_acc: 0.7315 - val_f1_micro: 0.5987\n",
      "Epoch 83/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9690 - acc: 0.6651 - f1_micro: 0.5988 - val_loss: 1.6039 - val_acc: 0.7321 - val_f1_micro: 0.5988\n",
      "Epoch 84/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9735 - acc: 0.6642 - f1_micro: 0.5988 - val_loss: 1.6013 - val_acc: 0.7270 - val_f1_micro: 0.5989\n",
      "Epoch 85/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9680 - acc: 0.6638 - f1_micro: 0.5989 - val_loss: 1.6085 - val_acc: 0.7286 - val_f1_micro: 0.5989\n",
      "Epoch 86/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9745 - acc: 0.6638 - f1_micro: 0.5989 - val_loss: 1.5988 - val_acc: 0.7305 - val_f1_micro: 0.5990\n",
      "Epoch 87/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9699 - acc: 0.6650 - f1_micro: 0.5990 - val_loss: 1.6013 - val_acc: 0.7324 - val_f1_micro: 0.5990\n",
      "Epoch 88/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9735 - acc: 0.6649 - f1_micro: 0.5990 - val_loss: 1.6014 - val_acc: 0.7338 - val_f1_micro: 0.5991\n",
      "Epoch 89/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9730 - acc: 0.6646 - f1_micro: 0.5991 - val_loss: 1.5969 - val_acc: 0.7321 - val_f1_micro: 0.5991\n",
      "Epoch 90/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9700 - acc: 0.6647 - f1_micro: 0.5991 - val_loss: 1.5957 - val_acc: 0.7265 - val_f1_micro: 0.5992\n",
      "Epoch 91/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9730 - acc: 0.6619 - f1_micro: 0.5992 - val_loss: 1.6017 - val_acc: 0.7241 - val_f1_micro: 0.5992\n",
      "Epoch 92/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9743 - acc: 0.6644 - f1_micro: 0.5992 - val_loss: 1.5950 - val_acc: 0.7306 - val_f1_micro: 0.5993\n",
      "Epoch 93/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9663 - acc: 0.6631 - f1_micro: 0.5993 - val_loss: 1.5886 - val_acc: 0.7350 - val_f1_micro: 0.5993\n",
      "Epoch 94/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9705 - acc: 0.6640 - f1_micro: 0.5993 - val_loss: 1.5948 - val_acc: 0.7298 - val_f1_micro: 0.5994\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9666 - acc: 0.6652 - f1_micro: 0.5994 - val_loss: 1.5950 - val_acc: 0.7322 - val_f1_micro: 0.5994\n",
      "Epoch 96/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9724 - acc: 0.6626 - f1_micro: 0.5994 - val_loss: 1.5998 - val_acc: 0.7339 - val_f1_micro: 0.5995\n",
      "Epoch 97/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9667 - acc: 0.6641 - f1_micro: 0.5995 - val_loss: 1.5968 - val_acc: 0.7305 - val_f1_micro: 0.5995\n",
      "Epoch 98/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9672 - acc: 0.6661 - f1_micro: 0.5995 - val_loss: 1.5963 - val_acc: 0.7356 - val_f1_micro: 0.5996\n",
      "Epoch 99/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9713 - acc: 0.6661 - f1_micro: 0.5996 - val_loss: 1.5955 - val_acc: 0.7328 - val_f1_micro: 0.5996\n",
      "Epoch 100/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9674 - acc: 0.6642 - f1_micro: 0.5996 - val_loss: 1.6015 - val_acc: 0.7304 - val_f1_micro: 0.5997\n",
      "Epoch 101/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9643 - acc: 0.6642 - f1_micro: 0.5997 - val_loss: 1.5941 - val_acc: 0.7341 - val_f1_micro: 0.5997\n",
      "Epoch 102/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9679 - acc: 0.6628 - f1_micro: 0.5998 - val_loss: 1.5957 - val_acc: 0.7363 - val_f1_micro: 0.5998\n",
      "Epoch 103/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9670 - acc: 0.6648 - f1_micro: 0.5998 - val_loss: 1.5931 - val_acc: 0.7345 - val_f1_micro: 0.5999\n",
      "Epoch 104/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9645 - acc: 0.6628 - f1_micro: 0.5999 - val_loss: 1.5930 - val_acc: 0.7361 - val_f1_micro: 0.5999\n",
      "Epoch 105/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9664 - acc: 0.6658 - f1_micro: 0.5999 - val_loss: 1.5999 - val_acc: 0.7338 - val_f1_micro: 0.6000\n",
      "Epoch 106/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9682 - acc: 0.6654 - f1_micro: 0.6000 - val_loss: 1.5942 - val_acc: 0.7311 - val_f1_micro: 0.6000\n",
      "Epoch 107/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9740 - acc: 0.6654 - f1_micro: 0.6001 - val_loss: 1.5994 - val_acc: 0.7358 - val_f1_micro: 0.6001\n",
      "Epoch 108/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9625 - acc: 0.6647 - f1_micro: 0.6001 - val_loss: 1.5931 - val_acc: 0.7301 - val_f1_micro: 0.6001\n",
      "Epoch 109/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9712 - acc: 0.6647 - f1_micro: 0.6001 - val_loss: 1.5941 - val_acc: 0.7346 - val_f1_micro: 0.6002\n",
      "Epoch 110/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9793 - acc: 0.6638 - f1_micro: 0.6002 - val_loss: 1.5889 - val_acc: 0.7362 - val_f1_micro: 0.6002\n",
      "Epoch 111/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9646 - acc: 0.6642 - f1_micro: 0.6002 - val_loss: 1.5913 - val_acc: 0.7268 - val_f1_micro: 0.6003\n",
      "Epoch 112/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9652 - acc: 0.6629 - f1_micro: 0.6003 - val_loss: 1.5926 - val_acc: 0.7321 - val_f1_micro: 0.6003\n",
      "Epoch 113/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9701 - acc: 0.6644 - f1_micro: 0.6003 - val_loss: 1.5977 - val_acc: 0.7320 - val_f1_micro: 0.6004\n",
      "Epoch 114/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9715 - acc: 0.6661 - f1_micro: 0.6004 - val_loss: 1.5970 - val_acc: 0.7272 - val_f1_micro: 0.6004\n",
      "Epoch 115/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9746 - acc: 0.6618 - f1_micro: 0.6004 - val_loss: 1.5962 - val_acc: 0.7363 - val_f1_micro: 0.6005\n",
      "Epoch 116/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9781 - acc: 0.6642 - f1_micro: 0.6005 - val_loss: 1.5990 - val_acc: 0.7307 - val_f1_micro: 0.6005\n",
      "Epoch 117/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9657 - acc: 0.6643 - f1_micro: 0.6005 - val_loss: 1.5927 - val_acc: 0.7281 - val_f1_micro: 0.6006\n",
      "Epoch 118/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9676 - acc: 0.6666 - f1_micro: 0.6006 - val_loss: 1.5922 - val_acc: 0.7293 - val_f1_micro: 0.6006\n",
      "Epoch 119/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9676 - acc: 0.6639 - f1_micro: 0.6006 - val_loss: 1.5875 - val_acc: 0.7338 - val_f1_micro: 0.6007\n",
      "Epoch 120/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9662 - acc: 0.6636 - f1_micro: 0.6007 - val_loss: 1.5923 - val_acc: 0.7327 - val_f1_micro: 0.6007\n",
      "Epoch 121/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9656 - acc: 0.6648 - f1_micro: 0.6007 - val_loss: 1.5914 - val_acc: 0.7334 - val_f1_micro: 0.6008\n",
      "Epoch 122/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9690 - acc: 0.6652 - f1_micro: 0.6008 - val_loss: 1.5929 - val_acc: 0.7339 - val_f1_micro: 0.6008\n",
      "Epoch 123/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9652 - acc: 0.6641 - f1_micro: 0.6008 - val_loss: 1.5840 - val_acc: 0.7312 - val_f1_micro: 0.6009\n",
      "Epoch 124/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9687 - acc: 0.6655 - f1_micro: 0.6009 - val_loss: 1.5910 - val_acc: 0.7341 - val_f1_micro: 0.6009\n",
      "Epoch 125/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9597 - acc: 0.6673 - f1_micro: 0.6009 - val_loss: 1.5952 - val_acc: 0.7305 - val_f1_micro: 0.6010\n",
      "Epoch 126/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9617 - acc: 0.6663 - f1_micro: 0.6010 - val_loss: 1.5937 - val_acc: 0.7346 - val_f1_micro: 0.6010\n",
      "Epoch 127/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9586 - acc: 0.6659 - f1_micro: 0.6010 - val_loss: 1.5958 - val_acc: 0.7324 - val_f1_micro: 0.6011\n",
      "Epoch 128/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9624 - acc: 0.6649 - f1_micro: 0.6011 - val_loss: 1.5894 - val_acc: 0.7269 - val_f1_micro: 0.6011\n",
      "Epoch 129/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9649 - acc: 0.6664 - f1_micro: 0.6011 - val_loss: 1.5886 - val_acc: 0.7350 - val_f1_micro: 0.6012\n",
      "Epoch 130/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9655 - acc: 0.6624 - f1_micro: 0.6012 - val_loss: 1.5891 - val_acc: 0.7324 - val_f1_micro: 0.6012\n",
      "Epoch 131/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9630 - acc: 0.6649 - f1_micro: 0.6013 - val_loss: 1.5896 - val_acc: 0.7251 - val_f1_micro: 0.6013\n",
      "Epoch 132/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9656 - acc: 0.6656 - f1_micro: 0.6013 - val_loss: 1.5937 - val_acc: 0.7351 - val_f1_micro: 0.6013\n",
      "Epoch 133/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9607 - acc: 0.6655 - f1_micro: 0.6014 - val_loss: 1.5934 - val_acc: 0.7345 - val_f1_micro: 0.6014\n",
      "Epoch 134/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9652 - acc: 0.6661 - f1_micro: 0.6014 - val_loss: 1.5830 - val_acc: 0.7312 - val_f1_micro: 0.6014\n",
      "Epoch 135/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9737 - acc: 0.6647 - f1_micro: 0.6015 - val_loss: 1.5929 - val_acc: 0.7356 - val_f1_micro: 0.6015\n",
      "Epoch 136/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9621 - acc: 0.6656 - f1_micro: 0.6015 - val_loss: 1.5916 - val_acc: 0.7368 - val_f1_micro: 0.6015\n",
      "Epoch 137/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9648 - acc: 0.6655 - f1_micro: 0.6016 - val_loss: 1.5904 - val_acc: 0.7284 - val_f1_micro: 0.6016\n",
      "Epoch 138/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9585 - acc: 0.6649 - f1_micro: 0.6016 - val_loss: 1.5855 - val_acc: 0.7382 - val_f1_micro: 0.6016\n",
      "Epoch 139/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9657 - acc: 0.6647 - f1_micro: 0.6017 - val_loss: 1.5898 - val_acc: 0.7313 - val_f1_micro: 0.6017\n",
      "Epoch 140/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9623 - acc: 0.6676 - f1_micro: 0.6017 - val_loss: 1.5952 - val_acc: 0.7343 - val_f1_micro: 0.6017\n",
      "Epoch 141/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9663 - acc: 0.6672 - f1_micro: 0.6018 - val_loss: 1.5955 - val_acc: 0.7346 - val_f1_micro: 0.6018\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9589 - acc: 0.6651 - f1_micro: 0.6018 - val_loss: 1.5893 - val_acc: 0.7334 - val_f1_micro: 0.6018\n",
      "Epoch 143/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9572 - acc: 0.6674 - f1_micro: 0.6019 - val_loss: 1.5906 - val_acc: 0.7365 - val_f1_micro: 0.6019\n",
      "Epoch 144/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9635 - acc: 0.6654 - f1_micro: 0.6019 - val_loss: 1.5922 - val_acc: 0.7350 - val_f1_micro: 0.6019\n",
      "Epoch 145/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9556 - acc: 0.6656 - f1_micro: 0.6020 - val_loss: 1.5889 - val_acc: 0.7351 - val_f1_micro: 0.6020\n",
      "Epoch 146/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9618 - acc: 0.6672 - f1_micro: 0.6020 - val_loss: 1.5950 - val_acc: 0.7353 - val_f1_micro: 0.6021\n",
      "Epoch 147/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9617 - acc: 0.6666 - f1_micro: 0.6021 - val_loss: 1.5897 - val_acc: 0.7336 - val_f1_micro: 0.6021\n",
      "Epoch 148/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9529 - acc: 0.6673 - f1_micro: 0.6021 - val_loss: 1.5824 - val_acc: 0.7281 - val_f1_micro: 0.6022\n",
      "Epoch 149/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9644 - acc: 0.6664 - f1_micro: 0.6022 - val_loss: 1.5884 - val_acc: 0.7398 - val_f1_micro: 0.6022\n",
      "Epoch 150/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9677 - acc: 0.6646 - f1_micro: 0.6022 - val_loss: 1.5919 - val_acc: 0.7316 - val_f1_micro: 0.6023\n",
      "Epoch 151/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9594 - acc: 0.6656 - f1_micro: 0.6023 - val_loss: 1.5867 - val_acc: 0.7297 - val_f1_micro: 0.6023\n",
      "Epoch 152/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9612 - acc: 0.6665 - f1_micro: 0.6023 - val_loss: 1.5850 - val_acc: 0.7341 - val_f1_micro: 0.6024\n",
      "Epoch 153/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9585 - acc: 0.6653 - f1_micro: 0.6024 - val_loss: 1.5906 - val_acc: 0.7319 - val_f1_micro: 0.6024\n",
      "Epoch 154/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9619 - acc: 0.6661 - f1_micro: 0.6024 - val_loss: 1.5899 - val_acc: 0.7346 - val_f1_micro: 0.6024\n",
      "Epoch 155/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9658 - acc: 0.6675 - f1_micro: 0.6025 - val_loss: 1.5882 - val_acc: 0.7259 - val_f1_micro: 0.6025\n",
      "Epoch 156/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9584 - acc: 0.6665 - f1_micro: 0.6025 - val_loss: 1.5832 - val_acc: 0.7388 - val_f1_micro: 0.6025\n",
      "Epoch 157/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9625 - acc: 0.6672 - f1_micro: 0.6026 - val_loss: 1.5956 - val_acc: 0.7302 - val_f1_micro: 0.6026\n",
      "Epoch 158/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9646 - acc: 0.6663 - f1_micro: 0.6026 - val_loss: 1.5885 - val_acc: 0.7392 - val_f1_micro: 0.6026\n",
      "Epoch 159/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9592 - acc: 0.6676 - f1_micro: 0.6027 - val_loss: 1.5866 - val_acc: 0.7319 - val_f1_micro: 0.6027\n",
      "Epoch 160/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9517 - acc: 0.6664 - f1_micro: 0.6027 - val_loss: 1.5901 - val_acc: 0.7346 - val_f1_micro: 0.6027\n",
      "Epoch 161/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9671 - acc: 0.6653 - f1_micro: 0.6028 - val_loss: 1.5903 - val_acc: 0.7390 - val_f1_micro: 0.6028\n",
      "Epoch 162/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9587 - acc: 0.6687 - f1_micro: 0.6028 - val_loss: 1.5825 - val_acc: 0.7258 - val_f1_micro: 0.6028\n",
      "Epoch 163/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9553 - acc: 0.6662 - f1_micro: 0.6028 - val_loss: 1.5844 - val_acc: 0.7400 - val_f1_micro: 0.6029\n",
      "Epoch 164/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9586 - acc: 0.6673 - f1_micro: 0.6029 - val_loss: 1.5818 - val_acc: 0.7395 - val_f1_micro: 0.6029\n",
      "Epoch 165/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9577 - acc: 0.6679 - f1_micro: 0.6029 - val_loss: 1.5875 - val_acc: 0.7299 - val_f1_micro: 0.6030\n",
      "Epoch 166/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9563 - acc: 0.6650 - f1_micro: 0.6030 - val_loss: 1.5784 - val_acc: 0.7315 - val_f1_micro: 0.6030\n",
      "Epoch 167/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9624 - acc: 0.6628 - f1_micro: 0.6030 - val_loss: 1.5834 - val_acc: 0.7360 - val_f1_micro: 0.6030\n",
      "Epoch 168/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9594 - acc: 0.6667 - f1_micro: 0.6030 - val_loss: 1.5802 - val_acc: 0.7365 - val_f1_micro: 0.6031\n",
      "Epoch 169/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9583 - acc: 0.6655 - f1_micro: 0.6031 - val_loss: 1.5882 - val_acc: 0.7386 - val_f1_micro: 0.6031\n",
      "Epoch 170/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9662 - acc: 0.6646 - f1_micro: 0.6032 - val_loss: 1.5857 - val_acc: 0.7413 - val_f1_micro: 0.6032\n",
      "Epoch 171/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9575 - acc: 0.6672 - f1_micro: 0.6032 - val_loss: 1.5851 - val_acc: 0.7349 - val_f1_micro: 0.6032\n",
      "Epoch 172/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9553 - acc: 0.6656 - f1_micro: 0.6032 - val_loss: 1.5836 - val_acc: 0.7354 - val_f1_micro: 0.6033\n",
      "Epoch 173/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9586 - acc: 0.6658 - f1_micro: 0.6033 - val_loss: 1.5837 - val_acc: 0.7297 - val_f1_micro: 0.6033\n",
      "Epoch 174/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9573 - acc: 0.6653 - f1_micro: 0.6033 - val_loss: 1.5819 - val_acc: 0.7287 - val_f1_micro: 0.6033\n",
      "Epoch 175/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9543 - acc: 0.6631 - f1_micro: 0.6034 - val_loss: 1.5811 - val_acc: 0.7302 - val_f1_micro: 0.6034\n",
      "Epoch 176/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9581 - acc: 0.6661 - f1_micro: 0.6034 - val_loss: 1.5811 - val_acc: 0.7317 - val_f1_micro: 0.6034\n",
      "Epoch 177/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9606 - acc: 0.6651 - f1_micro: 0.6035 - val_loss: 1.5827 - val_acc: 0.7326 - val_f1_micro: 0.6035\n",
      "Epoch 178/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9568 - acc: 0.6650 - f1_micro: 0.6035 - val_loss: 1.5837 - val_acc: 0.7375 - val_f1_micro: 0.6035\n",
      "Epoch 179/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9628 - acc: 0.6661 - f1_micro: 0.6036 - val_loss: 1.5781 - val_acc: 0.7343 - val_f1_micro: 0.6036\n",
      "Epoch 180/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9579 - acc: 0.6662 - f1_micro: 0.6036 - val_loss: 1.5820 - val_acc: 0.7284 - val_f1_micro: 0.6036\n",
      "Epoch 181/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9628 - acc: 0.6633 - f1_micro: 0.6036 - val_loss: 1.5804 - val_acc: 0.7334 - val_f1_micro: 0.6037\n",
      "Epoch 182/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9568 - acc: 0.6663 - f1_micro: 0.6037 - val_loss: 1.5799 - val_acc: 0.7372 - val_f1_micro: 0.6037\n",
      "Epoch 183/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9555 - acc: 0.6662 - f1_micro: 0.6037 - val_loss: 1.5891 - val_acc: 0.7351 - val_f1_micro: 0.6037\n",
      "Epoch 184/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9581 - acc: 0.6648 - f1_micro: 0.6038 - val_loss: 1.5790 - val_acc: 0.7297 - val_f1_micro: 0.6038\n",
      "Epoch 185/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9588 - acc: 0.6660 - f1_micro: 0.6038 - val_loss: 1.5811 - val_acc: 0.7336 - val_f1_micro: 0.6038\n",
      "Epoch 186/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9482 - acc: 0.6673 - f1_micro: 0.6038 - val_loss: 1.5832 - val_acc: 0.7371 - val_f1_micro: 0.6039\n",
      "Epoch 187/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9656 - acc: 0.6658 - f1_micro: 0.6039 - val_loss: 1.5756 - val_acc: 0.7366 - val_f1_micro: 0.6039\n",
      "Epoch 188/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9620 - acc: 0.6646 - f1_micro: 0.6039 - val_loss: 1.5834 - val_acc: 0.7281 - val_f1_micro: 0.6039\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9578 - acc: 0.6659 - f1_micro: 0.6040 - val_loss: 1.5796 - val_acc: 0.7326 - val_f1_micro: 0.6040\n",
      "Epoch 190/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9534 - acc: 0.6658 - f1_micro: 0.6040 - val_loss: 1.5820 - val_acc: 0.7331 - val_f1_micro: 0.6040\n",
      "Epoch 191/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9624 - acc: 0.6629 - f1_micro: 0.6041 - val_loss: 1.5843 - val_acc: 0.7345 - val_f1_micro: 0.6041\n",
      "Epoch 192/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9574 - acc: 0.6636 - f1_micro: 0.6041 - val_loss: 1.5839 - val_acc: 0.7366 - val_f1_micro: 0.6041\n",
      "Epoch 193/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9566 - acc: 0.6663 - f1_micro: 0.6041 - val_loss: 1.5779 - val_acc: 0.7343 - val_f1_micro: 0.6042\n",
      "Epoch 194/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9522 - acc: 0.6659 - f1_micro: 0.6042 - val_loss: 1.5826 - val_acc: 0.7320 - val_f1_micro: 0.6042\n",
      "Epoch 195/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9575 - acc: 0.6649 - f1_micro: 0.6043 - val_loss: 1.5797 - val_acc: 0.7312 - val_f1_micro: 0.6043\n",
      "Epoch 196/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9532 - acc: 0.6646 - f1_micro: 0.6043 - val_loss: 1.5755 - val_acc: 0.7282 - val_f1_micro: 0.6043\n",
      "Epoch 197/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9530 - acc: 0.6663 - f1_micro: 0.6043 - val_loss: 1.5767 - val_acc: 0.7345 - val_f1_micro: 0.6044\n",
      "Epoch 198/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9507 - acc: 0.6662 - f1_micro: 0.6044 - val_loss: 1.5844 - val_acc: 0.7344 - val_f1_micro: 0.6044\n",
      "Epoch 199/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9583 - acc: 0.6660 - f1_micro: 0.6044 - val_loss: 1.5837 - val_acc: 0.7372 - val_f1_micro: 0.6045\n",
      "Epoch 200/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9503 - acc: 0.6674 - f1_micro: 0.6045 - val_loss: 1.5797 - val_acc: 0.7338 - val_f1_micro: 0.6045\n",
      "Epoch 201/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9529 - acc: 0.6682 - f1_micro: 0.6045 - val_loss: 1.5780 - val_acc: 0.7314 - val_f1_micro: 0.6046\n",
      "Epoch 202/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9596 - acc: 0.6652 - f1_micro: 0.6046 - val_loss: 1.5732 - val_acc: 0.7296 - val_f1_micro: 0.6046\n",
      "Epoch 203/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9596 - acc: 0.6642 - f1_micro: 0.6046 - val_loss: 1.5816 - val_acc: 0.7326 - val_f1_micro: 0.6046\n",
      "Epoch 204/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9530 - acc: 0.6653 - f1_micro: 0.6047 - val_loss: 1.5807 - val_acc: 0.7311 - val_f1_micro: 0.6047\n",
      "Epoch 205/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9560 - acc: 0.6649 - f1_micro: 0.6047 - val_loss: 1.5783 - val_acc: 0.7393 - val_f1_micro: 0.6047\n",
      "Epoch 206/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9489 - acc: 0.6657 - f1_micro: 0.6047 - val_loss: 1.5780 - val_acc: 0.7333 - val_f1_micro: 0.6048\n",
      "Epoch 207/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9447 - acc: 0.6673 - f1_micro: 0.6048 - val_loss: 1.5746 - val_acc: 0.7271 - val_f1_micro: 0.6048\n",
      "Epoch 208/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9526 - acc: 0.6659 - f1_micro: 0.6048 - val_loss: 1.5764 - val_acc: 0.7397 - val_f1_micro: 0.6049\n",
      "Epoch 209/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9548 - acc: 0.6636 - f1_micro: 0.6049 - val_loss: 1.5774 - val_acc: 0.7336 - val_f1_micro: 0.6049\n",
      "Epoch 210/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9476 - acc: 0.6643 - f1_micro: 0.6049 - val_loss: 1.5731 - val_acc: 0.7364 - val_f1_micro: 0.6049\n",
      "Epoch 211/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9517 - acc: 0.6657 - f1_micro: 0.6050 - val_loss: 1.5764 - val_acc: 0.7285 - val_f1_micro: 0.6050\n",
      "Epoch 212/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9571 - acc: 0.6647 - f1_micro: 0.6050 - val_loss: 1.5762 - val_acc: 0.7435 - val_f1_micro: 0.6050\n",
      "Epoch 213/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9515 - acc: 0.6674 - f1_micro: 0.6051 - val_loss: 1.5772 - val_acc: 0.7379 - val_f1_micro: 0.6051\n",
      "Epoch 214/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9567 - acc: 0.6681 - f1_micro: 0.6051 - val_loss: 1.5749 - val_acc: 0.7342 - val_f1_micro: 0.6051\n",
      "Epoch 215/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9524 - acc: 0.6666 - f1_micro: 0.6052 - val_loss: 1.5748 - val_acc: 0.7372 - val_f1_micro: 0.6052\n",
      "Epoch 216/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9552 - acc: 0.6668 - f1_micro: 0.6052 - val_loss: 1.5711 - val_acc: 0.7388 - val_f1_micro: 0.6052\n",
      "Epoch 217/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9520 - acc: 0.6676 - f1_micro: 0.6052 - val_loss: 1.5791 - val_acc: 0.7314 - val_f1_micro: 0.6053\n",
      "Epoch 218/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9514 - acc: 0.6685 - f1_micro: 0.6053 - val_loss: 1.5726 - val_acc: 0.7280 - val_f1_micro: 0.6053\n",
      "Epoch 219/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9522 - acc: 0.6677 - f1_micro: 0.6053 - val_loss: 1.5720 - val_acc: 0.7270 - val_f1_micro: 0.6053\n",
      "Epoch 220/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9495 - acc: 0.6664 - f1_micro: 0.6054 - val_loss: 1.5720 - val_acc: 0.7309 - val_f1_micro: 0.6054\n",
      "Epoch 221/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9504 - acc: 0.6646 - f1_micro: 0.6054 - val_loss: 1.5696 - val_acc: 0.7350 - val_f1_micro: 0.6054\n",
      "Epoch 222/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9468 - acc: 0.6666 - f1_micro: 0.6055 - val_loss: 1.5726 - val_acc: 0.7347 - val_f1_micro: 0.6055\n",
      "Epoch 223/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9519 - acc: 0.6667 - f1_micro: 0.6055 - val_loss: 1.5727 - val_acc: 0.7313 - val_f1_micro: 0.6055\n",
      "Epoch 224/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9452 - acc: 0.6659 - f1_micro: 0.6055 - val_loss: 1.5770 - val_acc: 0.7350 - val_f1_micro: 0.6056\n",
      "Epoch 225/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9532 - acc: 0.6657 - f1_micro: 0.6056 - val_loss: 1.5732 - val_acc: 0.7308 - val_f1_micro: 0.6056\n",
      "Epoch 226/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9513 - acc: 0.6671 - f1_micro: 0.6056 - val_loss: 1.5797 - val_acc: 0.7344 - val_f1_micro: 0.6057\n",
      "Epoch 227/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9557 - acc: 0.6644 - f1_micro: 0.6057 - val_loss: 1.5768 - val_acc: 0.7346 - val_f1_micro: 0.6057\n",
      "Epoch 228/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9538 - acc: 0.6663 - f1_micro: 0.6057 - val_loss: 1.5814 - val_acc: 0.7341 - val_f1_micro: 0.6057\n",
      "Epoch 229/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9543 - acc: 0.6661 - f1_micro: 0.6057 - val_loss: 1.5761 - val_acc: 0.7322 - val_f1_micro: 0.6058\n",
      "Epoch 230/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9473 - acc: 0.6656 - f1_micro: 0.6058 - val_loss: 1.5770 - val_acc: 0.7386 - val_f1_micro: 0.6058\n",
      "Epoch 231/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9500 - acc: 0.6679 - f1_micro: 0.6058 - val_loss: 1.5790 - val_acc: 0.7390 - val_f1_micro: 0.6058\n",
      "Epoch 232/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9501 - acc: 0.6675 - f1_micro: 0.6059 - val_loss: 1.5742 - val_acc: 0.7309 - val_f1_micro: 0.6059\n",
      "Epoch 233/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9541 - acc: 0.6650 - f1_micro: 0.6059 - val_loss: 1.5786 - val_acc: 0.7327 - val_f1_micro: 0.6059\n",
      "Epoch 234/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9445 - acc: 0.6669 - f1_micro: 0.6060 - val_loss: 1.5726 - val_acc: 0.7334 - val_f1_micro: 0.6060\n",
      "Epoch 235/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9447 - acc: 0.6673 - f1_micro: 0.6060 - val_loss: 1.5707 - val_acc: 0.7343 - val_f1_micro: 0.6060\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9484 - acc: 0.6677 - f1_micro: 0.6060 - val_loss: 1.5726 - val_acc: 0.7363 - val_f1_micro: 0.6060\n",
      "Epoch 237/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9514 - acc: 0.6667 - f1_micro: 0.6061 - val_loss: 1.5679 - val_acc: 0.7341 - val_f1_micro: 0.6061\n",
      "Epoch 238/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9529 - acc: 0.6663 - f1_micro: 0.6061 - val_loss: 1.5740 - val_acc: 0.7331 - val_f1_micro: 0.6061\n",
      "Epoch 239/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9491 - acc: 0.6685 - f1_micro: 0.6061 - val_loss: 1.5695 - val_acc: 0.7374 - val_f1_micro: 0.6062\n",
      "Epoch 240/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9509 - acc: 0.6658 - f1_micro: 0.6062 - val_loss: 1.5673 - val_acc: 0.7384 - val_f1_micro: 0.6062\n",
      "Epoch 241/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9468 - acc: 0.6667 - f1_micro: 0.6062 - val_loss: 1.5721 - val_acc: 0.7385 - val_f1_micro: 0.6063\n",
      "Epoch 242/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9525 - acc: 0.6666 - f1_micro: 0.6063 - val_loss: 1.5688 - val_acc: 0.7402 - val_f1_micro: 0.6063\n",
      "Epoch 243/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9485 - acc: 0.6673 - f1_micro: 0.6063 - val_loss: 1.5687 - val_acc: 0.7361 - val_f1_micro: 0.6063\n",
      "Epoch 244/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9504 - acc: 0.6670 - f1_micro: 0.6063 - val_loss: 1.5636 - val_acc: 0.7319 - val_f1_micro: 0.6064\n",
      "Epoch 245/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9496 - acc: 0.6679 - f1_micro: 0.6064 - val_loss: 1.5713 - val_acc: 0.7366 - val_f1_micro: 0.6064\n",
      "Epoch 246/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9497 - acc: 0.6667 - f1_micro: 0.6064 - val_loss: 1.5680 - val_acc: 0.7315 - val_f1_micro: 0.6064\n",
      "Epoch 247/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9496 - acc: 0.6665 - f1_micro: 0.6064 - val_loss: 1.5714 - val_acc: 0.7243 - val_f1_micro: 0.6065\n",
      "Epoch 248/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9482 - acc: 0.6661 - f1_micro: 0.6065 - val_loss: 1.5636 - val_acc: 0.7287 - val_f1_micro: 0.6065\n",
      "Epoch 249/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9424 - acc: 0.6672 - f1_micro: 0.6065 - val_loss: 1.5649 - val_acc: 0.7350 - val_f1_micro: 0.6065\n",
      "Epoch 250/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9475 - acc: 0.6664 - f1_micro: 0.6066 - val_loss: 1.5704 - val_acc: 0.7329 - val_f1_micro: 0.6066\n",
      "Epoch 251/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9438 - acc: 0.6673 - f1_micro: 0.6066 - val_loss: 1.5691 - val_acc: 0.7306 - val_f1_micro: 0.6066\n",
      "Epoch 252/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9512 - acc: 0.6661 - f1_micro: 0.6066 - val_loss: 1.5688 - val_acc: 0.7314 - val_f1_micro: 0.6066\n",
      "Epoch 253/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9460 - acc: 0.6678 - f1_micro: 0.6067 - val_loss: 1.5736 - val_acc: 0.7382 - val_f1_micro: 0.6067\n",
      "Epoch 254/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9426 - acc: 0.6678 - f1_micro: 0.6067 - val_loss: 1.5706 - val_acc: 0.7368 - val_f1_micro: 0.6067\n",
      "Epoch 255/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9423 - acc: 0.6679 - f1_micro: 0.6067 - val_loss: 1.5688 - val_acc: 0.7284 - val_f1_micro: 0.6067\n",
      "Epoch 256/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9445 - acc: 0.6672 - f1_micro: 0.6068 - val_loss: 1.5711 - val_acc: 0.7376 - val_f1_micro: 0.6068\n",
      "Epoch 257/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9435 - acc: 0.6672 - f1_micro: 0.6068 - val_loss: 1.5664 - val_acc: 0.7388 - val_f1_micro: 0.6068\n",
      "Epoch 258/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9459 - acc: 0.6660 - f1_micro: 0.6068 - val_loss: 1.5644 - val_acc: 0.7309 - val_f1_micro: 0.6069\n",
      "Epoch 259/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9460 - acc: 0.6675 - f1_micro: 0.6069 - val_loss: 1.5614 - val_acc: 0.7348 - val_f1_micro: 0.6069\n",
      "Epoch 260/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9416 - acc: 0.6678 - f1_micro: 0.6069 - val_loss: 1.5685 - val_acc: 0.7326 - val_f1_micro: 0.6070\n",
      "Epoch 261/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9432 - acc: 0.6677 - f1_micro: 0.6070 - val_loss: 1.5740 - val_acc: 0.7373 - val_f1_micro: 0.6070\n",
      "Epoch 262/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9423 - acc: 0.6689 - f1_micro: 0.6070 - val_loss: 1.5729 - val_acc: 0.7273 - val_f1_micro: 0.6070\n",
      "Epoch 263/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9447 - acc: 0.6657 - f1_micro: 0.6071 - val_loss: 1.5689 - val_acc: 0.7366 - val_f1_micro: 0.6071\n",
      "Epoch 264/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9448 - acc: 0.6667 - f1_micro: 0.6071 - val_loss: 1.5721 - val_acc: 0.7257 - val_f1_micro: 0.6071\n",
      "Epoch 265/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9476 - acc: 0.6694 - f1_micro: 0.6071 - val_loss: 1.5689 - val_acc: 0.7356 - val_f1_micro: 0.6072\n",
      "Epoch 266/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9437 - acc: 0.6672 - f1_micro: 0.6072 - val_loss: 1.5644 - val_acc: 0.7349 - val_f1_micro: 0.6072\n",
      "Epoch 267/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9442 - acc: 0.6668 - f1_micro: 0.6072 - val_loss: 1.5674 - val_acc: 0.7334 - val_f1_micro: 0.6072\n",
      "Epoch 268/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9446 - acc: 0.6681 - f1_micro: 0.6073 - val_loss: 1.5704 - val_acc: 0.7377 - val_f1_micro: 0.6073\n",
      "Epoch 269/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9449 - acc: 0.6674 - f1_micro: 0.6073 - val_loss: 1.5766 - val_acc: 0.7290 - val_f1_micro: 0.6073\n",
      "Epoch 270/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9515 - acc: 0.6665 - f1_micro: 0.6073 - val_loss: 1.5717 - val_acc: 0.7376 - val_f1_micro: 0.6074\n",
      "Epoch 271/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9415 - acc: 0.6656 - f1_micro: 0.6074 - val_loss: 1.5584 - val_acc: 0.7363 - val_f1_micro: 0.6074\n",
      "Epoch 272/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9373 - acc: 0.6667 - f1_micro: 0.6074 - val_loss: 1.5609 - val_acc: 0.7396 - val_f1_micro: 0.6074\n",
      "Epoch 273/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9408 - acc: 0.6669 - f1_micro: 0.6075 - val_loss: 1.5676 - val_acc: 0.7322 - val_f1_micro: 0.6075\n",
      "Epoch 274/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9468 - acc: 0.6684 - f1_micro: 0.6075 - val_loss: 1.5679 - val_acc: 0.7336 - val_f1_micro: 0.6075\n",
      "Epoch 275/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9451 - acc: 0.6666 - f1_micro: 0.6075 - val_loss: 1.5613 - val_acc: 0.7337 - val_f1_micro: 0.6076\n",
      "Epoch 276/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9405 - acc: 0.6655 - f1_micro: 0.6076 - val_loss: 1.5622 - val_acc: 0.7246 - val_f1_micro: 0.6076\n",
      "Epoch 277/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9472 - acc: 0.6671 - f1_micro: 0.6076 - val_loss: 1.5619 - val_acc: 0.7339 - val_f1_micro: 0.6076\n",
      "Epoch 278/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9490 - acc: 0.6680 - f1_micro: 0.6077 - val_loss: 1.5615 - val_acc: 0.7345 - val_f1_micro: 0.6077\n",
      "Epoch 279/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9385 - acc: 0.6684 - f1_micro: 0.6077 - val_loss: 1.5600 - val_acc: 0.7285 - val_f1_micro: 0.6077\n",
      "Epoch 280/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9428 - acc: 0.6661 - f1_micro: 0.6077 - val_loss: 1.5687 - val_acc: 0.7305 - val_f1_micro: 0.6078\n",
      "Epoch 281/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9449 - acc: 0.6662 - f1_micro: 0.6078 - val_loss: 1.5623 - val_acc: 0.7324 - val_f1_micro: 0.6078\n",
      "Epoch 282/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9460 - acc: 0.6667 - f1_micro: 0.6078 - val_loss: 1.5672 - val_acc: 0.7330 - val_f1_micro: 0.6078\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9394 - acc: 0.6668 - f1_micro: 0.6079 - val_loss: 1.5635 - val_acc: 0.7277 - val_f1_micro: 0.6079\n",
      "Epoch 284/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9383 - acc: 0.6666 - f1_micro: 0.6079 - val_loss: 1.5611 - val_acc: 0.7360 - val_f1_micro: 0.6079\n",
      "Epoch 285/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9378 - acc: 0.6669 - f1_micro: 0.6079 - val_loss: 1.5652 - val_acc: 0.7423 - val_f1_micro: 0.6079\n",
      "Epoch 286/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9384 - acc: 0.6668 - f1_micro: 0.6080 - val_loss: 1.5711 - val_acc: 0.7321 - val_f1_micro: 0.6080\n",
      "Epoch 287/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9492 - acc: 0.6669 - f1_micro: 0.6080 - val_loss: 1.5639 - val_acc: 0.7327 - val_f1_micro: 0.6080\n",
      "Epoch 288/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9482 - acc: 0.6648 - f1_micro: 0.6080 - val_loss: 1.5648 - val_acc: 0.7343 - val_f1_micro: 0.6080\n",
      "Epoch 289/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9445 - acc: 0.6673 - f1_micro: 0.6081 - val_loss: 1.5630 - val_acc: 0.7333 - val_f1_micro: 0.6081\n",
      "Epoch 290/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9416 - acc: 0.6659 - f1_micro: 0.6081 - val_loss: 1.5563 - val_acc: 0.7385 - val_f1_micro: 0.6081\n",
      "Epoch 291/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9402 - acc: 0.6669 - f1_micro: 0.6081 - val_loss: 1.5658 - val_acc: 0.7333 - val_f1_micro: 0.6082\n",
      "Epoch 292/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9406 - acc: 0.6674 - f1_micro: 0.6082 - val_loss: 1.5715 - val_acc: 0.7329 - val_f1_micro: 0.6082\n",
      "Epoch 293/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9446 - acc: 0.6660 - f1_micro: 0.6082 - val_loss: 1.5678 - val_acc: 0.7364 - val_f1_micro: 0.6082\n",
      "Epoch 294/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9450 - acc: 0.6652 - f1_micro: 0.6082 - val_loss: 1.5595 - val_acc: 0.7319 - val_f1_micro: 0.6083\n",
      "Epoch 295/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9401 - acc: 0.6664 - f1_micro: 0.6083 - val_loss: 1.5669 - val_acc: 0.7327 - val_f1_micro: 0.6083\n",
      "Epoch 296/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9477 - acc: 0.6675 - f1_micro: 0.6083 - val_loss: 1.5631 - val_acc: 0.7353 - val_f1_micro: 0.6083\n",
      "Epoch 297/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9450 - acc: 0.6668 - f1_micro: 0.6084 - val_loss: 1.5650 - val_acc: 0.7337 - val_f1_micro: 0.6084\n",
      "Epoch 298/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9380 - acc: 0.6686 - f1_micro: 0.6084 - val_loss: 1.5588 - val_acc: 0.7377 - val_f1_micro: 0.6084\n",
      "Epoch 299/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9410 - acc: 0.6669 - f1_micro: 0.6084 - val_loss: 1.5652 - val_acc: 0.7353 - val_f1_micro: 0.6085\n",
      "Epoch 300/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9437 - acc: 0.6694 - f1_micro: 0.6085 - val_loss: 1.5671 - val_acc: 0.7313 - val_f1_micro: 0.6085\n",
      "Epoch 301/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9391 - acc: 0.6674 - f1_micro: 0.6085 - val_loss: 1.5580 - val_acc: 0.7315 - val_f1_micro: 0.6085\n",
      "Epoch 302/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9490 - acc: 0.6678 - f1_micro: 0.6085 - val_loss: 1.5625 - val_acc: 0.7320 - val_f1_micro: 0.6086\n",
      "Epoch 303/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9368 - acc: 0.6684 - f1_micro: 0.6086 - val_loss: 1.5662 - val_acc: 0.7332 - val_f1_micro: 0.6086\n",
      "Epoch 304/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9404 - acc: 0.6653 - f1_micro: 0.6086 - val_loss: 1.5624 - val_acc: 0.7364 - val_f1_micro: 0.6086\n",
      "Epoch 305/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9438 - acc: 0.6683 - f1_micro: 0.6087 - val_loss: 1.5590 - val_acc: 0.7322 - val_f1_micro: 0.6087\n",
      "Epoch 306/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9403 - acc: 0.6670 - f1_micro: 0.6087 - val_loss: 1.5568 - val_acc: 0.7381 - val_f1_micro: 0.6087\n",
      "Epoch 307/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9389 - acc: 0.6682 - f1_micro: 0.6087 - val_loss: 1.5593 - val_acc: 0.7366 - val_f1_micro: 0.6088\n",
      "Epoch 308/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9412 - acc: 0.6689 - f1_micro: 0.6088 - val_loss: 1.5626 - val_acc: 0.7381 - val_f1_micro: 0.6088\n",
      "Epoch 309/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9412 - acc: 0.6654 - f1_micro: 0.6088 - val_loss: 1.5603 - val_acc: 0.7348 - val_f1_micro: 0.6088\n",
      "Epoch 310/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9320 - acc: 0.6672 - f1_micro: 0.6089 - val_loss: 1.5541 - val_acc: 0.7433 - val_f1_micro: 0.6089\n",
      "Epoch 311/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9409 - acc: 0.6671 - f1_micro: 0.6089 - val_loss: 1.5599 - val_acc: 0.7372 - val_f1_micro: 0.6089\n",
      "Epoch 312/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9412 - acc: 0.6655 - f1_micro: 0.6089 - val_loss: 1.5602 - val_acc: 0.7296 - val_f1_micro: 0.6090\n",
      "Epoch 313/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9414 - acc: 0.6685 - f1_micro: 0.6090 - val_loss: 1.5636 - val_acc: 0.7377 - val_f1_micro: 0.6090\n",
      "Epoch 314/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9440 - acc: 0.6662 - f1_micro: 0.6090 - val_loss: 1.5571 - val_acc: 0.7369 - val_f1_micro: 0.6090\n",
      "Epoch 315/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9369 - acc: 0.6669 - f1_micro: 0.6091 - val_loss: 1.5594 - val_acc: 0.7400 - val_f1_micro: 0.6091\n",
      "Epoch 316/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9387 - acc: 0.6688 - f1_micro: 0.6091 - val_loss: 1.5572 - val_acc: 0.7349 - val_f1_micro: 0.6091\n",
      "Epoch 317/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9378 - acc: 0.6662 - f1_micro: 0.6091 - val_loss: 1.5520 - val_acc: 0.7282 - val_f1_micro: 0.6092\n",
      "Epoch 318/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9387 - acc: 0.6689 - f1_micro: 0.6092 - val_loss: 1.5563 - val_acc: 0.7341 - val_f1_micro: 0.6092\n",
      "Epoch 319/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9403 - acc: 0.6698 - f1_micro: 0.6092 - val_loss: 1.5559 - val_acc: 0.7311 - val_f1_micro: 0.6092\n",
      "Epoch 320/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9387 - acc: 0.6665 - f1_micro: 0.6092 - val_loss: 1.5602 - val_acc: 0.7339 - val_f1_micro: 0.6093\n",
      "Epoch 321/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9364 - acc: 0.6661 - f1_micro: 0.6093 - val_loss: 1.5582 - val_acc: 0.7417 - val_f1_micro: 0.6093\n",
      "Epoch 322/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9439 - acc: 0.6673 - f1_micro: 0.6093 - val_loss: 1.5577 - val_acc: 0.7337 - val_f1_micro: 0.6093\n",
      "Epoch 323/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9378 - acc: 0.6676 - f1_micro: 0.6093 - val_loss: 1.5607 - val_acc: 0.7295 - val_f1_micro: 0.6094\n",
      "Epoch 324/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9459 - acc: 0.6675 - f1_micro: 0.6094 - val_loss: 1.5607 - val_acc: 0.7351 - val_f1_micro: 0.6094\n",
      "Epoch 325/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9437 - acc: 0.6668 - f1_micro: 0.6094 - val_loss: 1.5516 - val_acc: 0.7326 - val_f1_micro: 0.6094\n",
      "Epoch 326/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9395 - acc: 0.6664 - f1_micro: 0.6095 - val_loss: 1.5528 - val_acc: 0.7320 - val_f1_micro: 0.6095\n",
      "Epoch 327/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9452 - acc: 0.6671 - f1_micro: 0.6095 - val_loss: 1.5602 - val_acc: 0.7446 - val_f1_micro: 0.6095\n",
      "Epoch 328/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9411 - acc: 0.6674 - f1_micro: 0.6095 - val_loss: 1.5561 - val_acc: 0.7377 - val_f1_micro: 0.6095\n",
      "Epoch 329/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9387 - acc: 0.6665 - f1_micro: 0.6096 - val_loss: 1.5583 - val_acc: 0.7297 - val_f1_micro: 0.6096\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9382 - acc: 0.6660 - f1_micro: 0.6096 - val_loss: 1.5536 - val_acc: 0.7340 - val_f1_micro: 0.6096\n",
      "Epoch 331/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9355 - acc: 0.6673 - f1_micro: 0.6096 - val_loss: 1.5539 - val_acc: 0.7418 - val_f1_micro: 0.6097\n",
      "Epoch 332/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9371 - acc: 0.6668 - f1_micro: 0.6097 - val_loss: 1.5583 - val_acc: 0.7386 - val_f1_micro: 0.6097\n",
      "Epoch 333/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9387 - acc: 0.6654 - f1_micro: 0.6097 - val_loss: 1.5567 - val_acc: 0.7424 - val_f1_micro: 0.6097\n",
      "Epoch 334/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9370 - acc: 0.6683 - f1_micro: 0.6097 - val_loss: 1.5540 - val_acc: 0.7377 - val_f1_micro: 0.6098\n",
      "Epoch 335/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9346 - acc: 0.6678 - f1_micro: 0.6098 - val_loss: 1.5568 - val_acc: 0.7340 - val_f1_micro: 0.6098\n",
      "Epoch 336/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9418 - acc: 0.6687 - f1_micro: 0.6098 - val_loss: 1.5577 - val_acc: 0.7383 - val_f1_micro: 0.6098\n",
      "Epoch 337/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9394 - acc: 0.6669 - f1_micro: 0.6099 - val_loss: 1.5571 - val_acc: 0.7378 - val_f1_micro: 0.6099\n",
      "Epoch 338/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9376 - acc: 0.6660 - f1_micro: 0.6099 - val_loss: 1.5510 - val_acc: 0.7407 - val_f1_micro: 0.6099\n",
      "Epoch 339/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9403 - acc: 0.6667 - f1_micro: 0.6099 - val_loss: 1.5492 - val_acc: 0.7375 - val_f1_micro: 0.6099\n",
      "Epoch 340/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9373 - acc: 0.6675 - f1_micro: 0.6100 - val_loss: 1.5610 - val_acc: 0.7374 - val_f1_micro: 0.6100\n",
      "Epoch 341/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9383 - acc: 0.6701 - f1_micro: 0.6100 - val_loss: 1.5568 - val_acc: 0.7354 - val_f1_micro: 0.6100\n",
      "Epoch 342/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9313 - acc: 0.6674 - f1_micro: 0.6100 - val_loss: 1.5544 - val_acc: 0.7356 - val_f1_micro: 0.6101\n",
      "Epoch 343/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9388 - acc: 0.6690 - f1_micro: 0.6101 - val_loss: 1.5631 - val_acc: 0.7372 - val_f1_micro: 0.6101\n",
      "Epoch 344/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9342 - acc: 0.6698 - f1_micro: 0.6101 - val_loss: 1.5596 - val_acc: 0.7427 - val_f1_micro: 0.6101\n",
      "Epoch 345/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9427 - acc: 0.6668 - f1_micro: 0.6102 - val_loss: 1.5581 - val_acc: 0.7456 - val_f1_micro: 0.6102\n",
      "Epoch 346/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9318 - acc: 0.6699 - f1_micro: 0.6102 - val_loss: 1.5538 - val_acc: 0.7356 - val_f1_micro: 0.6102\n",
      "Epoch 347/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9390 - acc: 0.6689 - f1_micro: 0.6102 - val_loss: 1.5579 - val_acc: 0.7358 - val_f1_micro: 0.6103\n",
      "Epoch 348/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9366 - acc: 0.6670 - f1_micro: 0.6103 - val_loss: 1.5501 - val_acc: 0.7356 - val_f1_micro: 0.6103\n",
      "Epoch 349/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9403 - acc: 0.6665 - f1_micro: 0.6103 - val_loss: 1.5579 - val_acc: 0.7353 - val_f1_micro: 0.6103\n",
      "Epoch 350/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9340 - acc: 0.6689 - f1_micro: 0.6103 - val_loss: 1.5538 - val_acc: 0.7342 - val_f1_micro: 0.6104\n",
      "Epoch 351/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9323 - acc: 0.6661 - f1_micro: 0.6104 - val_loss: 1.5489 - val_acc: 0.7356 - val_f1_micro: 0.6104\n",
      "Epoch 352/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9417 - acc: 0.6674 - f1_micro: 0.6104 - val_loss: 1.5511 - val_acc: 0.7398 - val_f1_micro: 0.6104\n",
      "Epoch 353/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9390 - acc: 0.6682 - f1_micro: 0.6104 - val_loss: 1.5546 - val_acc: 0.7415 - val_f1_micro: 0.6105\n",
      "Epoch 354/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9329 - acc: 0.6668 - f1_micro: 0.6105 - val_loss: 1.5504 - val_acc: 0.7402 - val_f1_micro: 0.6105\n",
      "Epoch 355/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9346 - acc: 0.6674 - f1_micro: 0.6105 - val_loss: 1.5519 - val_acc: 0.7376 - val_f1_micro: 0.6105\n",
      "Epoch 356/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9328 - acc: 0.6699 - f1_micro: 0.6106 - val_loss: 1.5509 - val_acc: 0.7384 - val_f1_micro: 0.6106\n",
      "Epoch 357/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9308 - acc: 0.6681 - f1_micro: 0.6106 - val_loss: 1.5516 - val_acc: 0.7409 - val_f1_micro: 0.6106\n",
      "Epoch 358/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9350 - acc: 0.6685 - f1_micro: 0.6106 - val_loss: 1.5548 - val_acc: 0.7391 - val_f1_micro: 0.6106\n",
      "Epoch 359/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9321 - acc: 0.6696 - f1_micro: 0.6107 - val_loss: 1.5558 - val_acc: 0.7309 - val_f1_micro: 0.6107\n",
      "Epoch 360/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9274 - acc: 0.6698 - f1_micro: 0.6107 - val_loss: 1.5508 - val_acc: 0.7434 - val_f1_micro: 0.6107\n",
      "Epoch 361/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9320 - acc: 0.6677 - f1_micro: 0.6107 - val_loss: 1.5509 - val_acc: 0.7389 - val_f1_micro: 0.6107\n",
      "Epoch 362/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9361 - acc: 0.6677 - f1_micro: 0.6108 - val_loss: 1.5504 - val_acc: 0.7383 - val_f1_micro: 0.6108\n",
      "Epoch 363/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9338 - acc: 0.6681 - f1_micro: 0.6108 - val_loss: 1.5501 - val_acc: 0.7319 - val_f1_micro: 0.6108\n",
      "Epoch 364/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9321 - acc: 0.6679 - f1_micro: 0.6108 - val_loss: 1.5548 - val_acc: 0.7364 - val_f1_micro: 0.6108\n",
      "Epoch 365/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9375 - acc: 0.6686 - f1_micro: 0.6109 - val_loss: 1.5523 - val_acc: 0.7292 - val_f1_micro: 0.6109\n",
      "Epoch 366/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9319 - acc: 0.6668 - f1_micro: 0.6109 - val_loss: 1.5512 - val_acc: 0.7400 - val_f1_micro: 0.6109\n",
      "Epoch 367/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9286 - acc: 0.6674 - f1_micro: 0.6109 - val_loss: 1.5498 - val_acc: 0.7404 - val_f1_micro: 0.6109\n",
      "Epoch 368/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9283 - acc: 0.6689 - f1_micro: 0.6110 - val_loss: 1.5464 - val_acc: 0.7333 - val_f1_micro: 0.6110\n",
      "Epoch 369/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9383 - acc: 0.6678 - f1_micro: 0.6110 - val_loss: 1.5500 - val_acc: 0.7436 - val_f1_micro: 0.6110\n",
      "Epoch 370/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9416 - acc: 0.6678 - f1_micro: 0.6110 - val_loss: 1.5525 - val_acc: 0.7346 - val_f1_micro: 0.6111\n",
      "Epoch 371/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9292 - acc: 0.6683 - f1_micro: 0.6111 - val_loss: 1.5518 - val_acc: 0.7367 - val_f1_micro: 0.6111\n",
      "Epoch 372/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9365 - acc: 0.6684 - f1_micro: 0.6111 - val_loss: 1.5538 - val_acc: 0.7331 - val_f1_micro: 0.6111\n",
      "Epoch 373/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9281 - acc: 0.6691 - f1_micro: 0.6111 - val_loss: 1.5520 - val_acc: 0.7361 - val_f1_micro: 0.6112\n",
      "Epoch 374/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9289 - acc: 0.6673 - f1_micro: 0.6112 - val_loss: 1.5472 - val_acc: 0.7322 - val_f1_micro: 0.6112\n",
      "Epoch 375/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9357 - acc: 0.6678 - f1_micro: 0.6112 - val_loss: 1.5514 - val_acc: 0.7311 - val_f1_micro: 0.6112\n",
      "Epoch 376/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9295 - acc: 0.6689 - f1_micro: 0.6113 - val_loss: 1.5513 - val_acc: 0.7366 - val_f1_micro: 0.6113\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9355 - acc: 0.6674 - f1_micro: 0.6113 - val_loss: 1.5527 - val_acc: 0.7386 - val_f1_micro: 0.6113\n",
      "Epoch 378/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9360 - acc: 0.6675 - f1_micro: 0.6113 - val_loss: 1.5453 - val_acc: 0.7324 - val_f1_micro: 0.6113\n",
      "Epoch 379/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9322 - acc: 0.6687 - f1_micro: 0.6114 - val_loss: 1.5494 - val_acc: 0.7364 - val_f1_micro: 0.6114\n",
      "Epoch 380/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9263 - acc: 0.6663 - f1_micro: 0.6114 - val_loss: 1.5507 - val_acc: 0.7332 - val_f1_micro: 0.6114\n",
      "Epoch 381/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9342 - acc: 0.6674 - f1_micro: 0.6114 - val_loss: 1.5489 - val_acc: 0.7364 - val_f1_micro: 0.6114\n",
      "Epoch 382/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9307 - acc: 0.6672 - f1_micro: 0.6115 - val_loss: 1.5546 - val_acc: 0.7398 - val_f1_micro: 0.6115\n",
      "Epoch 383/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9276 - acc: 0.6683 - f1_micro: 0.6115 - val_loss: 1.5523 - val_acc: 0.7334 - val_f1_micro: 0.6115\n",
      "Epoch 384/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9256 - acc: 0.6689 - f1_micro: 0.6115 - val_loss: 1.5503 - val_acc: 0.7360 - val_f1_micro: 0.6115\n",
      "Epoch 385/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9303 - acc: 0.6689 - f1_micro: 0.6116 - val_loss: 1.5455 - val_acc: 0.7371 - val_f1_micro: 0.6116\n",
      "Epoch 386/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9279 - acc: 0.6686 - f1_micro: 0.6116 - val_loss: 1.5499 - val_acc: 0.7383 - val_f1_micro: 0.6116\n",
      "Epoch 387/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9317 - acc: 0.6692 - f1_micro: 0.6116 - val_loss: 1.5502 - val_acc: 0.7371 - val_f1_micro: 0.6117\n",
      "Epoch 388/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9321 - acc: 0.6660 - f1_micro: 0.6117 - val_loss: 1.5539 - val_acc: 0.7353 - val_f1_micro: 0.6117\n",
      "Epoch 389/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9376 - acc: 0.6672 - f1_micro: 0.6117 - val_loss: 1.5513 - val_acc: 0.7374 - val_f1_micro: 0.6117\n",
      "Epoch 390/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9342 - acc: 0.6695 - f1_micro: 0.6117 - val_loss: 1.5488 - val_acc: 0.7358 - val_f1_micro: 0.6118\n",
      "Epoch 391/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9383 - acc: 0.6674 - f1_micro: 0.6118 - val_loss: 1.5548 - val_acc: 0.7378 - val_f1_micro: 0.6118\n",
      "Epoch 392/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9358 - acc: 0.6671 - f1_micro: 0.6118 - val_loss: 1.5536 - val_acc: 0.7441 - val_f1_micro: 0.6118\n",
      "Epoch 393/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9362 - acc: 0.6668 - f1_micro: 0.6118 - val_loss: 1.5560 - val_acc: 0.7391 - val_f1_micro: 0.6119\n",
      "Epoch 394/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9257 - acc: 0.6676 - f1_micro: 0.6119 - val_loss: 1.5449 - val_acc: 0.7376 - val_f1_micro: 0.6119\n",
      "Epoch 395/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9248 - acc: 0.6705 - f1_micro: 0.6119 - val_loss: 1.5496 - val_acc: 0.7381 - val_f1_micro: 0.6119\n",
      "Epoch 396/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9273 - acc: 0.6686 - f1_micro: 0.6119 - val_loss: 1.5476 - val_acc: 0.7374 - val_f1_micro: 0.6120\n",
      "Epoch 397/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9289 - acc: 0.6676 - f1_micro: 0.6120 - val_loss: 1.5512 - val_acc: 0.7371 - val_f1_micro: 0.6120\n",
      "Epoch 398/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9336 - acc: 0.6680 - f1_micro: 0.6120 - val_loss: 1.5442 - val_acc: 0.7372 - val_f1_micro: 0.6120\n",
      "Epoch 399/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9239 - acc: 0.6685 - f1_micro: 0.6121 - val_loss: 1.5500 - val_acc: 0.7385 - val_f1_micro: 0.6121\n",
      "Epoch 400/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9415 - acc: 0.6664 - f1_micro: 0.6121 - val_loss: 1.5535 - val_acc: 0.7321 - val_f1_micro: 0.6121\n",
      "Epoch 401/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9219 - acc: 0.6695 - f1_micro: 0.6121 - val_loss: 1.5501 - val_acc: 0.7392 - val_f1_micro: 0.6121\n",
      "Epoch 402/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9363 - acc: 0.6671 - f1_micro: 0.6122 - val_loss: 1.5461 - val_acc: 0.7440 - val_f1_micro: 0.6122\n",
      "Epoch 403/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9338 - acc: 0.6681 - f1_micro: 0.6122 - val_loss: 1.5465 - val_acc: 0.7411 - val_f1_micro: 0.6122\n",
      "Epoch 404/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9315 - acc: 0.6668 - f1_micro: 0.6122 - val_loss: 1.5531 - val_acc: 0.7438 - val_f1_micro: 0.6122\n",
      "Epoch 405/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9281 - acc: 0.6659 - f1_micro: 0.6123 - val_loss: 1.5495 - val_acc: 0.7395 - val_f1_micro: 0.6123\n",
      "Epoch 406/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9303 - acc: 0.6679 - f1_micro: 0.6123 - val_loss: 1.5502 - val_acc: 0.7410 - val_f1_micro: 0.6123\n",
      "Epoch 407/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9295 - acc: 0.6676 - f1_micro: 0.6123 - val_loss: 1.5458 - val_acc: 0.7366 - val_f1_micro: 0.6123\n",
      "Epoch 408/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9279 - acc: 0.6659 - f1_micro: 0.6124 - val_loss: 1.5454 - val_acc: 0.7402 - val_f1_micro: 0.6124\n",
      "Epoch 409/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9306 - acc: 0.6686 - f1_micro: 0.6124 - val_loss: 1.5478 - val_acc: 0.7380 - val_f1_micro: 0.6124\n",
      "Epoch 410/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9290 - acc: 0.6692 - f1_micro: 0.6124 - val_loss: 1.5419 - val_acc: 0.7396 - val_f1_micro: 0.6125\n",
      "Epoch 411/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9297 - acc: 0.6667 - f1_micro: 0.6125 - val_loss: 1.5487 - val_acc: 0.7373 - val_f1_micro: 0.6125\n",
      "Epoch 412/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9282 - acc: 0.6697 - f1_micro: 0.6125 - val_loss: 1.5502 - val_acc: 0.7348 - val_f1_micro: 0.6125\n",
      "Epoch 413/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9275 - acc: 0.6691 - f1_micro: 0.6125 - val_loss: 1.5496 - val_acc: 0.7383 - val_f1_micro: 0.6126\n",
      "Epoch 414/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9236 - acc: 0.6690 - f1_micro: 0.6126 - val_loss: 1.5478 - val_acc: 0.7390 - val_f1_micro: 0.6126\n",
      "Epoch 415/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9265 - acc: 0.6699 - f1_micro: 0.6126 - val_loss: 1.5428 - val_acc: 0.7321 - val_f1_micro: 0.6126\n",
      "Epoch 416/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9333 - acc: 0.6674 - f1_micro: 0.6126 - val_loss: 1.5515 - val_acc: 0.7345 - val_f1_micro: 0.6127\n",
      "Epoch 417/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9346 - acc: 0.6675 - f1_micro: 0.6127 - val_loss: 1.5420 - val_acc: 0.7454 - val_f1_micro: 0.6127\n",
      "Epoch 418/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9289 - acc: 0.6679 - f1_micro: 0.6127 - val_loss: 1.5463 - val_acc: 0.7382 - val_f1_micro: 0.6127\n",
      "Epoch 419/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9268 - acc: 0.6675 - f1_micro: 0.6127 - val_loss: 1.5386 - val_acc: 0.7353 - val_f1_micro: 0.6127\n",
      "Epoch 420/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9256 - acc: 0.6682 - f1_micro: 0.6128 - val_loss: 1.5468 - val_acc: 0.7341 - val_f1_micro: 0.6128\n",
      "Epoch 421/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9263 - acc: 0.6693 - f1_micro: 0.6128 - val_loss: 1.5421 - val_acc: 0.7360 - val_f1_micro: 0.6128\n",
      "Epoch 422/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9275 - acc: 0.6693 - f1_micro: 0.6128 - val_loss: 1.5402 - val_acc: 0.7464 - val_f1_micro: 0.6129\n",
      "Epoch 423/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9358 - acc: 0.6686 - f1_micro: 0.6129 - val_loss: 1.5441 - val_acc: 0.7421 - val_f1_micro: 0.6129\n",
      "Epoch 424/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9322 - acc: 0.6677 - f1_micro: 0.6129 - val_loss: 1.5493 - val_acc: 0.7356 - val_f1_micro: 0.6129\n",
      "Epoch 425/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9247 - acc: 0.6686 - f1_micro: 0.6129 - val_loss: 1.5395 - val_acc: 0.7369 - val_f1_micro: 0.6130\n",
      "Epoch 426/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9246 - acc: 0.6687 - f1_micro: 0.6130 - val_loss: 1.5461 - val_acc: 0.7385 - val_f1_micro: 0.6130\n",
      "Epoch 427/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9336 - acc: 0.6681 - f1_micro: 0.6130 - val_loss: 1.5445 - val_acc: 0.7289 - val_f1_micro: 0.6130\n",
      "Epoch 428/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9300 - acc: 0.6696 - f1_micro: 0.6130 - val_loss: 1.5430 - val_acc: 0.7351 - val_f1_micro: 0.6131\n",
      "Epoch 429/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9281 - acc: 0.6698 - f1_micro: 0.6131 - val_loss: 1.5483 - val_acc: 0.7376 - val_f1_micro: 0.6131\n",
      "Epoch 430/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9296 - acc: 0.6681 - f1_micro: 0.6131 - val_loss: 1.5441 - val_acc: 0.7342 - val_f1_micro: 0.6131\n",
      "Epoch 431/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9256 - acc: 0.6709 - f1_micro: 0.6131 - val_loss: 1.5539 - val_acc: 0.7366 - val_f1_micro: 0.6132\n",
      "Epoch 432/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9282 - acc: 0.6687 - f1_micro: 0.6132 - val_loss: 1.5443 - val_acc: 0.7343 - val_f1_micro: 0.6132\n",
      "Epoch 433/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9361 - acc: 0.6661 - f1_micro: 0.6132 - val_loss: 1.5446 - val_acc: 0.7270 - val_f1_micro: 0.6132\n",
      "Epoch 434/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9278 - acc: 0.6690 - f1_micro: 0.6132 - val_loss: 1.5472 - val_acc: 0.7330 - val_f1_micro: 0.6132\n",
      "Epoch 435/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9186 - acc: 0.6681 - f1_micro: 0.6133 - val_loss: 1.5410 - val_acc: 0.7415 - val_f1_micro: 0.6133\n",
      "Epoch 436/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9203 - acc: 0.6687 - f1_micro: 0.6133 - val_loss: 1.5425 - val_acc: 0.7349 - val_f1_micro: 0.6133\n",
      "Epoch 437/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9248 - acc: 0.6680 - f1_micro: 0.6133 - val_loss: 1.5425 - val_acc: 0.7407 - val_f1_micro: 0.6133\n",
      "Epoch 438/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9223 - acc: 0.6672 - f1_micro: 0.6134 - val_loss: 1.5458 - val_acc: 0.7366 - val_f1_micro: 0.6134\n",
      "Epoch 439/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9214 - acc: 0.6701 - f1_micro: 0.6134 - val_loss: 1.5432 - val_acc: 0.7395 - val_f1_micro: 0.6134\n",
      "Epoch 440/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9303 - acc: 0.6680 - f1_micro: 0.6134 - val_loss: 1.5412 - val_acc: 0.7431 - val_f1_micro: 0.6134\n",
      "Epoch 441/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9284 - acc: 0.6706 - f1_micro: 0.6134 - val_loss: 1.5452 - val_acc: 0.7355 - val_f1_micro: 0.6135\n",
      "Epoch 442/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9273 - acc: 0.6720 - f1_micro: 0.6135 - val_loss: 1.5504 - val_acc: 0.7400 - val_f1_micro: 0.6135\n",
      "Epoch 443/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9252 - acc: 0.6706 - f1_micro: 0.6135 - val_loss: 1.5459 - val_acc: 0.7423 - val_f1_micro: 0.6135\n",
      "Epoch 444/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9268 - acc: 0.6685 - f1_micro: 0.6135 - val_loss: 1.5418 - val_acc: 0.7415 - val_f1_micro: 0.6135\n",
      "Epoch 445/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9167 - acc: 0.6718 - f1_micro: 0.6136 - val_loss: 1.5429 - val_acc: 0.7389 - val_f1_micro: 0.6136\n",
      "Epoch 446/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9163 - acc: 0.6693 - f1_micro: 0.6136 - val_loss: 1.5427 - val_acc: 0.7384 - val_f1_micro: 0.6136\n",
      "Epoch 447/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9265 - acc: 0.6682 - f1_micro: 0.6136 - val_loss: 1.5417 - val_acc: 0.7389 - val_f1_micro: 0.6136\n",
      "Epoch 448/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9295 - acc: 0.6676 - f1_micro: 0.6137 - val_loss: 1.5434 - val_acc: 0.7388 - val_f1_micro: 0.6137\n",
      "Epoch 449/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9295 - acc: 0.6687 - f1_micro: 0.6137 - val_loss: 1.5465 - val_acc: 0.7376 - val_f1_micro: 0.6137\n",
      "Epoch 450/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9251 - acc: 0.6681 - f1_micro: 0.6137 - val_loss: 1.5446 - val_acc: 0.7426 - val_f1_micro: 0.6138\n",
      "Epoch 451/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9204 - acc: 0.6688 - f1_micro: 0.6138 - val_loss: 1.5395 - val_acc: 0.7346 - val_f1_micro: 0.6138\n",
      "Epoch 452/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9211 - acc: 0.6686 - f1_micro: 0.6138 - val_loss: 1.5448 - val_acc: 0.7362 - val_f1_micro: 0.6138\n",
      "Epoch 453/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9258 - acc: 0.6685 - f1_micro: 0.6138 - val_loss: 1.5438 - val_acc: 0.7358 - val_f1_micro: 0.6139\n",
      "Epoch 454/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9310 - acc: 0.6693 - f1_micro: 0.6139 - val_loss: 1.5396 - val_acc: 0.7364 - val_f1_micro: 0.6139\n",
      "Epoch 455/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9259 - acc: 0.6701 - f1_micro: 0.6139 - val_loss: 1.5486 - val_acc: 0.7366 - val_f1_micro: 0.6139\n",
      "Epoch 456/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9276 - acc: 0.6683 - f1_micro: 0.6139 - val_loss: 1.5457 - val_acc: 0.7397 - val_f1_micro: 0.6140\n",
      "Epoch 457/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9346 - acc: 0.6690 - f1_micro: 0.6140 - val_loss: 1.5432 - val_acc: 0.7399 - val_f1_micro: 0.6140\n",
      "Epoch 458/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9270 - acc: 0.6683 - f1_micro: 0.6140 - val_loss: 1.5443 - val_acc: 0.7383 - val_f1_micro: 0.6140\n",
      "Epoch 459/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9202 - acc: 0.6686 - f1_micro: 0.6140 - val_loss: 1.5401 - val_acc: 0.7414 - val_f1_micro: 0.6140\n",
      "Epoch 460/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9260 - acc: 0.6697 - f1_micro: 0.6141 - val_loss: 1.5427 - val_acc: 0.7373 - val_f1_micro: 0.6141\n",
      "Epoch 461/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9235 - acc: 0.6677 - f1_micro: 0.6141 - val_loss: 1.5529 - val_acc: 0.7386 - val_f1_micro: 0.6141\n",
      "Epoch 462/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9244 - acc: 0.6680 - f1_micro: 0.6141 - val_loss: 1.5447 - val_acc: 0.7435 - val_f1_micro: 0.6142\n",
      "Epoch 463/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9214 - acc: 0.6685 - f1_micro: 0.6142 - val_loss: 1.5455 - val_acc: 0.7412 - val_f1_micro: 0.6142\n",
      "Epoch 464/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9185 - acc: 0.6702 - f1_micro: 0.6142 - val_loss: 1.5381 - val_acc: 0.7397 - val_f1_micro: 0.6142\n",
      "Epoch 465/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9196 - acc: 0.6708 - f1_micro: 0.6142 - val_loss: 1.5381 - val_acc: 0.7360 - val_f1_micro: 0.6143\n",
      "Epoch 466/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9271 - acc: 0.6681 - f1_micro: 0.6143 - val_loss: 1.5453 - val_acc: 0.7378 - val_f1_micro: 0.6143\n",
      "Epoch 467/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9247 - acc: 0.6690 - f1_micro: 0.6143 - val_loss: 1.5424 - val_acc: 0.7428 - val_f1_micro: 0.6143\n",
      "Epoch 468/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9222 - acc: 0.6688 - f1_micro: 0.6143 - val_loss: 1.5412 - val_acc: 0.7391 - val_f1_micro: 0.6144\n",
      "Epoch 469/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9249 - acc: 0.6689 - f1_micro: 0.6144 - val_loss: 1.5405 - val_acc: 0.7393 - val_f1_micro: 0.6144\n",
      "Epoch 470/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9184 - acc: 0.6709 - f1_micro: 0.6144 - val_loss: 1.5440 - val_acc: 0.7411 - val_f1_micro: 0.6144\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9251 - acc: 0.6705 - f1_micro: 0.6144 - val_loss: 1.5362 - val_acc: 0.7346 - val_f1_micro: 0.6145\n",
      "Epoch 472/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9279 - acc: 0.6674 - f1_micro: 0.6145 - val_loss: 1.5371 - val_acc: 0.7249 - val_f1_micro: 0.6145\n",
      "Epoch 473/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9209 - acc: 0.6676 - f1_micro: 0.6145 - val_loss: 1.5382 - val_acc: 0.7424 - val_f1_micro: 0.6145\n",
      "Epoch 474/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9207 - acc: 0.6697 - f1_micro: 0.6145 - val_loss: 1.5510 - val_acc: 0.7319 - val_f1_micro: 0.6146\n",
      "Epoch 475/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9285 - acc: 0.6690 - f1_micro: 0.6146 - val_loss: 1.5460 - val_acc: 0.7472 - val_f1_micro: 0.6146\n",
      "Epoch 476/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9229 - acc: 0.6689 - f1_micro: 0.6146 - val_loss: 1.5466 - val_acc: 0.7310 - val_f1_micro: 0.6146\n",
      "Epoch 477/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9229 - acc: 0.6700 - f1_micro: 0.6146 - val_loss: 1.5445 - val_acc: 0.7429 - val_f1_micro: 0.6147\n",
      "Epoch 478/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9283 - acc: 0.6691 - f1_micro: 0.6147 - val_loss: 1.5398 - val_acc: 0.7418 - val_f1_micro: 0.6147\n",
      "Epoch 479/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9242 - acc: 0.6696 - f1_micro: 0.6147 - val_loss: 1.5453 - val_acc: 0.7408 - val_f1_micro: 0.6147\n",
      "Epoch 480/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9202 - acc: 0.6709 - f1_micro: 0.6147 - val_loss: 1.5341 - val_acc: 0.7365 - val_f1_micro: 0.6148\n",
      "Epoch 481/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9239 - acc: 0.6708 - f1_micro: 0.6148 - val_loss: 1.5348 - val_acc: 0.7351 - val_f1_micro: 0.6148\n",
      "Epoch 482/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9231 - acc: 0.6703 - f1_micro: 0.6148 - val_loss: 1.5376 - val_acc: 0.7380 - val_f1_micro: 0.6148\n",
      "Epoch 483/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9204 - acc: 0.6691 - f1_micro: 0.6148 - val_loss: 1.5377 - val_acc: 0.7392 - val_f1_micro: 0.6148\n",
      "Epoch 484/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9214 - acc: 0.6695 - f1_micro: 0.6149 - val_loss: 1.5394 - val_acc: 0.7372 - val_f1_micro: 0.6149\n",
      "Epoch 485/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9249 - acc: 0.6681 - f1_micro: 0.6149 - val_loss: 1.5378 - val_acc: 0.7298 - val_f1_micro: 0.6149\n",
      "Epoch 486/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9141 - acc: 0.6668 - f1_micro: 0.6149 - val_loss: 1.5365 - val_acc: 0.7429 - val_f1_micro: 0.6149\n",
      "Epoch 487/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9184 - acc: 0.6699 - f1_micro: 0.6150 - val_loss: 1.5374 - val_acc: 0.7371 - val_f1_micro: 0.6150\n",
      "Epoch 488/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9255 - acc: 0.6670 - f1_micro: 0.6150 - val_loss: 1.5366 - val_acc: 0.7404 - val_f1_micro: 0.6150\n",
      "Epoch 489/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9224 - acc: 0.6691 - f1_micro: 0.6150 - val_loss: 1.5375 - val_acc: 0.7428 - val_f1_micro: 0.6150\n",
      "Epoch 490/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9167 - acc: 0.6701 - f1_micro: 0.6150 - val_loss: 1.5397 - val_acc: 0.7368 - val_f1_micro: 0.6151\n",
      "Epoch 491/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9236 - acc: 0.6695 - f1_micro: 0.6151 - val_loss: 1.5395 - val_acc: 0.7442 - val_f1_micro: 0.6151\n",
      "Epoch 492/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9282 - acc: 0.6689 - f1_micro: 0.6151 - val_loss: 1.5443 - val_acc: 0.7413 - val_f1_micro: 0.6151\n",
      "Epoch 493/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9232 - acc: 0.6702 - f1_micro: 0.6151 - val_loss: 1.5458 - val_acc: 0.7357 - val_f1_micro: 0.6152\n",
      "Epoch 494/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9234 - acc: 0.6687 - f1_micro: 0.6152 - val_loss: 1.5354 - val_acc: 0.7428 - val_f1_micro: 0.6152\n",
      "Epoch 495/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9208 - acc: 0.6679 - f1_micro: 0.6152 - val_loss: 1.5355 - val_acc: 0.7336 - val_f1_micro: 0.6152\n",
      "Epoch 496/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9213 - acc: 0.6663 - f1_micro: 0.6152 - val_loss: 1.5383 - val_acc: 0.7313 - val_f1_micro: 0.6152\n",
      "Epoch 497/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9236 - acc: 0.6692 - f1_micro: 0.6153 - val_loss: 1.5357 - val_acc: 0.7324 - val_f1_micro: 0.6153\n",
      "Epoch 498/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9174 - acc: 0.6685 - f1_micro: 0.6153 - val_loss: 1.5435 - val_acc: 0.7355 - val_f1_micro: 0.6153\n",
      "Epoch 499/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9236 - acc: 0.6681 - f1_micro: 0.6153 - val_loss: 1.5450 - val_acc: 0.7348 - val_f1_micro: 0.6153\n",
      "Epoch 500/500\n",
      "94731/94731 [==============================] - 0s - loss: 1.9247 - acc: 0.6695 - f1_micro: 0.6153 - val_loss: 1.5416 - val_acc: 0.7385 - val_f1_micro: 0.6154\n",
      "CPU times: user 8min 52s, sys: 53.5 s, total: 9min 45s\n",
      "Wall time: 7min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# And trained it via:\n",
    "batch_size = 1000\n",
    "model.fit(\n",
    "    {'wv_input': wv_train, 'fs_input': fs_train},\n",
    "    {'main_output': y_train},\n",
    "    epochs=500, batch_size=batch_size,   # 500\n",
    "    validation_split=0.2,\n",
    "    validation_data=(\n",
    "        {'wv_input': wv_val, 'fs_input': fs_val},\n",
    "        {'main_output': y_val}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9223 - acc: 0.6675 - f1_micro: 0.6154 - val_loss: 1.5412 - val_acc: 0.7377 - val_f1_micro: 0.6154\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9187 - acc: 0.6692 - f1_micro: 0.6154 - val_loss: 1.5448 - val_acc: 0.7409 - val_f1_micro: 0.6154\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9205 - acc: 0.6683 - f1_micro: 0.6155 - val_loss: 1.5374 - val_acc: 0.7429 - val_f1_micro: 0.6155\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9227 - acc: 0.6703 - f1_micro: 0.6155 - val_loss: 1.5447 - val_acc: 0.7329 - val_f1_micro: 0.6155\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9203 - acc: 0.6685 - f1_micro: 0.6155 - val_loss: 1.5355 - val_acc: 0.7364 - val_f1_micro: 0.6155\n",
      "\n",
      "Done with epoch: 5\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9157 - acc: 0.6708 - f1_micro: 0.6156 - val_loss: 1.5441 - val_acc: 0.7309 - val_f1_micro: 0.6156\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9195 - acc: 0.6718 - f1_micro: 0.6156 - val_loss: 1.5377 - val_acc: 0.7392 - val_f1_micro: 0.6156\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9227 - acc: 0.6692 - f1_micro: 0.6156 - val_loss: 1.5405 - val_acc: 0.7402 - val_f1_micro: 0.6156\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9229 - acc: 0.6687 - f1_micro: 0.6157 - val_loss: 1.5405 - val_acc: 0.7365 - val_f1_micro: 0.6157\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9125 - acc: 0.6702 - f1_micro: 0.6157 - val_loss: 1.5456 - val_acc: 0.7384 - val_f1_micro: 0.6157\n",
      "\n",
      "Done with epoch: 10\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9177 - acc: 0.6720 - f1_micro: 0.6157 - val_loss: 1.5304 - val_acc: 0.7401 - val_f1_micro: 0.6157\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9179 - acc: 0.6688 - f1_micro: 0.6158 - val_loss: 1.5427 - val_acc: 0.7348 - val_f1_micro: 0.6158\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9193 - acc: 0.6691 - f1_micro: 0.6158 - val_loss: 1.5412 - val_acc: 0.7327 - val_f1_micro: 0.6158\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9191 - acc: 0.6695 - f1_micro: 0.6158 - val_loss: 1.5363 - val_acc: 0.7391 - val_f1_micro: 0.6158\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9168 - acc: 0.6708 - f1_micro: 0.6158 - val_loss: 1.5342 - val_acc: 0.7397 - val_f1_micro: 0.6159\n",
      "\n",
      "Done with epoch: 15\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9221 - acc: 0.6693 - f1_micro: 0.6159 - val_loss: 1.5335 - val_acc: 0.7367 - val_f1_micro: 0.6159\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9162 - acc: 0.6710 - f1_micro: 0.6159 - val_loss: 1.5338 - val_acc: 0.7372 - val_f1_micro: 0.6159\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9207 - acc: 0.6691 - f1_micro: 0.6159 - val_loss: 1.5289 - val_acc: 0.7334 - val_f1_micro: 0.6159\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9201 - acc: 0.6689 - f1_micro: 0.6160 - val_loss: 1.5354 - val_acc: 0.7397 - val_f1_micro: 0.6160\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9173 - acc: 0.6686 - f1_micro: 0.6160 - val_loss: 1.5293 - val_acc: 0.7378 - val_f1_micro: 0.6160\n",
      "\n",
      "Done with epoch: 20\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9213 - acc: 0.6697 - f1_micro: 0.6160 - val_loss: 1.5390 - val_acc: 0.7385 - val_f1_micro: 0.6160\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9134 - acc: 0.6703 - f1_micro: 0.6160 - val_loss: 1.5306 - val_acc: 0.7446 - val_f1_micro: 0.6161\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9137 - acc: 0.6691 - f1_micro: 0.6161 - val_loss: 1.5293 - val_acc: 0.7324 - val_f1_micro: 0.6161\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9200 - acc: 0.6687 - f1_micro: 0.6161 - val_loss: 1.5279 - val_acc: 0.7380 - val_f1_micro: 0.6161\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9173 - acc: 0.6707 - f1_micro: 0.6161 - val_loss: 1.5320 - val_acc: 0.7384 - val_f1_micro: 0.6161\n",
      "\n",
      "Done with epoch: 25\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9218 - acc: 0.6680 - f1_micro: 0.6162 - val_loss: 1.5398 - val_acc: 0.7353 - val_f1_micro: 0.6162\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9213 - acc: 0.6706 - f1_micro: 0.6162 - val_loss: 1.5378 - val_acc: 0.7366 - val_f1_micro: 0.6162\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9178 - acc: 0.6674 - f1_micro: 0.6162 - val_loss: 1.5325 - val_acc: 0.7420 - val_f1_micro: 0.6162\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9145 - acc: 0.6706 - f1_micro: 0.6162 - val_loss: 1.5377 - val_acc: 0.7373 - val_f1_micro: 0.6163\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9217 - acc: 0.6684 - f1_micro: 0.6163 - val_loss: 1.5379 - val_acc: 0.7366 - val_f1_micro: 0.6163\n",
      "\n",
      "Done with epoch: 30\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9244 - acc: 0.6682 - f1_micro: 0.6163 - val_loss: 1.5349 - val_acc: 0.7404 - val_f1_micro: 0.6163\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9196 - acc: 0.6697 - f1_micro: 0.6163 - val_loss: 1.5400 - val_acc: 0.7365 - val_f1_micro: 0.6163\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9243 - acc: 0.6687 - f1_micro: 0.6164 - val_loss: 1.5347 - val_acc: 0.7321 - val_f1_micro: 0.6164\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9224 - acc: 0.6673 - f1_micro: 0.6164 - val_loss: 1.5369 - val_acc: 0.7340 - val_f1_micro: 0.6164\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9201 - acc: 0.6716 - f1_micro: 0.6164 - val_loss: 1.5311 - val_acc: 0.7353 - val_f1_micro: 0.6164\n",
      "\n",
      "Done with epoch: 35\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9151 - acc: 0.6704 - f1_micro: 0.6164 - val_loss: 1.5329 - val_acc: 0.7430 - val_f1_micro: 0.6165\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9166 - acc: 0.6699 - f1_micro: 0.6165 - val_loss: 1.5355 - val_acc: 0.7363 - val_f1_micro: 0.6165\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9159 - acc: 0.6695 - f1_micro: 0.6165 - val_loss: 1.5343 - val_acc: 0.7384 - val_f1_micro: 0.6165\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9203 - acc: 0.6693 - f1_micro: 0.6165 - val_loss: 1.5319 - val_acc: 0.7421 - val_f1_micro: 0.6165\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9196 - acc: 0.6695 - f1_micro: 0.6166 - val_loss: 1.5358 - val_acc: 0.7420 - val_f1_micro: 0.6166\n",
      "\n",
      "Done with epoch: 40\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9214 - acc: 0.6685 - f1_micro: 0.6166 - val_loss: 1.5326 - val_acc: 0.7331 - val_f1_micro: 0.6166\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9204 - acc: 0.6691 - f1_micro: 0.6166 - val_loss: 1.5288 - val_acc: 0.7346 - val_f1_micro: 0.6166\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9157 - acc: 0.6710 - f1_micro: 0.6166 - val_loss: 1.5351 - val_acc: 0.7412 - val_f1_micro: 0.6167\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9197 - acc: 0.6702 - f1_micro: 0.6167 - val_loss: 1.5314 - val_acc: 0.7348 - val_f1_micro: 0.6167\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9189 - acc: 0.6673 - f1_micro: 0.6167 - val_loss: 1.5315 - val_acc: 0.7330 - val_f1_micro: 0.6167\n",
      "\n",
      "Done with epoch: 45\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9168 - acc: 0.6684 - f1_micro: 0.6167 - val_loss: 1.5317 - val_acc: 0.7375 - val_f1_micro: 0.6167\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9138 - acc: 0.6709 - f1_micro: 0.6168 - val_loss: 1.5307 - val_acc: 0.7391 - val_f1_micro: 0.6168\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9131 - acc: 0.6717 - f1_micro: 0.6168 - val_loss: 1.5264 - val_acc: 0.7419 - val_f1_micro: 0.6168\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9199 - acc: 0.6672 - f1_micro: 0.6168 - val_loss: 1.5316 - val_acc: 0.7385 - val_f1_micro: 0.6168\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9186 - acc: 0.6696 - f1_micro: 0.6168 - val_loss: 1.5284 - val_acc: 0.7426 - val_f1_micro: 0.6169\n",
      "\n",
      "Done with epoch: 50\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9110 - acc: 0.6696 - f1_micro: 0.6169 - val_loss: 1.5327 - val_acc: 0.7442 - val_f1_micro: 0.6169\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9129 - acc: 0.6703 - f1_micro: 0.6169 - val_loss: 1.5261 - val_acc: 0.7390 - val_f1_micro: 0.6169\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9144 - acc: 0.6704 - f1_micro: 0.6169 - val_loss: 1.5295 - val_acc: 0.7461 - val_f1_micro: 0.6169\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9168 - acc: 0.6730 - f1_micro: 0.6170 - val_loss: 1.5240 - val_acc: 0.7399 - val_f1_micro: 0.6170\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9155 - acc: 0.6707 - f1_micro: 0.6170 - val_loss: 1.5344 - val_acc: 0.7353 - val_f1_micro: 0.6170\n",
      "\n",
      "Done with epoch: 55\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9163 - acc: 0.6689 - f1_micro: 0.6170 - val_loss: 1.5346 - val_acc: 0.7369 - val_f1_micro: 0.6170\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9159 - acc: 0.6714 - f1_micro: 0.6170 - val_loss: 1.5308 - val_acc: 0.7423 - val_f1_micro: 0.6171\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9163 - acc: 0.6694 - f1_micro: 0.6171 - val_loss: 1.5292 - val_acc: 0.7455 - val_f1_micro: 0.6171\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9081 - acc: 0.6707 - f1_micro: 0.6171 - val_loss: 1.5238 - val_acc: 0.7374 - val_f1_micro: 0.6171\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9140 - acc: 0.6717 - f1_micro: 0.6171 - val_loss: 1.5328 - val_acc: 0.7412 - val_f1_micro: 0.6171\n",
      "\n",
      "Done with epoch: 60\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9186 - acc: 0.6683 - f1_micro: 0.6172 - val_loss: 1.5345 - val_acc: 0.7364 - val_f1_micro: 0.6172\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9187 - acc: 0.6680 - f1_micro: 0.6172 - val_loss: 1.5311 - val_acc: 0.7355 - val_f1_micro: 0.6172\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9183 - acc: 0.6688 - f1_micro: 0.6172 - val_loss: 1.5279 - val_acc: 0.7397 - val_f1_micro: 0.6172\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9157 - acc: 0.6700 - f1_micro: 0.6172 - val_loss: 1.5268 - val_acc: 0.7419 - val_f1_micro: 0.6173\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9132 - acc: 0.6700 - f1_micro: 0.6173 - val_loss: 1.5275 - val_acc: 0.7444 - val_f1_micro: 0.6173\n",
      "\n",
      "Done with epoch: 65\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9218 - acc: 0.6701 - f1_micro: 0.6173 - val_loss: 1.5284 - val_acc: 0.7375 - val_f1_micro: 0.6173\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9176 - acc: 0.6695 - f1_micro: 0.6173 - val_loss: 1.5287 - val_acc: 0.7425 - val_f1_micro: 0.6173\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9181 - acc: 0.6697 - f1_micro: 0.6174 - val_loss: 1.5254 - val_acc: 0.7304 - val_f1_micro: 0.6174\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9161 - acc: 0.6680 - f1_micro: 0.6174 - val_loss: 1.5287 - val_acc: 0.7375 - val_f1_micro: 0.6174\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9101 - acc: 0.6701 - f1_micro: 0.6174 - val_loss: 1.5288 - val_acc: 0.7424 - val_f1_micro: 0.6174\n",
      "\n",
      "Done with epoch: 70\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9076 - acc: 0.6702 - f1_micro: 0.6174 - val_loss: 1.5222 - val_acc: 0.7496 - val_f1_micro: 0.6175\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9161 - acc: 0.6712 - f1_micro: 0.6175 - val_loss: 1.5262 - val_acc: 0.7423 - val_f1_micro: 0.6175\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9112 - acc: 0.6719 - f1_micro: 0.6175 - val_loss: 1.5292 - val_acc: 0.7428 - val_f1_micro: 0.6175\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9248 - acc: 0.6686 - f1_micro: 0.6175 - val_loss: 1.5388 - val_acc: 0.7340 - val_f1_micro: 0.6175\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9124 - acc: 0.6708 - f1_micro: 0.6175 - val_loss: 1.5288 - val_acc: 0.7438 - val_f1_micro: 0.6176\n",
      "\n",
      "Done with epoch: 75\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9127 - acc: 0.6672 - f1_micro: 0.6176 - val_loss: 1.5232 - val_acc: 0.7396 - val_f1_micro: 0.6176\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9084 - acc: 0.6717 - f1_micro: 0.6176 - val_loss: 1.5325 - val_acc: 0.7345 - val_f1_micro: 0.6176\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9156 - acc: 0.6711 - f1_micro: 0.6176 - val_loss: 1.5301 - val_acc: 0.7365 - val_f1_micro: 0.6176\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9143 - acc: 0.6719 - f1_micro: 0.6176 - val_loss: 1.5262 - val_acc: 0.7413 - val_f1_micro: 0.6177\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9228 - acc: 0.6676 - f1_micro: 0.6177 - val_loss: 1.5310 - val_acc: 0.7364 - val_f1_micro: 0.6177\n",
      "\n",
      "Done with epoch: 80\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9089 - acc: 0.6713 - f1_micro: 0.6177 - val_loss: 1.5319 - val_acc: 0.7435 - val_f1_micro: 0.6177\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9145 - acc: 0.6706 - f1_micro: 0.6177 - val_loss: 1.5279 - val_acc: 0.7416 - val_f1_micro: 0.6177\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9085 - acc: 0.6704 - f1_micro: 0.6178 - val_loss: 1.5272 - val_acc: 0.7411 - val_f1_micro: 0.6178\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9151 - acc: 0.6681 - f1_micro: 0.6178 - val_loss: 1.5379 - val_acc: 0.7383 - val_f1_micro: 0.6178\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9109 - acc: 0.6725 - f1_micro: 0.6178 - val_loss: 1.5329 - val_acc: 0.7369 - val_f1_micro: 0.6178\n",
      "\n",
      "Done with epoch: 85\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9074 - acc: 0.6688 - f1_micro: 0.6178 - val_loss: 1.5381 - val_acc: 0.7372 - val_f1_micro: 0.6179\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9080 - acc: 0.6701 - f1_micro: 0.6179 - val_loss: 1.5309 - val_acc: 0.7388 - val_f1_micro: 0.6179\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9125 - acc: 0.6695 - f1_micro: 0.6179 - val_loss: 1.5329 - val_acc: 0.7421 - val_f1_micro: 0.6179\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9118 - acc: 0.6685 - f1_micro: 0.6179 - val_loss: 1.5257 - val_acc: 0.7449 - val_f1_micro: 0.6179\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9124 - acc: 0.6714 - f1_micro: 0.6180 - val_loss: 1.5223 - val_acc: 0.7432 - val_f1_micro: 0.6180\n",
      "\n",
      "Done with epoch: 90\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 1.9131 - acc: 0.6711 - f1_micro: 0.6180 - val_loss: 1.5285 - val_acc: 0.7411 - val_f1_micro: 0.6180\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9154 - acc: 0.6713 - f1_micro: 0.6180 - val_loss: 1.5232 - val_acc: 0.7392 - val_f1_micro: 0.6180\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9101 - acc: 0.6670 - f1_micro: 0.6181 - val_loss: 1.5253 - val_acc: 0.7344 - val_f1_micro: 0.6181\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9152 - acc: 0.6685 - f1_micro: 0.6181 - val_loss: 1.5293 - val_acc: 0.7348 - val_f1_micro: 0.6181\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9085 - acc: 0.6697 - f1_micro: 0.6181 - val_loss: 1.5267 - val_acc: 0.7393 - val_f1_micro: 0.6181\n",
      "\n",
      "Done with epoch: 95\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9121 - acc: 0.6704 - f1_micro: 0.6181 - val_loss: 1.5282 - val_acc: 0.7481 - val_f1_micro: 0.6182\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9227 - acc: 0.6704 - f1_micro: 0.6182 - val_loss: 1.5314 - val_acc: 0.7371 - val_f1_micro: 0.6182\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9129 - acc: 0.6697 - f1_micro: 0.6182 - val_loss: 1.5234 - val_acc: 0.7386 - val_f1_micro: 0.6182\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9084 - acc: 0.6714 - f1_micro: 0.6182 - val_loss: 1.5235 - val_acc: 0.7372 - val_f1_micro: 0.6182\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 1.9141 - acc: 0.6703 - f1_micro: 0.6183 - val_loss: 1.5295 - val_acc: 0.7438 - val_f1_micro: 0.6183\n",
      "\n",
      "Done with epoch: 100\n",
      "\n",
      "CPU times: user 1min 45s, sys: 10.6 s, total: 1min 56s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = 'models/tfidf-word2vec-fasttext_2010-2014-data_cat-crossentropy-2014-b-val-sc_tfidf_wv_fs.model'\n",
    "epochs = 5\n",
    "for i in xrange(0, 100 // epochs):\n",
    "    hist = model.fit(\n",
    "        {'wv_input': wv_train, 'fs_input': fs_train},\n",
    "        {'main_output': y_train},\n",
    "        epochs=epochs, batch_size=batch_size,   # 500\n",
    "        validation_split=0.2,\n",
    "        validation_data=(\n",
    "            {'wv_input': wv_val, 'fs_input': fs_val},\n",
    "            {'main_output': y_val}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save(model_name.format(i))\n",
    "    print\n",
    "    print('Done with epoch: {}'.format((i + 1) * epochs))\n",
    "    with open('lstm-word2vec-fasttext.epoch.csv', 'a') as fl:\n",
    "        fl.write(model_name + '\\n')\n",
    "        fl.write('Epoch {}\\n'.format((i + 1) * epochs))\n",
    "        fl.write('{}\\n'.format(datetime.now()))\n",
    "        fl.write('\\n'.join(['{}: {}'.format(k, v[0]) for k, v in hist.history.items()]))\n",
    "        fl.write('\\n\\n')\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.6427 - main_output_loss: 1.3490 - aux_output_loss: 1.4686 - main_output_acc: 0.7712 - main_output_f1_micro: 0.6905 - aux_output_acc: 0.7839 - aux_output_f1_micro: 0.1085 - val_loss: 1.4372 - val_main_output_loss: 1.1302 - val_aux_output_loss: 1.5348 - val_main_output_acc: 0.7941 - val_main_output_f1_micro: 0.6913 - val_aux_output_acc: 0.7884 - val_aux_output_f1_micro: 0.1086\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5833 - main_output_loss: 1.2980 - aux_output_loss: 1.4265 - main_output_acc: 0.7738 - main_output_f1_micro: 0.6921 - aux_output_acc: 0.7855 - aux_output_f1_micro: 0.1088 - val_loss: 1.4210 - val_main_output_loss: 1.1179 - val_aux_output_loss: 1.5153 - val_main_output_acc: 0.7967 - val_main_output_f1_micro: 0.6930 - val_aux_output_acc: 0.7949 - val_aux_output_f1_micro: 0.1090\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5656 - main_output_loss: 1.2841 - aux_output_loss: 1.4076 - main_output_acc: 0.7752 - main_output_f1_micro: 0.6938 - aux_output_acc: 0.7870 - aux_output_f1_micro: 0.1091 - val_loss: 1.4123 - val_main_output_loss: 1.1124 - val_aux_output_loss: 1.4995 - val_main_output_acc: 0.7930 - val_main_output_f1_micro: 0.6946 - val_aux_output_acc: 0.7916 - val_aux_output_f1_micro: 0.1093\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5551 - main_output_loss: 1.2755 - aux_output_loss: 1.3983 - main_output_acc: 0.7779 - main_output_f1_micro: 0.6955 - aux_output_acc: 0.7867 - aux_output_f1_micro: 0.1094 - val_loss: 1.4043 - val_main_output_loss: 1.1070 - val_aux_output_loss: 1.4866 - val_main_output_acc: 0.7988 - val_main_output_f1_micro: 0.6963 - val_aux_output_acc: 0.7918 - val_aux_output_f1_micro: 0.1096\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5504 - main_output_loss: 1.2725 - aux_output_loss: 1.3899 - main_output_acc: 0.7771 - main_output_f1_micro: 0.6971 - aux_output_acc: 0.7872 - aux_output_f1_micro: 0.1097 - val_loss: 1.4002 - val_main_output_loss: 1.1043 - val_aux_output_loss: 1.4796 - val_main_output_acc: 0.8088 - val_main_output_f1_micro: 0.6980 - val_aux_output_acc: 0.7916 - val_aux_output_f1_micro: 0.1099\n",
      "\n",
      "Done with epoch: 100\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5459 - main_output_loss: 1.2693 - aux_output_loss: 1.3833 - main_output_acc: 0.7773 - main_output_f1_micro: 0.6988 - aux_output_acc: 0.7896 - aux_output_f1_micro: 0.1100 - val_loss: 1.3938 - val_main_output_loss: 1.0999 - val_aux_output_loss: 1.4694 - val_main_output_acc: 0.8001 - val_main_output_f1_micro: 0.6996 - val_aux_output_acc: 0.7926 - val_aux_output_f1_micro: 0.1102\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5393 - main_output_loss: 1.2640 - aux_output_loss: 1.3764 - main_output_acc: 0.7776 - main_output_f1_micro: 0.7004 - aux_output_acc: 0.7896 - aux_output_f1_micro: 0.1103 - val_loss: 1.3912 - val_main_output_loss: 1.0979 - val_aux_output_loss: 1.4664 - val_main_output_acc: 0.7940 - val_main_output_f1_micro: 0.7012 - val_aux_output_acc: 0.7917 - val_aux_output_f1_micro: 0.1105\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5369 - main_output_loss: 1.2627 - aux_output_loss: 1.3708 - main_output_acc: 0.7815 - main_output_f1_micro: 0.7020 - aux_output_acc: 0.7890 - aux_output_f1_micro: 0.1106 - val_loss: 1.3821 - val_main_output_loss: 1.0902 - val_aux_output_loss: 1.4594 - val_main_output_acc: 0.8066 - val_main_output_f1_micro: 0.7028 - val_aux_output_acc: 0.7940 - val_aux_output_f1_micro: 0.1108\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5302 - main_output_loss: 1.2571 - aux_output_loss: 1.3656 - main_output_acc: 0.7796 - main_output_f1_micro: 0.7036 - aux_output_acc: 0.7897 - aux_output_f1_micro: 0.1109 - val_loss: 1.3878 - val_main_output_loss: 1.0954 - val_aux_output_loss: 1.4620 - val_main_output_acc: 0.7964 - val_main_output_f1_micro: 0.7044 - val_aux_output_acc: 0.7954 - val_aux_output_f1_micro: 0.1111\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5282 - main_output_loss: 1.2561 - aux_output_loss: 1.3605 - main_output_acc: 0.7775 - main_output_f1_micro: 0.7052 - aux_output_acc: 0.7913 - aux_output_f1_micro: 0.1112 - val_loss: 1.3855 - val_main_output_loss: 1.0954 - val_aux_output_loss: 1.4505 - val_main_output_acc: 0.7928 - val_main_output_f1_micro: 0.7059 - val_aux_output_acc: 0.7964 - val_aux_output_f1_micro: 0.1114\n",
      "\n",
      "Done with epoch: 105\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5262 - main_output_loss: 1.2551 - aux_output_loss: 1.3553 - main_output_acc: 0.7791 - main_output_f1_micro: 0.7067 - aux_output_acc: 0.7902 - aux_output_f1_micro: 0.1115 - val_loss: 1.3707 - val_main_output_loss: 1.0832 - val_aux_output_loss: 1.4377 - val_main_output_acc: 0.8034 - val_main_output_f1_micro: 0.7075 - val_aux_output_acc: 0.7948 - val_aux_output_f1_micro: 0.1117\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5205 - main_output_loss: 1.2508 - aux_output_loss: 1.3484 - main_output_acc: 0.7803 - main_output_f1_micro: 0.7083 - aux_output_acc: 0.7910 - aux_output_f1_micro: 0.1118 - val_loss: 1.3696 - val_main_output_loss: 1.0827 - val_aux_output_loss: 1.4348 - val_main_output_acc: 0.8018 - val_main_output_f1_micro: 0.7090 - val_aux_output_acc: 0.7929 - val_aux_output_f1_micro: 0.1120\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5220 - main_output_loss: 1.2527 - aux_output_loss: 1.3466 - main_output_acc: 0.7793 - main_output_f1_micro: 0.7098 - aux_output_acc: 0.7904 - aux_output_f1_micro: 0.1122 - val_loss: 1.3695 - val_main_output_loss: 1.0827 - val_aux_output_loss: 1.4341 - val_main_output_acc: 0.7963 - val_main_output_f1_micro: 0.7105 - val_aux_output_acc: 0.7926 - val_aux_output_f1_micro: 0.1123\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5216 - main_output_loss: 1.2530 - aux_output_loss: 1.3431 - main_output_acc: 0.7800 - main_output_f1_micro: 0.7113 - aux_output_acc: 0.7914 - aux_output_f1_micro: 0.1125 - val_loss: 1.3708 - val_main_output_loss: 1.0860 - val_aux_output_loss: 1.4240 - val_main_output_acc: 0.8011 - val_main_output_f1_micro: 0.7120 - val_aux_output_acc: 0.7966 - val_aux_output_f1_micro: 0.1126\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5163 - main_output_loss: 1.2486 - aux_output_loss: 1.3386 - main_output_acc: 0.7807 - main_output_f1_micro: 0.7128 - aux_output_acc: 0.7916 - aux_output_f1_micro: 0.1128 - val_loss: 1.3660 - val_main_output_loss: 1.0814 - val_aux_output_loss: 1.4232 - val_main_output_acc: 0.7929 - val_main_output_f1_micro: 0.7135 - val_aux_output_acc: 0.7930 - val_aux_output_f1_micro: 0.1130\n",
      "\n",
      "Done with epoch: 110\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5152 - main_output_loss: 1.2484 - aux_output_loss: 1.3341 - main_output_acc: 0.7792 - main_output_f1_micro: 0.7142 - aux_output_acc: 0.7911 - aux_output_f1_micro: 0.1131 - val_loss: 1.3651 - val_main_output_loss: 1.0816 - val_aux_output_loss: 1.4173 - val_main_output_acc: 0.7956 - val_main_output_f1_micro: 0.7149 - val_aux_output_acc: 0.7927 - val_aux_output_f1_micro: 0.1133\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5089 - main_output_loss: 1.2427 - aux_output_loss: 1.3309 - main_output_acc: 0.7803 - main_output_f1_micro: 0.7156 - aux_output_acc: 0.7912 - aux_output_f1_micro: 0.1135 - val_loss: 1.3627 - val_main_output_loss: 1.0794 - val_aux_output_loss: 1.4167 - val_main_output_acc: 0.7989 - val_main_output_f1_micro: 0.7164 - val_aux_output_acc: 0.7968 - val_aux_output_f1_micro: 0.1136\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5082 - main_output_loss: 1.2431 - aux_output_loss: 1.3254 - main_output_acc: 0.7813 - main_output_f1_micro: 0.7171 - aux_output_acc: 0.7917 - aux_output_f1_micro: 0.1138 - val_loss: 1.3610 - val_main_output_loss: 1.0787 - val_aux_output_loss: 1.4114 - val_main_output_acc: 0.8054 - val_main_output_f1_micro: 0.7178 - val_aux_output_acc: 0.7916 - val_aux_output_f1_micro: 0.1139\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5043 - main_output_loss: 1.2400 - aux_output_loss: 1.3214 - main_output_acc: 0.7788 - main_output_f1_micro: 0.7185 - aux_output_acc: 0.7905 - aux_output_f1_micro: 0.1141 - val_loss: 1.3573 - val_main_output_loss: 1.0764 - val_aux_output_loss: 1.4045 - val_main_output_acc: 0.8025 - val_main_output_f1_micro: 0.7192 - val_aux_output_acc: 0.7962 - val_aux_output_f1_micro: 0.1143\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5045 - main_output_loss: 1.2409 - aux_output_loss: 1.3181 - main_output_acc: 0.7785 - main_output_f1_micro: 0.7199 - aux_output_acc: 0.7919 - aux_output_f1_micro: 0.1145 - val_loss: 1.3638 - val_main_output_loss: 1.0828 - val_aux_output_loss: 1.4050 - val_main_output_acc: 0.7982 - val_main_output_f1_micro: 0.7205 - val_aux_output_acc: 0.7963 - val_aux_output_f1_micro: 0.1146\n",
      "\n",
      "Done with epoch: 115\n",
      "\n",
      "CPU times: user 20min 20s, sys: 1min 53s, total: 22min 14s\n",
      "Wall time: 17min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# i = 100\n",
    "# batch_size = 600\n",
    "batch_size = 1200\n",
    "for j in xrange(i, i + (20 // epochs)):\n",
    "    hist = model.fit(\n",
    "        {'main_input': x_train, 'wv_input': wv_train, 'fs_input': fs_train},\n",
    "        {'main_output': y_train, 'aux_output': y_train},\n",
    "        epochs=epochs, batch_size=batch_size,   # 500\n",
    "        validation_split=0.2,\n",
    "        validation_data=(\n",
    "            {'main_input': x_val, 'wv_input': wv_val, 'fs_input': fs_val},\n",
    "            {'main_output': y_val, 'aux_output': y_val}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save(model_name.format(i))\n",
    "    print\n",
    "    print('Done with epoch: {}'.format((j + 1) * epochs))\n",
    "    with open('lstm-word2vec-fasttext.epoch.csv', 'a') as fl:\n",
    "        fl.write(model_name + '\\n')\n",
    "        fl.write('Epoch {}\\n'.format((j + 1) * epochs))\n",
    "        fl.write('{}\\n'.format(datetime.now()))\n",
    "        fl.write('\\n'.join(['{}: {}'.format(k, v[0]) for k, v in hist.history.items()]))\n",
    "        fl.write('\\n\\n')\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\n",
    "#     'models/lstm-word2vec-fasttext_2010-2014-data_categorical-crossentropy-2014-b-val-standard_scaled_wv_fs.model',\n",
    "#     custom_objects={'f1_micro': f1_micro}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = model.predict({'wv_input': wv_train[:100], 'fs_input': fs_train[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0af7f9e5d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09bd27bbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ix = 29\n",
    "pd.Series(g[ix]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([ 1, 98]),), (array([ 1, 98]),))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "np.where(y_train[ix] == 1), np.where(g[ix] > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = TfidfModel.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.dictionary')\n",
    "tfidf = TfidfModel.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = Word2Vec.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsmodel = fasttext.load_model('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_tfidf_word2vec(tokens, stopwords=[]):\n",
    "#     global wvmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    wv_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords and w in wvmodel.wv.vocab)]\n",
    "    ).map(\n",
    "        lambda x: tfidf[dictionary.doc2bow(x)]\n",
    "    ).map(\n",
    "        lambda x: np.array([wvmodel[dictionary.id2token[id]] * w for id, w in x]).mean(axis=0) if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return wv_feature_vec\n",
    "\n",
    "\n",
    "def transform_tfidf_fasttext(tokens, stopwords=[]):\n",
    "#     global fsmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    fs_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(\n",
    "        lambda x: tfidf[dictionary.doc2bow(x)]\n",
    "    ).map(\n",
    "        lambda x: np.array([np.array(fsmodel[dictionary.id2token[id]]) * w for id, w in x]).mean(axis=0) if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return fs_feature_vec\n",
    "\n",
    "def transform_fasttext(tokens, stopwords=[]):\n",
    "    global fsmodel\n",
    "    # This requires fsmodel to be present in the namespace.\n",
    "    fs_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(lambda x: np.array([fsmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return fs_feature_vec\n",
    "\n",
    "\n",
    "def transform_unsupervised_sentiment_neuron(tokens, stopwords=[]):\n",
    "    # This requires fsmodel to be present in the namespace.\n",
    "    \n",
    "    usn_feature_vec = usnmodel.transform(tokens)\n",
    "\n",
    "    # usn_feature_vec = tokens.map(\n",
    "    #     lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    # ).map(lambda x: np.array([usnmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return usn_feature_vec\n",
    "\n",
    "\n",
    "def transform_word2vec(tokens, stopwords=[]):\n",
    "    global wvmodel\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    wv_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords and w in wvmodel.wv.vocab)]\n",
    "    ).map(lambda x: np.array([wvmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return wv_feature_vec\n",
    "\n",
    "\n",
    "def parallel_generate_word_vectors(samp, transformer, stopwords, batch, num_proc):\n",
    "    with Parallel(n_jobs=num_proc) as parallel:\n",
    "        dataset = []\n",
    "        is_break = False\n",
    "        i = 0\n",
    "\n",
    "        while not is_break:\n",
    "            payload = []\n",
    "\n",
    "            for j in xrange(num_proc):\n",
    "                t_df = samp[(i + j) * batch: (i + 1 + j) * batch]\n",
    "\n",
    "                if t_df.empty:\n",
    "                    is_break = True\n",
    "                    continue\n",
    "\n",
    "                payload.append(\n",
    "                    delayed(transformer)(\n",
    "                        t_df, stopwords\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            print('Current batch in main thread: {}'.format((i + j) * batch))\n",
    "\n",
    "            if payload:\n",
    "                results = parallel(payload)\n",
    "                dataset.extend(results)\n",
    "                i += num_proc\n",
    "\n",
    "    return pd.concat(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(pred, scale_param=0.75, min_thresh=0.05, thresh = 0.5):\n",
    "#     mx = pred.mean() + 3 * pred.std()\n",
    "    return np.where(pred > thresh)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2idx_transform(word, _word2idx):\n",
    "    return _word2idx.get(word, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_for(df, min_batch=2000, stopwords=[], num_proc=7):\n",
    "    df_tokens = transform_text(df)\n",
    "    \n",
    "    batch = min(df_tokens.shape[0] / num_proc, min_batch)\n",
    "\n",
    "    print('Computing fs features...')\n",
    "    fvec = parallel_generate_word_vectors(df_tokens, transform_fasttext, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    print('Computing wv features...')\n",
    "    wvec = parallel_generate_word_vectors(df_tokens, transform_word2vec, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    print('Mapping word indices...')\n",
    "    word_indices = df_tokens.map(lambda x: [word2idx_transform(i, _word2idx) for i in x.split()])\n",
    "\n",
    "    print('Computing tfidf fs features...')\n",
    "    tfidf_fvec = parallel_generate_word_vectors(df_tokens, transform_tfidf_fasttext, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    print('Computing tfidf wv features...')\n",
    "    tfidf_wvec = parallel_generate_word_vectors(df_tokens, transform_tfidf_word2vec, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    \n",
    "    return word_indices, wvec, fvec, tfidf_wvec, tfidf_fvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/TestData.json') as fl:\n",
    "    data = json.load(fl)\n",
    "    test_df = pd.DataFrame(data['TestData']).T\n",
    "    del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fs features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "Mapping word indices...\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "CPU times: user 50.3 s, sys: 4.88 s, total: 55.2 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_word_indices,test_wvec, test_fvec, test_tfidf_wvec, test_tfidf_fvec = extract_features_for(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(np.all(test_wvec[test_wvec.isnull()].index == test_fvec[test_fvec.isnull()].index))\n",
    "test_null_index = test_wvec[test_wvec.isnull()].index.union(test_fvec[test_fvec.isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'TestData_02543', u'TestData_05012', u'TestData_05830'], dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_null_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 32 ms, total: 1.21 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_test_index = test_word_indices.index.difference(test_null_index)\n",
    "x_test = test_word_indices.ix[valid_test_index].map(lambda x: [top_token2ind.get(i, 0) for i in x])\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "wv_test = np.vstack(test_wvec.ix[valid_test_index])\n",
    "fs_test = np.vstack(test_fvec.ix[valid_test_index])\n",
    "\n",
    "\n",
    "wv_test = wv_sc.transform(wv_test)\n",
    "fs_test = fs_sc.transform(fs_test)\n",
    "\n",
    "tfidf_wv_test = np.vstack(test_tfidf_wvec.ix[valid_test_index])\n",
    "tfidf_fs_test = np.vstack(test_tfidf_fvec.ix[valid_test_index])\n",
    "\n",
    "\n",
    "tfidf_wv_test = wv_sc.transform(tfidf_wv_test)\n",
    "tfidf_fs_test = fs_sc.transform(tfidf_fs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 500\n",
    "# test_probas = model.predict({'wv_input': wv_test, 'fs_input': fs_test}, batch_size=batch_size)\n",
    "test_probas = model.predict({'wv_input': tfidf_wv_test, 'fs_input': tfidf_fs_test}, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_test_probas = test_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   9.53572221e-11,   2.34125965e-11, ...,\n",
       "          4.58941804e-13,   7.55075030e-18,   0.00000000e+00],\n",
       "       [  7.10577062e-37,   1.95997995e-06,   6.26461588e-06, ...,\n",
       "          1.58872293e-08,   1.58961299e-07,   9.53767052e-37],\n",
       "       [  0.00000000e+00,   3.49002214e-08,   1.33569715e-08, ...,\n",
       "          7.51075777e-06,   3.19748228e-10,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  6.72936563e-26,   5.19690383e-03,   1.15589380e-01, ...,\n",
       "          2.78446732e-09,   7.49666803e-03,   2.38802543e-25],\n",
       "       [  7.92818150e-29,   1.94924721e-03,   2.61581095e-04, ...,\n",
       "          8.19239574e-08,   1.91242202e-06,   4.57425551e-28],\n",
       "       [  0.00000000e+00,   6.89471854e-05,   1.37220457e-04, ...,\n",
       "          1.12836277e-11,   3.48008816e-10,   8.89052297e-38]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_test_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_df.ix[test_df.index.difference(test_null_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2542, 5011, 5829]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_index = [int(s.split('_')[1]) - 1 for s in test_null_index]  # Subtract 1 since test index starts at 1 while enumerate starts at 0\n",
    "skip_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7578, 160), (7581, 3))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_test_probas.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 31.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# valid_test_feature_vec found below!\n",
    "test_values = np.zeros([main_test_probas.shape[0], len(topics)])\n",
    "for ix, pred in enumerate(main_test_probas):\n",
    "    for v in get_classes(pred, thresh=0.2):\n",
    "        test_values[ix][v] = 1\n",
    "\n",
    "test_sub_df = pd.DataFrame(\n",
    "    test_values,\n",
    "    index=test_df.ix[test_df.index.difference(test_null_index)].index,\n",
    "    columns=topics\n",
    ")\n",
    "\n",
    "null_test_df = pd.DataFrame(\n",
    "    np.zeros((len(test_null_index), len(topics))),\n",
    "    index=test_null_index,\n",
    "    columns=topics\n",
    ")\n",
    "\n",
    "test_sub_df = test_sub_df.append(null_test_df)\n",
    "test_sub_df = test_sub_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9627 (0.5), 14297 (0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12622.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14432.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7581.000000\n",
       "mean      882.691070\n",
       "std       621.585386\n",
       "min         0.000000\n",
       "25%       552.000000\n",
       "50%       770.000000\n",
       "75%      1026.000000\n",
       "max      8171.000000\n",
       "Name: bodyText, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_indices.map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_indices.map(len).quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df.ix['TestData_04490'].bodyText.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 51s - loss: 1.5045 - main_output_loss: 1.2409 - aux_output_loss: 1.3181 - main_output_acc: 0.7785 - main_output_f1_micro: 0.7199 - aux_output_acc: 0.7919 - aux_output_f1_micro: 0.1145 - val_loss: 1.3638 - val_main_output_loss: 1.0828 - val_aux_output_loss: 1.4050 - val_main_output_acc: 0.7982 - val_main_output_f1_micro: 0.7205 - val_aux_output_acc: 0.7963 - val_aux_output_f1_micro: 0.1146\n"
     ]
    }
   ],
   "source": [
    "print '94731/94731 [==============================] - 51s - loss: 1.5045 - main_output_loss: 1.2409 - aux_output_loss: 1.3181 - main_output_acc: 0.7785 - main_output_f1_micro: 0.7199 - aux_output_acc: 0.7919 - aux_output_f1_micro: 0.1145 - val_loss: 1.3638 - val_main_output_loss: 1.0828 - val_aux_output_loss: 1.4050 - val_main_output_acc: 0.7982 - val_main_output_f1_micro: 0.7205 - val_aux_output_acc: 0.7963 - val_aux_output_f1_micro: 0.1146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sub_df.astype(int).reset_index().rename(\n",
    "    columns={'index': 'id'}\n",
    ").sort_values('id').to_csv(\n",
    "    'tfidf_word2vec_300-fasttext_300-deep_stack_net-epochs_800-f1_0.6183-data_2010_2014-val_data_2014-thresh_0.2-with_sc_wv_fs.csv', \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7581, 160)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TestData_04490\tThe World Health Organisation has convened an ...\t[]\t28-01-2016\n",
    "TestData_04550\tSpraying pesticides will fail to deal with the...\t[]\t02-02-2016\n",
    "TestData_05683\tViolent protests at Trump rally in California ...\t[]\t03-06-2016\n",
    "TestData_05869\tLast weekend, we saw the darkest side of human...\t[]\t17-06-2016\n",
    "TestData_06148\tAs dusk falls over Copacabana beach, Ubira San...\t[]\t16-07-2016\n",
    "TestData_06291\tIt is 3pm and yet another patient is brought t...\t[]\t27-07-2016\n",
    "TestData_06610\tHuddled around their hives, beekeepers around ...\t[]\t04-09-2016\n",
    "TestData_06708\tA United Nations high-level panel on access to...\t[]\t14-09-2016\n",
    "TestData_07263\tWHO: Zika virus is no longer a world threat Th...\t[]\t19-11-2016\n",
    "TestData_07478\t1 World Health Organisation declares a public ...\t[]\t18-12-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guncrime        1.0\n",
       "usguncontrol    1.0\n",
       "Name: TestData_05869, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 5868\n",
    "test_sub_df.iloc[ix][test_sub_df.iloc[ix] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 32.6 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# adjust_index = 0\n",
    "# # valid_test_feature_vec found below!\n",
    "# test_values = np.zeros([test_df.shape[0], len(topics)])\n",
    "# for ix, pred in enumerate(main_test_probas):\n",
    "#     if ix in skip_index:\n",
    "#         test_values[ix] = np.nan\n",
    "#         # Increment adjust index so that we have the correct index for other samples\n",
    "#         adjust_index += 1\n",
    "#         continue\n",
    "\n",
    "#     for v in get_classes(pred, thresh=0.05):\n",
    "#         test_values[ix + adjust_index][v] = 1\n",
    "\n",
    "# test_sub_df = pd.DataFrame(test_values, columns=sorted(topics), index=test_df.index)\n",
    "\n",
    "# q = test_sub_df.sum(axis=1)\n",
    "# assert(len(q[q.isnull()].index.difference(test_null_index)) == 0)\n",
    "\n",
    "# test_sub_df = test_sub_df.fillna(0)\n",
    "\n",
    "# # for i in test_feature_vec[test_feature_vec.isnull()].index:\n",
    "# #     test_sub_df.ix[i] = np.zeros(len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestData_02543    0.0\n",
       "TestData_05012    0.0\n",
       "TestData_05830    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.ix[test_null_index].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11656.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sub_df.astype(int).reset_index().rename(\n",
    "    columns={'index': 'id'}\n",
    ").sort_values('id').to_csv(\n",
    "    'lstm_300-word2vec_300-fasttext_300-maxlen_500-dense_64_64_64-cat_cross-epoch_210-batch_size_750-val_main_output_f1_micro_0.5760-main_output_f1_micro_0.5751-main_output_loss_0.9143-data_2010_2013-val_data_2014-thresh_0.05.csv', \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: zikavirus, dtype: float64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = test_sub_df['zikavirus']\n",
    "e[e==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14328"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_submission = pd.read_csv('basic_nn_submission_0.649_accuracy_multi_class.csv')\n",
    "top_submission.set_index('id').sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9280"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_index_lstm_sub = pd.read_csv('lstm.2014b_training_700_maxlen_64cell_100epochs_0.0025_threshold.csv')\n",
    "wrong_index_lstm_sub.set_index('id').sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34952"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_sub = pd.read_csv('basic_nn_submission_full_training_data_0.9958_validation_accuracy_binary_crossentropy.csv')\n",
    "some_sub.set_index('id').sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2197, 160)\n",
      "(3957, 160)\n",
      "(12, 160)\n",
      "(1503, 160)\n"
     ]
    }
   ],
   "source": [
    "print top_submission.set_index('id')[top_submission.set_index('id').sum(axis=1) == 0].shape\n",
    "\n",
    "print wrong_index_lstm_sub.set_index('id')[wrong_index_lstm_sub.set_index('id').sum(axis=1) == 0].shape\n",
    "\n",
    "print some_sub.set_index('id')[some_sub.set_index('id').sum(axis=1) == 0].shape\n",
    "\n",
    "print test_sub_df[test_sub_df.sum(axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestData_00011     0\n",
       "TestData_00012     0\n",
       "TestData_00015     0\n",
       "TestData_00027     3\n",
       "TestData_00029     0\n",
       "TestData_00038     1\n",
       "TestData_00042     5\n",
       "TestData_00053     4\n",
       "TestData_00056     1\n",
       "TestData_00060     1\n",
       "TestData_00066     0\n",
       "TestData_00085     0\n",
       "TestData_00087     1\n",
       "TestData_00090     0\n",
       "TestData_00092     0\n",
       "TestData_00107     3\n",
       "TestData_00111     0\n",
       "TestData_00114     0\n",
       "TestData_00115     1\n",
       "TestData_00118     0\n",
       "TestData_00119     0\n",
       "TestData_00121     0\n",
       "TestData_00123     0\n",
       "TestData_00125     0\n",
       "TestData_00127     0\n",
       "TestData_00128     1\n",
       "TestData_00139     1\n",
       "TestData_00140     1\n",
       "TestData_00144     0\n",
       "TestData_00147     2\n",
       "                  ..\n",
       "TestData_07445     0\n",
       "TestData_07456     3\n",
       "TestData_07461     1\n",
       "TestData_07462     4\n",
       "TestData_07465     0\n",
       "TestData_07468     0\n",
       "TestData_07471     1\n",
       "TestData_07475     0\n",
       "TestData_07486    10\n",
       "TestData_07495     1\n",
       "TestData_07509     0\n",
       "TestData_07514     3\n",
       "TestData_07515     1\n",
       "TestData_07523     0\n",
       "TestData_07533     2\n",
       "TestData_07534     2\n",
       "TestData_07542     1\n",
       "TestData_07544     2\n",
       "TestData_07545     0\n",
       "TestData_07552     2\n",
       "TestData_07556     5\n",
       "TestData_07563     1\n",
       "TestData_07565     0\n",
       "TestData_07566     0\n",
       "TestData_07569     0\n",
       "TestData_07571     3\n",
       "TestData_07572     1\n",
       "TestData_07579     6\n",
       "TestData_07580     2\n",
       "TestData_07581     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_submission.set_index('id').ix[q[q == 0].index].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_sub_df.sum(axis=1)\n",
    "q[q==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7581.000000\n",
       "mean        2.160929\n",
       "std         1.739411\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max        13.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = trainingY.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    236286.000000\n",
       "mean          1.392787\n",
       "std           0.762577\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           1.000000\n",
       "75%           2.000000\n",
       "max          15.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bodyText</th>\n",
       "      <th>topics</th>\n",
       "      <th>webPublicationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TestData_03241</th>\n",
       "      <td>A special British police unit was put on stand...</td>\n",
       "      <td>[]</td>\n",
       "      <td>15-11-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_04088</th>\n",
       "      <td>The youngest convict in a fatal gang-rape in N...</td>\n",
       "      <td>[]</td>\n",
       "      <td>20-12-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_06306</th>\n",
       "      <td>Former New York City mayor Rudy Giuliani has s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>28-07-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_06083</th>\n",
       "      <td>John Cantlie, the British journalist who has b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>13-07-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_05896</th>\n",
       "      <td>Lawyers for the companies that manufactured an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>20-06-2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         bodyText topics  \\\n",
       "TestData_03241  A special British police unit was put on stand...     []   \n",
       "TestData_04088  The youngest convict in a fatal gang-rape in N...     []   \n",
       "TestData_06306  Former New York City mayor Rudy Giuliani has s...     []   \n",
       "TestData_06083  John Cantlie, the British journalist who has b...     []   \n",
       "TestData_05896  Lawyers for the companies that manufactured an...     []   \n",
       "\n",
       "               webPublicationDate  \n",
       "TestData_03241         15-11-2015  \n",
       "TestData_04088         20-12-2015  \n",
       "TestData_06306         28-07-2016  \n",
       "TestData_06083         13-07-2016  \n",
       "TestData_05896         20-06-2016  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ix = 'TestData_04088'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights    1.0\n",
       "india          1.0\n",
       "Name: TestData_04088, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_sub_df.ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ukcrime    1\n",
       "Name: TestData_04088, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = top_submission.set_index('id').ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights    1\n",
       "india          1\n",
       "protest        1\n",
       "ukcrime        1\n",
       "Name: TestData_04088, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = some_sub.set_index('id').ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights    1\n",
       "Name: TestData_02924, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = wrong_index_lstm_sub.set_index('id').ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter-terrorism policy\n",
    " \n",
    "Foreign policy\n",
    " \n",
    "Defence policy\n",
    " \n",
    "Islamic State\n",
    " \n",
    "Syria\n",
    " \n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = trainingY.sum()\n",
    "unseen_topics = s[s.isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activism',\n",
       " 'bastilledaytruckattack',\n",
       " 'berlinchristmasmarketattack',\n",
       " 'brusselsattacks',\n",
       " 'charliehebdoattack',\n",
       " 'francetrainattack',\n",
       " 'munichshooting',\n",
       " 'orlandoterrorattack',\n",
       " 'parisattacks',\n",
       " 'peaceandreconciliation',\n",
       " 'sanbernardinoshooting',\n",
       " 'tunisiaattack2015',\n",
       " 'turkeycoupattempt',\n",
       " 'zikavirus'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(topics).intersection(unseen_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activism\n",
      "afghanistan\n",
      "aid\n",
      "algerianhostagecrisis\n",
      "alqaida\n",
      "alshabaab\n",
      "antiwar\n",
      "arabandmiddleeastprotests\n",
      "armstrade\n",
      "australianguncontrol\n",
      "australiansecurityandcounterterrorism\n",
      "bastilledaytruckattack\n",
      "belgium\n",
      "berlinchristmasmarketattack\n",
      "bigdata\n",
      "biometrics\n",
      "bokoharam\n",
      "bostonmarathonbombing\n",
      "britisharmy\n",
      "brusselsattacks\n",
      "cameroon\n",
      "carers\n",
      "charliehebdoattack\n",
      "chemicalweapons\n",
      "clusterbombs\n",
      "cobra\n",
      "conflictanddevelopment\n",
      "controversy\n",
      "criminaljustice\n",
      "cybercrime\n",
      "cyberwar\n",
      "darknet\n",
      "dataprotection\n",
      "debate\n",
      "defence\n",
      "deflation\n",
      "drones\n",
      "drugs\n",
      "drugspolicy\n",
      "drugstrade\n",
      "earthquakes\n",
      "ebola\n",
      "economy\n",
      "egypt\n",
      "encryption\n",
      "energy\n",
      "espionage\n",
      "ethics\n",
      "europeanarrestwarrant\n",
      "europeancourtofhumanrights\n",
      "events\n",
      "extradition\n",
      "famine\n",
      "farright\n",
      "firefighters\n",
      "forensicscience\n",
      "france\n",
      "francetrainattack\n",
      "freedomofspeech\n",
      "genevaconventions\n",
      "germany\n",
      "guncrime\n",
      "hacking\n",
      "hashtags\n",
      "helicoptercrashes\n",
      "humanitarianresponse\n",
      "humanrights\n",
      "humanrightsact\n",
      "humantrafficking\n",
      "immigration\n",
      "india\n",
      "indonesia\n",
      "internallydisplacedpeople\n",
      "internationalcourtofjustice\n",
      "internationalcriminaljustice\n",
      "internetsafety\n",
      "iraq\n",
      "isis\n",
      "israel\n",
      "jordan\n",
      "jubilee\n",
      "judiciary\n",
      "july7\n",
      "justiceandsecurity\n",
      "kenya\n",
      "knifecrime\n",
      "lebanon\n",
      "libya\n",
      "localgovernment\n",
      "logistics\n",
      "london\n",
      "londonriots\n",
      "malaysia\n",
      "mali\n",
      "malware\n",
      "metropolitanpolice\n",
      "middleeastpeacetalks\n",
      "migration\n",
      "military\n",
      "ministryofdefence\n",
      "morocco\n",
      "mrsa\n",
      "mumbaiterrorattacks\n",
      "munichshooting\n",
      "naturaldisasters\n",
      "nigeria\n",
      "nuclearweapons\n",
      "occupy\n",
      "organisedcrime\n",
      "orlandoterrorattack\n",
      "osamabinladen\n",
      "paris\n",
      "parisattacks\n",
      "peaceandreconciliation\n",
      "philippines\n",
      "piracy\n",
      "planecrashes\n",
      "police\n",
      "protest\n",
      "refugees\n",
      "religion\n",
      "retirementage\n",
      "rio20earthsummit\n",
      "royalairforce\n",
      "royalnavy\n",
      "russia\n",
      "sanbernardinoshooting\n",
      "saudiarabia\n",
      "september11\n",
      "slavery\n",
      "somalia\n",
      "southafrica\n",
      "southchinasea\n",
      "stopandsearch\n",
      "surveillance\n",
      "sydneysiege\n",
      "syria\n",
      "taliban\n",
      "terrorism\n",
      "thailand\n",
      "torture\n",
      "traincrashes\n",
      "transport\n",
      "tunisiaattack2015\n",
      "turkey\n",
      "turkeycoupattempt\n",
      "ukcrime\n",
      "uksecurity\n",
      "uksupremecourt\n",
      "undercoverpoliceandpolicing\n",
      "unitednations\n",
      "usguncontrol\n",
      "values\n",
      "warcrimes\n",
      "warreporting\n",
      "weaponstechnology\n",
      "womeninbusiness\n",
      "woolwichattack\n",
      "worldmigration\n",
      "zikavirus\n"
     ]
    }
   ],
   "source": [
    "for i in topics:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/ml-ai/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3445929"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(wvmodel['zika'], np.vstack(test_wvec.dropna())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/ml-ai/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38107796869050226"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(fsmodel['zika'], np.vstack(test_fvec.dropna())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyText              The World Health Organisation has convened an ...\n",
       "topics                                                               []\n",
       "webPublicationDate                                           28-01-2016\n",
       "Name: TestData_04490, dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.ix[4488 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyText              The United Nations security council has called...\n",
       "topics                                                               []\n",
       "webPublicationDate                                           17-09-2016\n",
       "Name: TestData_06730, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.ix[6727 + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyText              We are deeply concerned that the counter-terro...\n",
       "topics                                                               []\n",
       "webPublicationDate                                           02-02-2015\n",
       "Name: TestData_00360, dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.ix[359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugstrade    1.0\n",
       "Name: TestData_04490, dtype: float64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_sub_df.iloc[4488 + 1]\n",
    "q[q > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
