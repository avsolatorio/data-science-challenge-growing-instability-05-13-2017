{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from growing_instability_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('../data/sampleSubmission.csv')\n",
    "topics = sorted(set(sample_sub.columns.difference(['id'])))\n",
    "\n",
    "topic2actual = {}\n",
    "for i in sample_sub.columns:\n",
    "    if 'id' == i:\n",
    "        continue\n",
    "    topic2actual[i] = segment(i)\n",
    "    \n",
    "target_columns = sorted(topics)\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.73 s, sys: 1.4 s, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'wvec_trainingX')\n",
    "# fvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'fvec_trainingX')\n",
    "\n",
    "wvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_wvec_trainingX')\n",
    "fvec_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_fvec_trainingX')\n",
    "\n",
    "word2idx_trainingX = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'word2idx_trainingX')\n",
    "_word2idx = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', '_word2idx')\n",
    "trainingY = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', 'trainingY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "\n",
    "top_tokens = Counter()\n",
    "for i in word2idx_trainingX:\n",
    "    top_tokens.update(set(i[:maxlen]))\n",
    "\n",
    "top_tokens = pd.DataFrame(top_tokens.most_common(), columns=['token', 'freq'])\n",
    "top_doc_tokens = top_tokens[top_tokens.freq > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_token2ind = {}\n",
    "for i, j in enumerate(top_doc_tokens.token):\n",
    "    top_token2ind[j] = i + 1  # Add 1 to start with 1 since 0 is special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 s, sys: 140 ms, total: 28 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2ind = top_token2ind\n",
    "\n",
    "ind2class = dict(enumerate(topics))\n",
    "class2ind = {j: i for i, j in ind2class.items()}\n",
    "\n",
    "num_samples = trainingY.shape[0]\n",
    "\n",
    "training_X = word2idx_trainingX.head(num_samples)\n",
    "\n",
    "training_Y = pd.DataFrame(zip(*np.where(trainingY.head(num_samples) == 1)), columns=['iloc', 'topics'])\n",
    "training_WV = wvec_trainingX.head(num_samples)\n",
    "training_FS = fvec_trainingX.head(num_samples)\n",
    "\n",
    "training_Y = training_Y.groupby('iloc')['topics'].apply(list)\n",
    "training_Y.index = trainingY.head(num_samples).index\n",
    "\n",
    "# indices = sorted(training_Y.index.copy())\n",
    "indices = sorted(training_Y.index[training_Y.index.str.contains('^201[0-9]')])\n",
    "# np.random.shuffle(indices)\n",
    "indices = pd.Index(indices)\n",
    "\n",
    "training_X = training_X.ix[indices]\n",
    "training_WV = training_WV.ix[indices]\n",
    "training_FS = training_FS.ix[indices]\n",
    "training_Y = training_Y.ix[indices]\n",
    "\n",
    "# Transform index to top index\n",
    "training_X = training_X.map(lambda x: [top_token2ind.get(i, 0) for i in x])\n",
    "\n",
    "dataset = zip(training_X, training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010a_TrainingData_00008    0.2\n",
       "2010a_TrainingData_00022    0.2\n",
       "2010a_TrainingData_00027    0.2\n",
       "2010a_TrainingData_00034    0.2\n",
       "2010a_TrainingData_00036    0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample_weights = training_Y.index.map(lambda x: (int(re.findall('^201([0-9])', x)[0]) + 1.))\n",
    "training_sample_weights = pd.Series(training_sample_weights / training_sample_weights.max(), index=training_Y.index)\n",
    "training_sample_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_sc = StandardScaler()\n",
    "fs_sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "def build_target(y, size):\n",
    "    e = np.zeros(size)\n",
    "    e[y] = 1\n",
    "    return e\n",
    "\n",
    "def build_input_output_data(X, WV, FS, Y, maxlen):\n",
    "\n",
    "    x = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    y = np.vstack(Y.map(lambda x: build_target(x, len(topics))))\n",
    "    wv = np.vstack(WV)\n",
    "    fs = np.vstack(FS)\n",
    "    \n",
    "    return x, wv, fs, y\n",
    "\n",
    "\n",
    "test_ix = training_Y.index.str.contains('^201[0-4]')\n",
    "val_ix = training_Y.index.str.contains('^2014[b]')\n",
    "\n",
    "\n",
    "x_train, wv_train, fs_train, y_train = build_input_output_data(\n",
    "    training_X.ix[test_ix],\n",
    "    training_WV.ix[test_ix],\n",
    "    training_FS.ix[test_ix],\n",
    "    training_Y.ix[test_ix],\n",
    "    maxlen=maxlen\n",
    ")\n",
    "\n",
    "\n",
    "x_val, wv_val, fs_val, y_val = build_input_output_data(\n",
    "    training_X.ix[val_ix],\n",
    "    training_WV.ix[val_ix],\n",
    "    training_FS.ix[val_ix],\n",
    "    training_Y.ix[val_ix],\n",
    "    maxlen=maxlen\n",
    ")\n",
    "\n",
    "wv_train = wv_sc.fit_transform(wv_train)\n",
    "fs_train = fs_sc.fit_transform(fs_train)\n",
    "\n",
    "wv_val = wv_sc.transform(wv_val)\n",
    "fs_val = fs_sc.transform(fs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94731,), (9424,), (94731,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_Y.shape, training_Y.ix[training_Y.index.str.contains('^2014[b]')].shape, training_sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Setup model\n",
    "# model_lstm = keras.models.Sequential()\n",
    "# model_lstm.add(keras.layers.Embedding(len(word2ind) + 1, 256))\n",
    "# # model_lstm.add(keras.layers.LSTM(32, return_sequences=False, input_shape=(None, len(word2ind) + 1)))\n",
    "# # model_lstm.add(keras.layers.Dropout(0.2))\n",
    "# model_lstm.add(keras.layers.LSTM(16, return_sequences=False))\n",
    "# model_lstm.add(keras.layers.Dense(128))\n",
    "# model_lstm.add(keras.layers.Activation('relu'))\n",
    "# model_lstm.add(keras.layers.Dropout(0.2))\n",
    "# model_lstm.add(keras.layers.Dense(len(class2ind)))\n",
    "# model_lstm.add(keras.layers.Activation('sigmoid'))\n",
    "# model_lstm.compile(\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # for i in range(6):\n",
    "# #     model_lstm.fit_generator(id_lstm_gen, steps_per_epoch=len(dataset), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86772972,  0.86588758,  1.0798322 , ...,  0.4315823 ,\n",
       "        0.35406384,  0.3516916 ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_train.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Convolution1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "num_filters = 10\n",
    "sz = 3\n",
    "\n",
    "wv_input = Input(shape=(300,), name='wv_input')\n",
    "fs_input = Input(shape=(300,), name='fs_input')\n",
    "\n",
    "wv_x = Dense(128, activation='relu')(wv_input)\n",
    "wv_x = Dropout(0.3)(wv_x)\n",
    "wv_x = Dense(512, activation='relu')(wv_x)\n",
    "wv_x = Dropout(0.3)(wv_x)\n",
    "\n",
    "fs_x = Dense(128, activation='relu')(fs_input)\n",
    "fs_x = Dropout(0.3)(fs_x)\n",
    "fs_x = Dense(512, activation='relu')(fs_x)\n",
    "fs_x = Dropout(0.3)(fs_x)\n",
    "\n",
    "x_mult = keras.layers.dot([wv_x, fs_x], 1)\n",
    "x = keras.layers.concatenate([wv_x, fs_x, x_mult])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(len(class2ind), activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=[wv_input, fs_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as K\n",
    "\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    TP = K.metrics.true_positives(y_true, K.round(y_pred))\n",
    "    FP = K.metrics.false_positives(y_true, K.round(y_pred))\n",
    "    FN = K.metrics.false_negatives(y_true, K.round(y_pred))\n",
    "    \n",
    "    p = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FP))\n",
    "    r = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FN))\n",
    "    \n",
    "    return (2.0 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "import keras.backend as KB\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = KB.sum(KB.round(KB.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = KB.sum(KB.round(KB.clip(y_pred, 0, 1)))\n",
    "    c3 = KB.sum(KB.round(KB.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "wv_input (InputLayer)            (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "fs_input (InputLayer)            (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 128)           38528                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 128)           38528                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 512)           66048                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 512)           66048                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 1025)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 128)           131328                                       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 256)           33024                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 128)           32896                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 160)           20640                                        \n",
      "====================================================================================================\n",
      "Total params: 427,040\n",
      "Trainable params: 427,040\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss={'main_output': 'categorical_crossentropy'},\n",
    "              loss_weights={'main_output': 1.}, metrics=['accuracy', f1_micro])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "# model.fit(X, y, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.train_on_batch(\n",
    "#     {'main_input': x_train[:10], 'wv_input': np.vstack(training_WV)[:10], 'fs_input': np.vstack(training_FS)[:10]},\n",
    "#     {'main_output': y_train[:10], 'aux_output': y_train[:10]}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.9674 - acc: 0.5478 - f1_micro: 0.2535 - val_loss: 2.7046 - val_acc: 0.6051 - val_f1_micro: 0.2645\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.9033 - acc: 0.5547 - f1_micro: 0.2746 - val_loss: 2.6373 - val_acc: 0.6115 - val_f1_micro: 0.2845\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.8473 - acc: 0.5614 - f1_micro: 0.2936 - val_loss: 2.5996 - val_acc: 0.6164 - val_f1_micro: 0.3020\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.8051 - acc: 0.5664 - f1_micro: 0.3099 - val_loss: 2.5656 - val_acc: 0.6161 - val_f1_micro: 0.3175\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.7725 - acc: 0.5720 - f1_micro: 0.3247 - val_loss: 2.5370 - val_acc: 0.6237 - val_f1_micro: 0.3315\n",
      "CPU times: user 7.11 s, sys: 464 ms, total: 7.57 s\n",
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# And trained it via:\n",
    "batch_size = 500\n",
    "model.fit(\n",
    "    {'wv_input': wv_train, 'fs_input': fs_train},\n",
    "    {'main_output': y_train},\n",
    "    epochs=5, batch_size=batch_size,   # 500\n",
    "    validation_split=0.2,\n",
    "    validation_data=(\n",
    "        {'wv_input': wv_val, 'fs_input': fs_val},\n",
    "        {'main_output': y_val}\n",
    "    ),\n",
    "#     sample_weight=training_sample_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0501 - acc: 0.6556 - f1_micro: 0.5768 - val_loss: 1.6969 - val_acc: 0.7228 - val_f1_micro: 0.5769\n",
      "Epoch 2/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0474 - acc: 0.6567 - f1_micro: 0.5770 - val_loss: 1.6902 - val_acc: 0.7241 - val_f1_micro: 0.5770\n",
      "Epoch 3/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0403 - acc: 0.6576 - f1_micro: 0.5771 - val_loss: 1.6878 - val_acc: 0.7265 - val_f1_micro: 0.5772\n",
      "Epoch 4/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0394 - acc: 0.6578 - f1_micro: 0.5773 - val_loss: 1.6887 - val_acc: 0.7190 - val_f1_micro: 0.5773\n",
      "Epoch 5/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0447 - acc: 0.6566 - f1_micro: 0.5774 - val_loss: 1.6905 - val_acc: 0.7217 - val_f1_micro: 0.5774\n",
      "Epoch 6/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0362 - acc: 0.6574 - f1_micro: 0.5775 - val_loss: 1.6836 - val_acc: 0.7197 - val_f1_micro: 0.5776\n",
      "Epoch 7/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0396 - acc: 0.6563 - f1_micro: 0.5777 - val_loss: 1.6825 - val_acc: 0.7186 - val_f1_micro: 0.5777\n",
      "Epoch 8/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0384 - acc: 0.6561 - f1_micro: 0.5778 - val_loss: 1.6860 - val_acc: 0.7238 - val_f1_micro: 0.5779\n",
      "Epoch 9/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0326 - acc: 0.6578 - f1_micro: 0.5779 - val_loss: 1.6832 - val_acc: 0.7208 - val_f1_micro: 0.5780\n",
      "Epoch 10/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0356 - acc: 0.6570 - f1_micro: 0.5781 - val_loss: 1.6833 - val_acc: 0.7207 - val_f1_micro: 0.5781\n",
      "Epoch 11/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0362 - acc: 0.6577 - f1_micro: 0.5782 - val_loss: 1.6879 - val_acc: 0.7242 - val_f1_micro: 0.5783\n",
      "Epoch 12/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0361 - acc: 0.6574 - f1_micro: 0.5783 - val_loss: 1.6833 - val_acc: 0.7202 - val_f1_micro: 0.5784\n",
      "Epoch 13/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0303 - acc: 0.6587 - f1_micro: 0.5785 - val_loss: 1.6793 - val_acc: 0.7249 - val_f1_micro: 0.5785\n",
      "Epoch 14/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0415 - acc: 0.6546 - f1_micro: 0.5786 - val_loss: 1.6845 - val_acc: 0.7267 - val_f1_micro: 0.5787\n",
      "Epoch 15/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0407 - acc: 0.6571 - f1_micro: 0.5787 - val_loss: 1.6821 - val_acc: 0.7227 - val_f1_micro: 0.5788\n",
      "Epoch 16/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0404 - acc: 0.6577 - f1_micro: 0.5788 - val_loss: 1.6809 - val_acc: 0.7207 - val_f1_micro: 0.5789\n",
      "Epoch 17/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0388 - acc: 0.6562 - f1_micro: 0.5790 - val_loss: 1.6795 - val_acc: 0.7296 - val_f1_micro: 0.5790\n",
      "Epoch 18/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0381 - acc: 0.6570 - f1_micro: 0.5791 - val_loss: 1.6759 - val_acc: 0.7188 - val_f1_micro: 0.5791\n",
      "Epoch 19/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0364 - acc: 0.6575 - f1_micro: 0.5792 - val_loss: 1.6824 - val_acc: 0.7225 - val_f1_micro: 0.5793\n",
      "Epoch 20/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0308 - acc: 0.6575 - f1_micro: 0.5793 - val_loss: 1.6746 - val_acc: 0.7207 - val_f1_micro: 0.5794\n",
      "Epoch 21/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0297 - acc: 0.6577 - f1_micro: 0.5794 - val_loss: 1.6741 - val_acc: 0.7295 - val_f1_micro: 0.5795\n",
      "Epoch 22/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0365 - acc: 0.6570 - f1_micro: 0.5796 - val_loss: 1.6734 - val_acc: 0.7326 - val_f1_micro: 0.5796\n",
      "Epoch 23/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0379 - acc: 0.6586 - f1_micro: 0.5797 - val_loss: 1.6744 - val_acc: 0.7244 - val_f1_micro: 0.5798\n",
      "Epoch 24/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0325 - acc: 0.6579 - f1_micro: 0.5798 - val_loss: 1.6724 - val_acc: 0.7211 - val_f1_micro: 0.5799\n",
      "Epoch 25/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0337 - acc: 0.6585 - f1_micro: 0.5799 - val_loss: 1.6726 - val_acc: 0.7272 - val_f1_micro: 0.5800\n",
      "Epoch 26/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0365 - acc: 0.6564 - f1_micro: 0.5800 - val_loss: 1.6737 - val_acc: 0.7282 - val_f1_micro: 0.5801\n",
      "Epoch 27/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0234 - acc: 0.6597 - f1_micro: 0.5802 - val_loss: 1.6735 - val_acc: 0.7236 - val_f1_micro: 0.5802\n",
      "Epoch 28/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0326 - acc: 0.6582 - f1_micro: 0.5803 - val_loss: 1.6776 - val_acc: 0.7293 - val_f1_micro: 0.5803\n",
      "Epoch 29/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0367 - acc: 0.6572 - f1_micro: 0.5804 - val_loss: 1.6733 - val_acc: 0.7256 - val_f1_micro: 0.5804\n",
      "Epoch 30/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0355 - acc: 0.6581 - f1_micro: 0.5805 - val_loss: 1.6776 - val_acc: 0.7245 - val_f1_micro: 0.5806\n",
      "Epoch 31/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0342 - acc: 0.6553 - f1_micro: 0.5806 - val_loss: 1.6790 - val_acc: 0.7261 - val_f1_micro: 0.5807\n",
      "Epoch 32/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0356 - acc: 0.6574 - f1_micro: 0.5807 - val_loss: 1.6818 - val_acc: 0.7268 - val_f1_micro: 0.5808\n",
      "Epoch 33/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0301 - acc: 0.6569 - f1_micro: 0.5808 - val_loss: 1.6731 - val_acc: 0.7213 - val_f1_micro: 0.5809\n",
      "Epoch 34/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0230 - acc: 0.6590 - f1_micro: 0.5809 - val_loss: 1.6723 - val_acc: 0.7130 - val_f1_micro: 0.5810\n",
      "Epoch 35/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0293 - acc: 0.6568 - f1_micro: 0.5810 - val_loss: 1.6718 - val_acc: 0.7235 - val_f1_micro: 0.5811\n",
      "Epoch 36/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0257 - acc: 0.6579 - f1_micro: 0.5811 - val_loss: 1.6693 - val_acc: 0.7294 - val_f1_micro: 0.5812\n",
      "Epoch 37/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0264 - acc: 0.6581 - f1_micro: 0.5812 - val_loss: 1.6770 - val_acc: 0.7229 - val_f1_micro: 0.5813\n",
      "Epoch 38/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0284 - acc: 0.6587 - f1_micro: 0.5814 - val_loss: 1.6690 - val_acc: 0.7160 - val_f1_micro: 0.5814\n",
      "Epoch 39/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0218 - acc: 0.6594 - f1_micro: 0.5815 - val_loss: 1.6660 - val_acc: 0.7223 - val_f1_micro: 0.5815\n",
      "Epoch 40/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0300 - acc: 0.6579 - f1_micro: 0.5816 - val_loss: 1.6722 - val_acc: 0.7228 - val_f1_micro: 0.5817\n",
      "Epoch 41/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0315 - acc: 0.6565 - f1_micro: 0.5817 - val_loss: 1.6721 - val_acc: 0.7228 - val_f1_micro: 0.5818\n",
      "Epoch 42/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0260 - acc: 0.6588 - f1_micro: 0.5818 - val_loss: 1.6681 - val_acc: 0.7263 - val_f1_micro: 0.5819\n",
      "Epoch 43/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0260 - acc: 0.6590 - f1_micro: 0.5820 - val_loss: 1.6692 - val_acc: 0.7219 - val_f1_micro: 0.5820\n",
      "Epoch 44/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0284 - acc: 0.6586 - f1_micro: 0.5821 - val_loss: 1.6611 - val_acc: 0.7257 - val_f1_micro: 0.5822\n",
      "Epoch 45/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0314 - acc: 0.6589 - f1_micro: 0.5822 - val_loss: 1.6652 - val_acc: 0.7253 - val_f1_micro: 0.5823\n",
      "Epoch 46/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0302 - acc: 0.6584 - f1_micro: 0.5823 - val_loss: 1.6677 - val_acc: 0.7186 - val_f1_micro: 0.5824\n",
      "Epoch 47/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0220 - acc: 0.6579 - f1_micro: 0.5824 - val_loss: 1.6648 - val_acc: 0.7172 - val_f1_micro: 0.5825\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 2.0232 - acc: 0.6588 - f1_micro: 0.5825 - val_loss: 1.6657 - val_acc: 0.7264 - val_f1_micro: 0.5826\n",
      "Epoch 49/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0220 - acc: 0.6580 - f1_micro: 0.5826 - val_loss: 1.6657 - val_acc: 0.7224 - val_f1_micro: 0.5827\n",
      "Epoch 50/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0270 - acc: 0.6596 - f1_micro: 0.5827 - val_loss: 1.6655 - val_acc: 0.7192 - val_f1_micro: 0.5828\n",
      "Epoch 51/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0264 - acc: 0.6573 - f1_micro: 0.5828 - val_loss: 1.6601 - val_acc: 0.7251 - val_f1_micro: 0.5829\n",
      "Epoch 52/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0308 - acc: 0.6579 - f1_micro: 0.5829 - val_loss: 1.6740 - val_acc: 0.7265 - val_f1_micro: 0.5830\n",
      "Epoch 53/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0270 - acc: 0.6595 - f1_micro: 0.5830 - val_loss: 1.6602 - val_acc: 0.7188 - val_f1_micro: 0.5831\n",
      "Epoch 54/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0244 - acc: 0.6576 - f1_micro: 0.5831 - val_loss: 1.6614 - val_acc: 0.7262 - val_f1_micro: 0.5832\n",
      "Epoch 55/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0228 - acc: 0.6606 - f1_micro: 0.5832 - val_loss: 1.6652 - val_acc: 0.7324 - val_f1_micro: 0.5833\n",
      "Epoch 56/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0298 - acc: 0.6589 - f1_micro: 0.5833 - val_loss: 1.6652 - val_acc: 0.7255 - val_f1_micro: 0.5833\n",
      "Epoch 57/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0280 - acc: 0.6592 - f1_micro: 0.5834 - val_loss: 1.6616 - val_acc: 0.7233 - val_f1_micro: 0.5834\n",
      "Epoch 58/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0214 - acc: 0.6594 - f1_micro: 0.5835 - val_loss: 1.6585 - val_acc: 0.7314 - val_f1_micro: 0.5835\n",
      "Epoch 59/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0207 - acc: 0.6599 - f1_micro: 0.5836 - val_loss: 1.6655 - val_acc: 0.7235 - val_f1_micro: 0.5836\n",
      "Epoch 60/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0231 - acc: 0.6595 - f1_micro: 0.5837 - val_loss: 1.6672 - val_acc: 0.7277 - val_f1_micro: 0.5837\n",
      "Epoch 61/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0237 - acc: 0.6587 - f1_micro: 0.5837 - val_loss: 1.6602 - val_acc: 0.7256 - val_f1_micro: 0.5838\n",
      "Epoch 62/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0209 - acc: 0.6601 - f1_micro: 0.5838 - val_loss: 1.6574 - val_acc: 0.7205 - val_f1_micro: 0.5839\n",
      "Epoch 63/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0200 - acc: 0.6590 - f1_micro: 0.5839 - val_loss: 1.6647 - val_acc: 0.7149 - val_f1_micro: 0.5839\n",
      "Epoch 64/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0208 - acc: 0.6576 - f1_micro: 0.5840 - val_loss: 1.6645 - val_acc: 0.7189 - val_f1_micro: 0.5840\n",
      "Epoch 65/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0226 - acc: 0.6577 - f1_micro: 0.5841 - val_loss: 1.6616 - val_acc: 0.7244 - val_f1_micro: 0.5841\n",
      "Epoch 66/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0320 - acc: 0.6573 - f1_micro: 0.5842 - val_loss: 1.6616 - val_acc: 0.7278 - val_f1_micro: 0.5842\n",
      "Epoch 67/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0182 - acc: 0.6584 - f1_micro: 0.5842 - val_loss: 1.6624 - val_acc: 0.7259 - val_f1_micro: 0.5843\n",
      "Epoch 68/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0190 - acc: 0.6584 - f1_micro: 0.5843 - val_loss: 1.6562 - val_acc: 0.7261 - val_f1_micro: 0.5844\n",
      "Epoch 69/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0125 - acc: 0.6598 - f1_micro: 0.5844 - val_loss: 1.6550 - val_acc: 0.7210 - val_f1_micro: 0.5845\n",
      "Epoch 70/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0150 - acc: 0.6612 - f1_micro: 0.5845 - val_loss: 1.6560 - val_acc: 0.7270 - val_f1_micro: 0.5846\n",
      "Epoch 71/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0232 - acc: 0.6599 - f1_micro: 0.5846 - val_loss: 1.6575 - val_acc: 0.7259 - val_f1_micro: 0.5846\n",
      "Epoch 72/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0243 - acc: 0.6586 - f1_micro: 0.5847 - val_loss: 1.6595 - val_acc: 0.7265 - val_f1_micro: 0.5847\n",
      "Epoch 73/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0199 - acc: 0.6591 - f1_micro: 0.5848 - val_loss: 1.6564 - val_acc: 0.7293 - val_f1_micro: 0.5848\n",
      "Epoch 74/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0247 - acc: 0.6596 - f1_micro: 0.5848 - val_loss: 1.6535 - val_acc: 0.7251 - val_f1_micro: 0.5849\n",
      "Epoch 75/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0203 - acc: 0.6592 - f1_micro: 0.5849 - val_loss: 1.6581 - val_acc: 0.7221 - val_f1_micro: 0.5850\n",
      "Epoch 76/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0200 - acc: 0.6592 - f1_micro: 0.5850 - val_loss: 1.6613 - val_acc: 0.7274 - val_f1_micro: 0.5851\n",
      "Epoch 77/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0220 - acc: 0.6584 - f1_micro: 0.5851 - val_loss: 1.6625 - val_acc: 0.7269 - val_f1_micro: 0.5851\n",
      "Epoch 78/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0150 - acc: 0.6587 - f1_micro: 0.5852 - val_loss: 1.6601 - val_acc: 0.7312 - val_f1_micro: 0.5852\n",
      "Epoch 79/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0246 - acc: 0.6603 - f1_micro: 0.5853 - val_loss: 1.6531 - val_acc: 0.7249 - val_f1_micro: 0.5853\n",
      "Epoch 80/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0212 - acc: 0.6598 - f1_micro: 0.5854 - val_loss: 1.6529 - val_acc: 0.7286 - val_f1_micro: 0.5854\n",
      "Epoch 81/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0174 - acc: 0.6588 - f1_micro: 0.5855 - val_loss: 1.6582 - val_acc: 0.7219 - val_f1_micro: 0.5855\n",
      "Epoch 82/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0121 - acc: 0.6606 - f1_micro: 0.5856 - val_loss: 1.6522 - val_acc: 0.7222 - val_f1_micro: 0.5856\n",
      "Epoch 83/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0163 - acc: 0.6596 - f1_micro: 0.5857 - val_loss: 1.6562 - val_acc: 0.7223 - val_f1_micro: 0.5857\n",
      "Epoch 84/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0195 - acc: 0.6584 - f1_micro: 0.5858 - val_loss: 1.6557 - val_acc: 0.7186 - val_f1_micro: 0.5858\n",
      "Epoch 85/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0129 - acc: 0.6589 - f1_micro: 0.5859 - val_loss: 1.6493 - val_acc: 0.7288 - val_f1_micro: 0.5859\n",
      "Epoch 86/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0172 - acc: 0.6575 - f1_micro: 0.5860 - val_loss: 1.6530 - val_acc: 0.7250 - val_f1_micro: 0.5860\n",
      "Epoch 87/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0186 - acc: 0.6584 - f1_micro: 0.5860 - val_loss: 1.6491 - val_acc: 0.7224 - val_f1_micro: 0.5861\n",
      "Epoch 88/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0123 - acc: 0.6582 - f1_micro: 0.5861 - val_loss: 1.6400 - val_acc: 0.7249 - val_f1_micro: 0.5861\n",
      "Epoch 89/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0104 - acc: 0.6597 - f1_micro: 0.5862 - val_loss: 1.6477 - val_acc: 0.7258 - val_f1_micro: 0.5862\n",
      "Epoch 90/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0172 - acc: 0.6606 - f1_micro: 0.5863 - val_loss: 1.6495 - val_acc: 0.7262 - val_f1_micro: 0.5863\n",
      "Epoch 91/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0152 - acc: 0.6597 - f1_micro: 0.5863 - val_loss: 1.6514 - val_acc: 0.7224 - val_f1_micro: 0.5864\n",
      "Epoch 92/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0105 - acc: 0.6600 - f1_micro: 0.5864 - val_loss: 1.6529 - val_acc: 0.7269 - val_f1_micro: 0.5865\n",
      "Epoch 93/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0157 - acc: 0.6594 - f1_micro: 0.5865 - val_loss: 1.6537 - val_acc: 0.7265 - val_f1_micro: 0.5866\n",
      "Epoch 94/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0190 - acc: 0.6590 - f1_micro: 0.5866 - val_loss: 1.6571 - val_acc: 0.7211 - val_f1_micro: 0.5866\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 2.0185 - acc: 0.6599 - f1_micro: 0.5867 - val_loss: 1.6525 - val_acc: 0.7259 - val_f1_micro: 0.5867\n",
      "Epoch 96/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0104 - acc: 0.6609 - f1_micro: 0.5867 - val_loss: 1.6420 - val_acc: 0.7319 - val_f1_micro: 0.5868\n",
      "Epoch 97/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0104 - acc: 0.6595 - f1_micro: 0.5868 - val_loss: 1.6445 - val_acc: 0.7305 - val_f1_micro: 0.5869\n",
      "Epoch 98/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0160 - acc: 0.6585 - f1_micro: 0.5869 - val_loss: 1.6576 - val_acc: 0.7245 - val_f1_micro: 0.5869\n",
      "Epoch 99/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0152 - acc: 0.6604 - f1_micro: 0.5870 - val_loss: 1.6536 - val_acc: 0.7306 - val_f1_micro: 0.5870\n",
      "Epoch 100/100\n",
      "94731/94731 [==============================] - 0s - loss: 2.0108 - acc: 0.6601 - f1_micro: 0.5871 - val_loss: 1.6446 - val_acc: 0.7264 - val_f1_micro: 0.5871\n",
      "CPU times: user 1min 45s, sys: 10.4 s, total: 1min 56s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# And trained it via:\n",
    "batch_size = 1000\n",
    "model.fit(\n",
    "    {'wv_input': wv_train, 'fs_input': fs_train},\n",
    "    {'main_output': y_train},\n",
    "    epochs=100, batch_size=batch_size,   # 500\n",
    "    validation_split=0.2,\n",
    "    validation_data=(\n",
    "        {'wv_input': wv_val, 'fs_input': fs_val},\n",
    "        {'main_output': y_val}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.2079 - acc: 0.6373 - f1_micro: 0.5371 - val_loss: 1.8958 - val_acc: 0.6907 - val_f1_micro: 0.5373\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.2125 - acc: 0.6369 - f1_micro: 0.5374 - val_loss: 1.8813 - val_acc: 0.6983 - val_f1_micro: 0.5376\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.2040 - acc: 0.6389 - f1_micro: 0.5378 - val_loss: 1.8925 - val_acc: 0.7026 - val_f1_micro: 0.5380\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1954 - acc: 0.6392 - f1_micro: 0.5381 - val_loss: 1.8799 - val_acc: 0.6939 - val_f1_micro: 0.5383\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1939 - acc: 0.6382 - f1_micro: 0.5385 - val_loss: 1.8777 - val_acc: 0.6949 - val_f1_micro: 0.5387\n",
      "\n",
      "Done with epoch: 5\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.2029 - acc: 0.6387 - f1_micro: 0.5389 - val_loss: 1.8782 - val_acc: 0.7000 - val_f1_micro: 0.5390\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.2016 - acc: 0.6398 - f1_micro: 0.5392 - val_loss: 1.8764 - val_acc: 0.7004 - val_f1_micro: 0.5393\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1938 - acc: 0.6425 - f1_micro: 0.5395 - val_loss: 1.8663 - val_acc: 0.7019 - val_f1_micro: 0.5396\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1892 - acc: 0.6418 - f1_micro: 0.5398 - val_loss: 1.8726 - val_acc: 0.6964 - val_f1_micro: 0.5399\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1970 - acc: 0.6388 - f1_micro: 0.5400 - val_loss: 1.8728 - val_acc: 0.7035 - val_f1_micro: 0.5402\n",
      "\n",
      "Done with epoch: 10\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1894 - acc: 0.6418 - f1_micro: 0.5404 - val_loss: 1.8724 - val_acc: 0.7055 - val_f1_micro: 0.5405\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1920 - acc: 0.6406 - f1_micro: 0.5407 - val_loss: 1.8650 - val_acc: 0.7006 - val_f1_micro: 0.5408\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.1836 - acc: 0.6410 - f1_micro: 0.5410 - val_loss: 1.8732 - val_acc: 0.7018 - val_f1_micro: 0.5411\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1982 - acc: 0.6382 - f1_micro: 0.5413 - val_loss: 1.8711 - val_acc: 0.7011 - val_f1_micro: 0.5414\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1842 - acc: 0.6415 - f1_micro: 0.5416 - val_loss: 1.8669 - val_acc: 0.6974 - val_f1_micro: 0.5418\n",
      "\n",
      "Done with epoch: 15\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1812 - acc: 0.6440 - f1_micro: 0.5419 - val_loss: 1.8587 - val_acc: 0.7066 - val_f1_micro: 0.5420\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1842 - acc: 0.6416 - f1_micro: 0.5422 - val_loss: 1.8522 - val_acc: 0.6969 - val_f1_micro: 0.5423\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1782 - acc: 0.6435 - f1_micro: 0.5424 - val_loss: 1.8642 - val_acc: 0.6983 - val_f1_micro: 0.5426\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1785 - acc: 0.6421 - f1_micro: 0.5427 - val_loss: 1.8597 - val_acc: 0.7083 - val_f1_micro: 0.5429\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1816 - acc: 0.6412 - f1_micro: 0.5430 - val_loss: 1.8541 - val_acc: 0.7053 - val_f1_micro: 0.5431\n",
      "\n",
      "Done with epoch: 20\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1765 - acc: 0.6416 - f1_micro: 0.5433 - val_loss: 1.8497 - val_acc: 0.6999 - val_f1_micro: 0.5434\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1765 - acc: 0.6401 - f1_micro: 0.5435 - val_loss: 1.8558 - val_acc: 0.6993 - val_f1_micro: 0.5436\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1777 - acc: 0.6420 - f1_micro: 0.5438 - val_loss: 1.8545 - val_acc: 0.7024 - val_f1_micro: 0.5439\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1879 - acc: 0.6401 - f1_micro: 0.5441 - val_loss: 1.8533 - val_acc: 0.7055 - val_f1_micro: 0.5442\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1793 - acc: 0.6413 - f1_micro: 0.5443 - val_loss: 1.8480 - val_acc: 0.7066 - val_f1_micro: 0.5445\n",
      "\n",
      "Done with epoch: 25\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1711 - acc: 0.6440 - f1_micro: 0.5446 - val_loss: 1.8406 - val_acc: 0.6943 - val_f1_micro: 0.5448\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 1s - loss: 2.1750 - acc: 0.6417 - f1_micro: 0.5449 - val_loss: 1.8435 - val_acc: 0.6985 - val_f1_micro: 0.5451\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1678 - acc: 0.6417 - f1_micro: 0.5452 - val_loss: 1.8347 - val_acc: 0.7032 - val_f1_micro: 0.5453\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1717 - acc: 0.6439 - f1_micro: 0.5455 - val_loss: 1.8376 - val_acc: 0.7048 - val_f1_micro: 0.5456\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1757 - acc: 0.6424 - f1_micro: 0.5457 - val_loss: 1.8383 - val_acc: 0.7051 - val_f1_micro: 0.5459\n",
      "\n",
      "Done with epoch: 30\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1777 - acc: 0.6408 - f1_micro: 0.5460 - val_loss: 1.8339 - val_acc: 0.7130 - val_f1_micro: 0.5461\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1707 - acc: 0.6433 - f1_micro: 0.5463 - val_loss: 1.8412 - val_acc: 0.7093 - val_f1_micro: 0.5464\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1591 - acc: 0.6435 - f1_micro: 0.5465 - val_loss: 1.8370 - val_acc: 0.7049 - val_f1_micro: 0.5467\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1676 - acc: 0.6433 - f1_micro: 0.5468 - val_loss: 1.8412 - val_acc: 0.7039 - val_f1_micro: 0.5469\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1655 - acc: 0.6435 - f1_micro: 0.5470 - val_loss: 1.8413 - val_acc: 0.7077 - val_f1_micro: 0.5471\n",
      "\n",
      "Done with epoch: 35\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1722 - acc: 0.6429 - f1_micro: 0.5473 - val_loss: 1.8366 - val_acc: 0.7049 - val_f1_micro: 0.5474\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1607 - acc: 0.6450 - f1_micro: 0.5475 - val_loss: 1.8269 - val_acc: 0.7105 - val_f1_micro: 0.5477\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1567 - acc: 0.6461 - f1_micro: 0.5478 - val_loss: 1.8326 - val_acc: 0.7001 - val_f1_micro: 0.5479\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1637 - acc: 0.6422 - f1_micro: 0.5480 - val_loss: 1.8337 - val_acc: 0.7017 - val_f1_micro: 0.5482\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1641 - acc: 0.6421 - f1_micro: 0.5483 - val_loss: 1.8296 - val_acc: 0.7043 - val_f1_micro: 0.5484\n",
      "\n",
      "Done with epoch: 40\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1592 - acc: 0.6437 - f1_micro: 0.5485 - val_loss: 1.8345 - val_acc: 0.7130 - val_f1_micro: 0.5486\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1604 - acc: 0.6446 - f1_micro: 0.5488 - val_loss: 1.8183 - val_acc: 0.7039 - val_f1_micro: 0.5489\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1602 - acc: 0.6432 - f1_micro: 0.5490 - val_loss: 1.8300 - val_acc: 0.7047 - val_f1_micro: 0.5491\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1572 - acc: 0.6443 - f1_micro: 0.5492 - val_loss: 1.8165 - val_acc: 0.7149 - val_f1_micro: 0.5493\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1555 - acc: 0.6447 - f1_micro: 0.5494 - val_loss: 1.8238 - val_acc: 0.7013 - val_f1_micro: 0.5495\n",
      "\n",
      "Done with epoch: 45\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 2.1610 - acc: 0.6459 - f1_micro: 0.5496 - val_loss: 1.8267 - val_acc: 0.7042 - val_f1_micro: 0.5497\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1505 - acc: 0.6443 - f1_micro: 0.5499 - val_loss: 1.8233 - val_acc: 0.7041 - val_f1_micro: 0.5500\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1541 - acc: 0.6444 - f1_micro: 0.5501 - val_loss: 1.8110 - val_acc: 0.7074 - val_f1_micro: 0.5502\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1451 - acc: 0.6449 - f1_micro: 0.5503 - val_loss: 1.8193 - val_acc: 0.7013 - val_f1_micro: 0.5504\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1495 - acc: 0.6458 - f1_micro: 0.5505 - val_loss: 1.8126 - val_acc: 0.7045 - val_f1_micro: 0.5507\n",
      "\n",
      "Done with epoch: 50\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1496 - acc: 0.6460 - f1_micro: 0.5508 - val_loss: 1.8163 - val_acc: 0.7079 - val_f1_micro: 0.5509\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1518 - acc: 0.6452 - f1_micro: 0.5510 - val_loss: 1.8166 - val_acc: 0.7114 - val_f1_micro: 0.5512\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1473 - acc: 0.6451 - f1_micro: 0.5513 - val_loss: 1.8158 - val_acc: 0.7064 - val_f1_micro: 0.5514\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1497 - acc: 0.6431 - f1_micro: 0.5516 - val_loss: 1.8231 - val_acc: 0.7051 - val_f1_micro: 0.5517\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1517 - acc: 0.6435 - f1_micro: 0.5518 - val_loss: 1.8127 - val_acc: 0.7036 - val_f1_micro: 0.5519\n",
      "\n",
      "Done with epoch: 55\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1480 - acc: 0.6448 - f1_micro: 0.5520 - val_loss: 1.8088 - val_acc: 0.7020 - val_f1_micro: 0.5521\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1456 - acc: 0.6464 - f1_micro: 0.5523 - val_loss: 1.8057 - val_acc: 0.7091 - val_f1_micro: 0.5524\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1442 - acc: 0.6469 - f1_micro: 0.5525 - val_loss: 1.8037 - val_acc: 0.7145 - val_f1_micro: 0.5526\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1459 - acc: 0.6464 - f1_micro: 0.5527 - val_loss: 1.8106 - val_acc: 0.7000 - val_f1_micro: 0.5528\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1452 - acc: 0.6453 - f1_micro: 0.5529 - val_loss: 1.7955 - val_acc: 0.7034 - val_f1_micro: 0.5531\n",
      "\n",
      "Done with epoch: 60\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1444 - acc: 0.6450 - f1_micro: 0.5532 - val_loss: 1.8036 - val_acc: 0.7164 - val_f1_micro: 0.5533\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1404 - acc: 0.6472 - f1_micro: 0.5534 - val_loss: 1.8010 - val_acc: 0.7037 - val_f1_micro: 0.5536\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1414 - acc: 0.6455 - f1_micro: 0.5537 - val_loss: 1.8000 - val_acc: 0.7094 - val_f1_micro: 0.5538\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1435 - acc: 0.6461 - f1_micro: 0.5539 - val_loss: 1.7963 - val_acc: 0.7073 - val_f1_micro: 0.5540\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1418 - acc: 0.6467 - f1_micro: 0.5541 - val_loss: 1.7980 - val_acc: 0.7060 - val_f1_micro: 0.5542\n",
      "\n",
      "Done with epoch: 65\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1381 - acc: 0.6454 - f1_micro: 0.5543 - val_loss: 1.7951 - val_acc: 0.7101 - val_f1_micro: 0.5544\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1420 - acc: 0.6470 - f1_micro: 0.5546 - val_loss: 1.7998 - val_acc: 0.7087 - val_f1_micro: 0.5547\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1367 - acc: 0.6471 - f1_micro: 0.5548 - val_loss: 1.7926 - val_acc: 0.7118 - val_f1_micro: 0.5549\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1416 - acc: 0.6471 - f1_micro: 0.5550 - val_loss: 1.7955 - val_acc: 0.7128 - val_f1_micro: 0.5551\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1338 - acc: 0.6453 - f1_micro: 0.5552 - val_loss: 1.7897 - val_acc: 0.7070 - val_f1_micro: 0.5553\n",
      "\n",
      "Done with epoch: 70\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1312 - acc: 0.6467 - f1_micro: 0.5554 - val_loss: 1.7987 - val_acc: 0.7073 - val_f1_micro: 0.5555\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1319 - acc: 0.6460 - f1_micro: 0.5556 - val_loss: 1.7926 - val_acc: 0.7038 - val_f1_micro: 0.5557\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1375 - acc: 0.6475 - f1_micro: 0.5558 - val_loss: 1.7970 - val_acc: 0.7114 - val_f1_micro: 0.5559\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1461 - acc: 0.6463 - f1_micro: 0.5560 - val_loss: 1.7994 - val_acc: 0.7072 - val_f1_micro: 0.5561\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1298 - acc: 0.6475 - f1_micro: 0.5562 - val_loss: 1.7848 - val_acc: 0.7115 - val_f1_micro: 0.5563\n",
      "\n",
      "Done with epoch: 75\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1278 - acc: 0.6463 - f1_micro: 0.5564 - val_loss: 1.7830 - val_acc: 0.7125 - val_f1_micro: 0.5565\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1364 - acc: 0.6443 - f1_micro: 0.5566 - val_loss: 1.7908 - val_acc: 0.7065 - val_f1_micro: 0.5567\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1312 - acc: 0.6473 - f1_micro: 0.5568 - val_loss: 1.7940 - val_acc: 0.7079 - val_f1_micro: 0.5569\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1302 - acc: 0.6458 - f1_micro: 0.5570 - val_loss: 1.7753 - val_acc: 0.7061 - val_f1_micro: 0.5571\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1308 - acc: 0.6471 - f1_micro: 0.5573 - val_loss: 1.7932 - val_acc: 0.7071 - val_f1_micro: 0.5574\n",
      "\n",
      "Done with epoch: 80\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1338 - acc: 0.6475 - f1_micro: 0.5575 - val_loss: 1.7843 - val_acc: 0.7074 - val_f1_micro: 0.5576\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1275 - acc: 0.6464 - f1_micro: 0.5577 - val_loss: 1.7841 - val_acc: 0.7076 - val_f1_micro: 0.5578\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1214 - acc: 0.6463 - f1_micro: 0.5579 - val_loss: 1.7816 - val_acc: 0.7101 - val_f1_micro: 0.5580\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1286 - acc: 0.6464 - f1_micro: 0.5581 - val_loss: 1.7798 - val_acc: 0.7079 - val_f1_micro: 0.5582\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1295 - acc: 0.6459 - f1_micro: 0.5583 - val_loss: 1.7876 - val_acc: 0.7088 - val_f1_micro: 0.5584\n",
      "\n",
      "Done with epoch: 85\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1194 - acc: 0.6473 - f1_micro: 0.5584 - val_loss: 1.7732 - val_acc: 0.7115 - val_f1_micro: 0.5585\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1265 - acc: 0.6462 - f1_micro: 0.5586 - val_loss: 1.7815 - val_acc: 0.7106 - val_f1_micro: 0.5587\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1268 - acc: 0.6466 - f1_micro: 0.5588 - val_loss: 1.7718 - val_acc: 0.7182 - val_f1_micro: 0.5589\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1227 - acc: 0.6476 - f1_micro: 0.5590 - val_loss: 1.7803 - val_acc: 0.7070 - val_f1_micro: 0.5591\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1204 - acc: 0.6488 - f1_micro: 0.5592 - val_loss: 1.7772 - val_acc: 0.7142 - val_f1_micro: 0.5593\n",
      "\n",
      "Done with epoch: 90\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 0s - loss: 2.1234 - acc: 0.6463 - f1_micro: 0.5594 - val_loss: 1.7688 - val_acc: 0.7053 - val_f1_micro: 0.5595\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1227 - acc: 0.6479 - f1_micro: 0.5596 - val_loss: 1.7825 - val_acc: 0.7105 - val_f1_micro: 0.5597\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1164 - acc: 0.6495 - f1_micro: 0.5598 - val_loss: 1.7733 - val_acc: 0.7131 - val_f1_micro: 0.5599\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1156 - acc: 0.6490 - f1_micro: 0.5599 - val_loss: 1.7702 - val_acc: 0.7131 - val_f1_micro: 0.5600\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1195 - acc: 0.6493 - f1_micro: 0.5601 - val_loss: 1.7648 - val_acc: 0.7049 - val_f1_micro: 0.5602\n",
      "\n",
      "Done with epoch: 95\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1284 - acc: 0.6472 - f1_micro: 0.5603 - val_loss: 1.7740 - val_acc: 0.7097 - val_f1_micro: 0.5604\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1204 - acc: 0.6473 - f1_micro: 0.5605 - val_loss: 1.7677 - val_acc: 0.7053 - val_f1_micro: 0.5605\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1191 - acc: 0.6480 - f1_micro: 0.5606 - val_loss: 1.7721 - val_acc: 0.7189 - val_f1_micro: 0.5607\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1158 - acc: 0.6479 - f1_micro: 0.5608 - val_loss: 1.7709 - val_acc: 0.7072 - val_f1_micro: 0.5609\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 0s - loss: 2.1138 - acc: 0.6488 - f1_micro: 0.5610 - val_loss: 1.7656 - val_acc: 0.7133 - val_f1_micro: 0.5611\n",
      "\n",
      "Done with epoch: 100\n",
      "\n",
      "CPU times: user 1min 58s, sys: 11.2 s, total: 2min 10s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = 'models/lstm-word2vec-fasttext_2010-2014-data_cat-crossentropy-2014-b-val-sc_tfidf_wv_fs.model'\n",
    "epochs = 5\n",
    "for i in xrange(0, 100 // epochs):\n",
    "    hist = model.fit(\n",
    "        {'main_input': x_train, 'wv_input': wv_train, 'fs_input': fs_train},\n",
    "        {'main_output': y_train, 'aux_output': y_train},\n",
    "        epochs=epochs, batch_size=batch_size,   # 500\n",
    "        validation_split=0.2,\n",
    "        validation_data=(\n",
    "            {'main_input': x_val, 'wv_input': wv_val, 'fs_input': fs_val},\n",
    "            {'main_output': y_val, 'aux_output': y_val}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save(model_name.format(i))\n",
    "    print\n",
    "    print('Done with epoch: {}'.format((i + 1) * epochs))\n",
    "    with open('lstm-word2vec-fasttext.epoch.csv', 'a') as fl:\n",
    "        fl.write(model_name + '\\n')\n",
    "        fl.write('Epoch {}\\n'.format((i + 1) * epochs))\n",
    "        fl.write('{}\\n'.format(datetime.now()))\n",
    "        fl.write('\\n'.join(['{}: {}'.format(k, v[0]) for k, v in hist.history.items()]))\n",
    "        fl.write('\\n\\n')\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.6427 - main_output_loss: 1.3490 - aux_output_loss: 1.4686 - main_output_acc: 0.7712 - main_output_f1_micro: 0.6905 - aux_output_acc: 0.7839 - aux_output_f1_micro: 0.1085 - val_loss: 1.4372 - val_main_output_loss: 1.1302 - val_aux_output_loss: 1.5348 - val_main_output_acc: 0.7941 - val_main_output_f1_micro: 0.6913 - val_aux_output_acc: 0.7884 - val_aux_output_f1_micro: 0.1086\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5833 - main_output_loss: 1.2980 - aux_output_loss: 1.4265 - main_output_acc: 0.7738 - main_output_f1_micro: 0.6921 - aux_output_acc: 0.7855 - aux_output_f1_micro: 0.1088 - val_loss: 1.4210 - val_main_output_loss: 1.1179 - val_aux_output_loss: 1.5153 - val_main_output_acc: 0.7967 - val_main_output_f1_micro: 0.6930 - val_aux_output_acc: 0.7949 - val_aux_output_f1_micro: 0.1090\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5656 - main_output_loss: 1.2841 - aux_output_loss: 1.4076 - main_output_acc: 0.7752 - main_output_f1_micro: 0.6938 - aux_output_acc: 0.7870 - aux_output_f1_micro: 0.1091 - val_loss: 1.4123 - val_main_output_loss: 1.1124 - val_aux_output_loss: 1.4995 - val_main_output_acc: 0.7930 - val_main_output_f1_micro: 0.6946 - val_aux_output_acc: 0.7916 - val_aux_output_f1_micro: 0.1093\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5551 - main_output_loss: 1.2755 - aux_output_loss: 1.3983 - main_output_acc: 0.7779 - main_output_f1_micro: 0.6955 - aux_output_acc: 0.7867 - aux_output_f1_micro: 0.1094 - val_loss: 1.4043 - val_main_output_loss: 1.1070 - val_aux_output_loss: 1.4866 - val_main_output_acc: 0.7988 - val_main_output_f1_micro: 0.6963 - val_aux_output_acc: 0.7918 - val_aux_output_f1_micro: 0.1096\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5504 - main_output_loss: 1.2725 - aux_output_loss: 1.3899 - main_output_acc: 0.7771 - main_output_f1_micro: 0.6971 - aux_output_acc: 0.7872 - aux_output_f1_micro: 0.1097 - val_loss: 1.4002 - val_main_output_loss: 1.1043 - val_aux_output_loss: 1.4796 - val_main_output_acc: 0.8088 - val_main_output_f1_micro: 0.6980 - val_aux_output_acc: 0.7916 - val_aux_output_f1_micro: 0.1099\n",
      "\n",
      "Done with epoch: 100\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5459 - main_output_loss: 1.2693 - aux_output_loss: 1.3833 - main_output_acc: 0.7773 - main_output_f1_micro: 0.6988 - aux_output_acc: 0.7896 - aux_output_f1_micro: 0.1100 - val_loss: 1.3938 - val_main_output_loss: 1.0999 - val_aux_output_loss: 1.4694 - val_main_output_acc: 0.8001 - val_main_output_f1_micro: 0.6996 - val_aux_output_acc: 0.7926 - val_aux_output_f1_micro: 0.1102\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5393 - main_output_loss: 1.2640 - aux_output_loss: 1.3764 - main_output_acc: 0.7776 - main_output_f1_micro: 0.7004 - aux_output_acc: 0.7896 - aux_output_f1_micro: 0.1103 - val_loss: 1.3912 - val_main_output_loss: 1.0979 - val_aux_output_loss: 1.4664 - val_main_output_acc: 0.7940 - val_main_output_f1_micro: 0.7012 - val_aux_output_acc: 0.7917 - val_aux_output_f1_micro: 0.1105\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5369 - main_output_loss: 1.2627 - aux_output_loss: 1.3708 - main_output_acc: 0.7815 - main_output_f1_micro: 0.7020 - aux_output_acc: 0.7890 - aux_output_f1_micro: 0.1106 - val_loss: 1.3821 - val_main_output_loss: 1.0902 - val_aux_output_loss: 1.4594 - val_main_output_acc: 0.8066 - val_main_output_f1_micro: 0.7028 - val_aux_output_acc: 0.7940 - val_aux_output_f1_micro: 0.1108\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5302 - main_output_loss: 1.2571 - aux_output_loss: 1.3656 - main_output_acc: 0.7796 - main_output_f1_micro: 0.7036 - aux_output_acc: 0.7897 - aux_output_f1_micro: 0.1109 - val_loss: 1.3878 - val_main_output_loss: 1.0954 - val_aux_output_loss: 1.4620 - val_main_output_acc: 0.7964 - val_main_output_f1_micro: 0.7044 - val_aux_output_acc: 0.7954 - val_aux_output_f1_micro: 0.1111\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5282 - main_output_loss: 1.2561 - aux_output_loss: 1.3605 - main_output_acc: 0.7775 - main_output_f1_micro: 0.7052 - aux_output_acc: 0.7913 - aux_output_f1_micro: 0.1112 - val_loss: 1.3855 - val_main_output_loss: 1.0954 - val_aux_output_loss: 1.4505 - val_main_output_acc: 0.7928 - val_main_output_f1_micro: 0.7059 - val_aux_output_acc: 0.7964 - val_aux_output_f1_micro: 0.1114\n",
      "\n",
      "Done with epoch: 105\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5262 - main_output_loss: 1.2551 - aux_output_loss: 1.3553 - main_output_acc: 0.7791 - main_output_f1_micro: 0.7067 - aux_output_acc: 0.7902 - aux_output_f1_micro: 0.1115 - val_loss: 1.3707 - val_main_output_loss: 1.0832 - val_aux_output_loss: 1.4377 - val_main_output_acc: 0.8034 - val_main_output_f1_micro: 0.7075 - val_aux_output_acc: 0.7948 - val_aux_output_f1_micro: 0.1117\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5205 - main_output_loss: 1.2508 - aux_output_loss: 1.3484 - main_output_acc: 0.7803 - main_output_f1_micro: 0.7083 - aux_output_acc: 0.7910 - aux_output_f1_micro: 0.1118 - val_loss: 1.3696 - val_main_output_loss: 1.0827 - val_aux_output_loss: 1.4348 - val_main_output_acc: 0.8018 - val_main_output_f1_micro: 0.7090 - val_aux_output_acc: 0.7929 - val_aux_output_f1_micro: 0.1120\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5220 - main_output_loss: 1.2527 - aux_output_loss: 1.3466 - main_output_acc: 0.7793 - main_output_f1_micro: 0.7098 - aux_output_acc: 0.7904 - aux_output_f1_micro: 0.1122 - val_loss: 1.3695 - val_main_output_loss: 1.0827 - val_aux_output_loss: 1.4341 - val_main_output_acc: 0.7963 - val_main_output_f1_micro: 0.7105 - val_aux_output_acc: 0.7926 - val_aux_output_f1_micro: 0.1123\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5216 - main_output_loss: 1.2530 - aux_output_loss: 1.3431 - main_output_acc: 0.7800 - main_output_f1_micro: 0.7113 - aux_output_acc: 0.7914 - aux_output_f1_micro: 0.1125 - val_loss: 1.3708 - val_main_output_loss: 1.0860 - val_aux_output_loss: 1.4240 - val_main_output_acc: 0.8011 - val_main_output_f1_micro: 0.7120 - val_aux_output_acc: 0.7966 - val_aux_output_f1_micro: 0.1126\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5163 - main_output_loss: 1.2486 - aux_output_loss: 1.3386 - main_output_acc: 0.7807 - main_output_f1_micro: 0.7128 - aux_output_acc: 0.7916 - aux_output_f1_micro: 0.1128 - val_loss: 1.3660 - val_main_output_loss: 1.0814 - val_aux_output_loss: 1.4232 - val_main_output_acc: 0.7929 - val_main_output_f1_micro: 0.7135 - val_aux_output_acc: 0.7930 - val_aux_output_f1_micro: 0.1130\n",
      "\n",
      "Done with epoch: 110\n",
      "\n",
      "Train on 94731 samples, validate on 9424 samples\n",
      "Epoch 1/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5152 - main_output_loss: 1.2484 - aux_output_loss: 1.3341 - main_output_acc: 0.7792 - main_output_f1_micro: 0.7142 - aux_output_acc: 0.7911 - aux_output_f1_micro: 0.1131 - val_loss: 1.3651 - val_main_output_loss: 1.0816 - val_aux_output_loss: 1.4173 - val_main_output_acc: 0.7956 - val_main_output_f1_micro: 0.7149 - val_aux_output_acc: 0.7927 - val_aux_output_f1_micro: 0.1133\n",
      "Epoch 2/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5089 - main_output_loss: 1.2427 - aux_output_loss: 1.3309 - main_output_acc: 0.7803 - main_output_f1_micro: 0.7156 - aux_output_acc: 0.7912 - aux_output_f1_micro: 0.1135 - val_loss: 1.3627 - val_main_output_loss: 1.0794 - val_aux_output_loss: 1.4167 - val_main_output_acc: 0.7989 - val_main_output_f1_micro: 0.7164 - val_aux_output_acc: 0.7968 - val_aux_output_f1_micro: 0.1136\n",
      "Epoch 3/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5082 - main_output_loss: 1.2431 - aux_output_loss: 1.3254 - main_output_acc: 0.7813 - main_output_f1_micro: 0.7171 - aux_output_acc: 0.7917 - aux_output_f1_micro: 0.1138 - val_loss: 1.3610 - val_main_output_loss: 1.0787 - val_aux_output_loss: 1.4114 - val_main_output_acc: 0.8054 - val_main_output_f1_micro: 0.7178 - val_aux_output_acc: 0.7916 - val_aux_output_f1_micro: 0.1139\n",
      "Epoch 4/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5043 - main_output_loss: 1.2400 - aux_output_loss: 1.3214 - main_output_acc: 0.7788 - main_output_f1_micro: 0.7185 - aux_output_acc: 0.7905 - aux_output_f1_micro: 0.1141 - val_loss: 1.3573 - val_main_output_loss: 1.0764 - val_aux_output_loss: 1.4045 - val_main_output_acc: 0.8025 - val_main_output_f1_micro: 0.7192 - val_aux_output_acc: 0.7962 - val_aux_output_f1_micro: 0.1143\n",
      "Epoch 5/5\n",
      "94731/94731 [==============================] - 51s - loss: 1.5045 - main_output_loss: 1.2409 - aux_output_loss: 1.3181 - main_output_acc: 0.7785 - main_output_f1_micro: 0.7199 - aux_output_acc: 0.7919 - aux_output_f1_micro: 0.1145 - val_loss: 1.3638 - val_main_output_loss: 1.0828 - val_aux_output_loss: 1.4050 - val_main_output_acc: 0.7982 - val_main_output_f1_micro: 0.7205 - val_aux_output_acc: 0.7963 - val_aux_output_f1_micro: 0.1146\n",
      "\n",
      "Done with epoch: 115\n",
      "\n",
      "CPU times: user 20min 20s, sys: 1min 53s, total: 22min 14s\n",
      "Wall time: 17min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# i = 100\n",
    "# batch_size = 600\n",
    "batch_size = 1200\n",
    "for j in xrange(i, i + (20 // epochs)):\n",
    "    hist = model.fit(\n",
    "        {'main_input': x_train, 'wv_input': wv_train, 'fs_input': fs_train},\n",
    "        {'main_output': y_train, 'aux_output': y_train},\n",
    "        epochs=epochs, batch_size=batch_size,   # 500\n",
    "        validation_split=0.2,\n",
    "        validation_data=(\n",
    "            {'main_input': x_val, 'wv_input': wv_val, 'fs_input': fs_val},\n",
    "            {'main_output': y_val, 'aux_output': y_val}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save(model_name.format(i))\n",
    "    print\n",
    "    print('Done with epoch: {}'.format((j + 1) * epochs))\n",
    "    with open('lstm-word2vec-fasttext.epoch.csv', 'a') as fl:\n",
    "        fl.write(model_name + '\\n')\n",
    "        fl.write('Epoch {}\\n'.format((j + 1) * epochs))\n",
    "        fl.write('{}\\n'.format(datetime.now()))\n",
    "        fl.write('\\n'.join(['{}: {}'.format(k, v[0]) for k, v in hist.history.items()]))\n",
    "        fl.write('\\n\\n')\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\n",
    "#     'models/lstm-word2vec-fasttext_2010-2014-data_categorical-crossentropy-2014-b-val-standard_scaled_wv_fs.model',\n",
    "#     custom_objects={'f1_micro': f1_micro}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = model.predict({'wv_input': wv_train[:100], 'fs_input': fs_train[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0af7f9e5d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09bd27bbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ix = 29\n",
    "pd.Series(g[ix]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([ 1, 98]),), (array([ 1, 98]),))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "np.where(y_train[ix] == 1), np.where(g[ix] > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = TfidfModel.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.dictionary')\n",
    "tfidf = TfidfModel.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = Word2Vec.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsmodel = fasttext.load_model('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_tfidf_word2vec(tokens, stopwords=[]):\n",
    "#     global wvmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    wv_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords and w in wvmodel.wv.vocab)]\n",
    "    ).map(\n",
    "        lambda x: tfidf[dictionary.doc2bow(x)]\n",
    "    ).map(\n",
    "        lambda x: np.array([wvmodel[dictionary.id2token[id]] * w for id, w in x]).mean(axis=0) if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return wv_feature_vec\n",
    "\n",
    "\n",
    "def transform_tfidf_fasttext(tokens, stopwords=[]):\n",
    "#     global fsmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    fs_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(\n",
    "        lambda x: tfidf[dictionary.doc2bow(x)]\n",
    "    ).map(\n",
    "        lambda x: np.array([np.array(fsmodel[dictionary.id2token[id]]) * w for id, w in x]).mean(axis=0) if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return fs_feature_vec\n",
    "\n",
    "def transform_fasttext(tokens, stopwords=[]):\n",
    "    global fsmodel\n",
    "    # This requires fsmodel to be present in the namespace.\n",
    "    fs_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(lambda x: np.array([fsmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return fs_feature_vec\n",
    "\n",
    "\n",
    "def transform_unsupervised_sentiment_neuron(tokens, stopwords=[]):\n",
    "    # This requires fsmodel to be present in the namespace.\n",
    "    \n",
    "    usn_feature_vec = usnmodel.transform(tokens)\n",
    "\n",
    "    # usn_feature_vec = tokens.map(\n",
    "    #     lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    # ).map(lambda x: np.array([usnmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return usn_feature_vec\n",
    "\n",
    "\n",
    "def transform_word2vec(tokens, stopwords=[]):\n",
    "    global wvmodel\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    wv_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords and w in wvmodel.wv.vocab)]\n",
    "    ).map(lambda x: np.array([wvmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return wv_feature_vec\n",
    "\n",
    "\n",
    "def parallel_generate_word_vectors(samp, transformer, stopwords, batch, num_proc):\n",
    "    with Parallel(n_jobs=num_proc) as parallel:\n",
    "        dataset = []\n",
    "        is_break = False\n",
    "        i = 0\n",
    "\n",
    "        while not is_break:\n",
    "            payload = []\n",
    "\n",
    "            for j in xrange(num_proc):\n",
    "                t_df = samp[(i + j) * batch: (i + 1 + j) * batch]\n",
    "\n",
    "                if t_df.empty:\n",
    "                    is_break = True\n",
    "                    continue\n",
    "\n",
    "                payload.append(\n",
    "                    delayed(transformer)(\n",
    "                        t_df, stopwords\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            print('Current batch in main thread: {}'.format((i + j) * batch))\n",
    "\n",
    "            if payload:\n",
    "                results = parallel(payload)\n",
    "                dataset.extend(results)\n",
    "                i += num_proc\n",
    "\n",
    "    return pd.concat(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(pred, scale_param=0.75, min_thresh=0.05, thresh = 0.5):\n",
    "#     mx = pred.mean() + 3 * pred.std()\n",
    "    return np.where(pred > thresh)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2idx_transform(word, _word2idx):\n",
    "    return _word2idx.get(word, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_for(df, min_batch=2000, stopwords=[], num_proc=7):\n",
    "    df_tokens = transform_text(df)\n",
    "    \n",
    "    batch = min(df_tokens.shape[0] / num_proc, min_batch)\n",
    "\n",
    "    print('Computing fs features...')\n",
    "    fvec = parallel_generate_word_vectors(df_tokens, transform_fasttext, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    print('Computing wv features...')\n",
    "    wvec = parallel_generate_word_vectors(df_tokens, transform_word2vec, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    print('Mapping word indices...')\n",
    "    word_indices = df_tokens.map(lambda x: [word2idx_transform(i, _word2idx) for i in x.split()])\n",
    "\n",
    "    print('Computing tfidf fs features...')\n",
    "    tfidf_fvec = parallel_generate_word_vectors(df_tokens, transform_tfidf_fasttext, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    print('Computing tfidf wv features...')\n",
    "    tfidf_wvec = parallel_generate_word_vectors(df_tokens, transform_tfidf_word2vec, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "    \n",
    "    return word_indices, wvec, fvec, tfidf_wvec, tfidf_fvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/TestData.json') as fl:\n",
    "    data = json.load(fl)\n",
    "    test_df = pd.DataFrame(data['TestData']).T\n",
    "    del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fs features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "Mapping word indices...\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 6498\n",
      "Current batch in main thread: 14079\n",
      "CPU times: user 50.3 s, sys: 4.88 s, total: 55.2 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_word_indices,test_wvec, test_fvec, test_tfidf_wvec, test_tfidf_fvec = extract_features_for(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(np.all(test_wvec[test_wvec.isnull()].index == test_fvec[test_fvec.isnull()].index))\n",
    "test_null_index = test_wvec[test_wvec.isnull()].index.union(test_fvec[test_fvec.isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'TestData_02543', u'TestData_05012', u'TestData_05830'], dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_null_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 32 ms, total: 1.21 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_test_index = test_word_indices.index.difference(test_null_index)\n",
    "x_test = test_word_indices.ix[valid_test_index].map(lambda x: [top_token2ind.get(i, 0) for i in x])\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "wv_test = np.vstack(test_wvec.ix[valid_test_index])\n",
    "fs_test = np.vstack(test_fvec.ix[valid_test_index])\n",
    "\n",
    "\n",
    "wv_test = wv_sc.transform(wv_test)\n",
    "fs_test = fs_sc.transform(fs_test)\n",
    "\n",
    "tfidf_wv_test = np.vstack(test_tfidf_wvec.ix[valid_test_index])\n",
    "tfidf_fs_test = np.vstack(test_tfidf_fvec.ix[valid_test_index])\n",
    "\n",
    "\n",
    "tfidf_wv_test = wv_sc.transform(tfidf_wv_test)\n",
    "tfidf_fs_test = fs_sc.transform(tfidf_fs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 500\n",
    "# test_probas = model.predict({'wv_input': wv_test, 'fs_input': fs_test}, batch_size=batch_size)\n",
    "test_probas = model.predict({'wv_input': tfidf_wv_test, 'fs_input': tfidf_fs_test}, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_test_probas = test_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.33391931e-09,   7.43984041e-11, ...,\n",
       "          6.42937700e-12,   1.90089215e-21,   0.00000000e+00],\n",
       "       [  2.31159530e-27,   1.16103211e-05,   5.12909501e-05, ...,\n",
       "          2.64404264e-07,   4.55858071e-08,   3.41608165e-27],\n",
       "       [  2.08303697e-35,   1.55344235e-08,   1.33646916e-08, ...,\n",
       "          8.23759401e-05,   4.98176641e-13,   1.61130501e-35],\n",
       "       ..., \n",
       "       [  5.49547242e-18,   4.37888224e-03,   3.03665251e-01, ...,\n",
       "          1.73911925e-08,   2.18975008e-03,   1.15527618e-17],\n",
       "       [  1.71496351e-20,   2.05606711e-03,   1.30320399e-03, ...,\n",
       "          1.34235484e-06,   1.37788322e-06,   6.11972451e-20],\n",
       "       [  9.46437685e-24,   3.17608385e-04,   7.27922074e-04, ...,\n",
       "          3.42477193e-08,   4.01360580e-08,   3.97110055e-23]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_test_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_df.ix[test_df.index.difference(test_null_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2542, 5011, 5829]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_index = [int(s.split('_')[1]) - 1 for s in test_null_index]  # Subtract 1 since test index starts at 1 while enumerate starts at 0\n",
    "skip_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7578, 160), (7581, 3))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_test_probas.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 31.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# valid_test_feature_vec found below!\n",
    "test_values = np.zeros([main_test_probas.shape[0], len(topics)])\n",
    "for ix, pred in enumerate(main_test_probas):\n",
    "    for v in get_classes(pred, thresh=0.3):\n",
    "        test_values[ix][v] = 1\n",
    "\n",
    "test_sub_df = pd.DataFrame(\n",
    "    test_values,\n",
    "    index=test_df.ix[test_df.index.difference(test_null_index)].index,\n",
    "    columns=topics\n",
    ")\n",
    "\n",
    "null_test_df = pd.DataFrame(\n",
    "    np.zeros((len(test_null_index), len(topics))),\n",
    "    index=test_null_index,\n",
    "    columns=topics\n",
    ")\n",
    "\n",
    "test_sub_df = test_sub_df.append(null_test_df)\n",
    "test_sub_df = test_sub_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9627 (0.5), 14297 (0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11433.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94731/94731 [==============================] - 51s - loss: 1.5045 - main_output_loss: 1.2409 - aux_output_loss: 1.3181 - main_output_acc: 0.7785 - main_output_f1_micro: 0.7199 - aux_output_acc: 0.7919 - aux_output_f1_micro: 0.1145 - val_loss: 1.3638 - val_main_output_loss: 1.0828 - val_aux_output_loss: 1.4050 - val_main_output_acc: 0.7982 - val_main_output_f1_micro: 0.7205 - val_aux_output_acc: 0.7963 - val_aux_output_f1_micro: 0.1146\n"
     ]
    }
   ],
   "source": [
    "print '94731/94731 [==============================] - 51s - loss: 1.5045 - main_output_loss: 1.2409 - aux_output_loss: 1.3181 - main_output_acc: 0.7785 - main_output_f1_micro: 0.7199 - aux_output_acc: 0.7919 - aux_output_f1_micro: 0.1145 - val_loss: 1.3638 - val_main_output_loss: 1.0828 - val_aux_output_loss: 1.4050 - val_main_output_acc: 0.7982 - val_main_output_f1_micro: 0.7205 - val_aux_output_acc: 0.7963 - val_aux_output_f1_micro: 0.1146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sub_df.astype(int).reset_index().rename(\n",
    "    columns={'index': 'id'}\n",
    ").sort_values('id').to_csv(\n",
    "    'tfidf_word2vec_300-fasttext_300-deep_stack_net-data_2010_2014-val_data_2014-thresh_0.3-with_sc_wv_fs.csv', \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7581, 160)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TestData_04490\tThe World Health Organisation has convened an ...\t[]\t28-01-2016\n",
    "TestData_04550\tSpraying pesticides will fail to deal with the...\t[]\t02-02-2016\n",
    "TestData_05683\tViolent protests at Trump rally in California ...\t[]\t03-06-2016\n",
    "TestData_05869\tLast weekend, we saw the darkest side of human...\t[]\t17-06-2016\n",
    "TestData_06148\tAs dusk falls over Copacabana beach, Ubira San...\t[]\t16-07-2016\n",
    "TestData_06291\tIt is 3pm and yet another patient is brought t...\t[]\t27-07-2016\n",
    "TestData_06610\tHuddled around their hives, beekeepers around ...\t[]\t04-09-2016\n",
    "TestData_06708\tA United Nations high-level panel on access to...\t[]\t14-09-2016\n",
    "TestData_07263\tWHO: Zika virus is no longer a world threat Th...\t[]\t19-11-2016\n",
    "TestData_07478\t1 World Health Organisation declares a public ...\t[]\t18-12-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guncrime    1.0\n",
       "Name: TestData_05683, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 5682\n",
    "test_sub_df.iloc[ix][test_sub_df.iloc[ix] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 32.6 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# adjust_index = 0\n",
    "# # valid_test_feature_vec found below!\n",
    "# test_values = np.zeros([test_df.shape[0], len(topics)])\n",
    "# for ix, pred in enumerate(main_test_probas):\n",
    "#     if ix in skip_index:\n",
    "#         test_values[ix] = np.nan\n",
    "#         # Increment adjust index so that we have the correct index for other samples\n",
    "#         adjust_index += 1\n",
    "#         continue\n",
    "\n",
    "#     for v in get_classes(pred, thresh=0.05):\n",
    "#         test_values[ix + adjust_index][v] = 1\n",
    "\n",
    "# test_sub_df = pd.DataFrame(test_values, columns=sorted(topics), index=test_df.index)\n",
    "\n",
    "# q = test_sub_df.sum(axis=1)\n",
    "# assert(len(q[q.isnull()].index.difference(test_null_index)) == 0)\n",
    "\n",
    "# test_sub_df = test_sub_df.fillna(0)\n",
    "\n",
    "# # for i in test_feature_vec[test_feature_vec.isnull()].index:\n",
    "# #     test_sub_df.ix[i] = np.zeros(len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestData_02543    0.0\n",
       "TestData_05012    0.0\n",
       "TestData_05830    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.ix[test_null_index].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11656.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sub_df.astype(int).reset_index().rename(\n",
    "    columns={'index': 'id'}\n",
    ").sort_values('id').to_csv(\n",
    "    'lstm_300-word2vec_300-fasttext_300-maxlen_500-dense_64_64_64-cat_cross-epoch_210-batch_size_750-val_main_output_f1_micro_0.5760-main_output_f1_micro_0.5751-main_output_loss_0.9143-data_2010_2013-val_data_2014-thresh_0.05.csv', \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: zikavirus, dtype: float64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = test_sub_df['zikavirus']\n",
    "e[e==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14328"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_submission = pd.read_csv('basic_nn_submission_0.649_accuracy_multi_class.csv')\n",
    "top_submission.set_index('id').sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9280"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_index_lstm_sub = pd.read_csv('lstm.2014b_training_700_maxlen_64cell_100epochs_0.0025_threshold.csv')\n",
    "wrong_index_lstm_sub.set_index('id').sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34952"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_sub = pd.read_csv('basic_nn_submission_full_training_data_0.9958_validation_accuracy_binary_crossentropy.csv')\n",
    "some_sub.set_index('id').sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2197, 160)\n",
      "(3957, 160)\n",
      "(12, 160)\n",
      "(1503, 160)\n"
     ]
    }
   ],
   "source": [
    "print top_submission.set_index('id')[top_submission.set_index('id').sum(axis=1) == 0].shape\n",
    "\n",
    "print wrong_index_lstm_sub.set_index('id')[wrong_index_lstm_sub.set_index('id').sum(axis=1) == 0].shape\n",
    "\n",
    "print some_sub.set_index('id')[some_sub.set_index('id').sum(axis=1) == 0].shape\n",
    "\n",
    "print test_sub_df[test_sub_df.sum(axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestData_00011     0\n",
       "TestData_00012     0\n",
       "TestData_00015     0\n",
       "TestData_00027     3\n",
       "TestData_00029     0\n",
       "TestData_00038     1\n",
       "TestData_00042     5\n",
       "TestData_00053     4\n",
       "TestData_00056     1\n",
       "TestData_00060     1\n",
       "TestData_00066     0\n",
       "TestData_00085     0\n",
       "TestData_00087     1\n",
       "TestData_00090     0\n",
       "TestData_00092     0\n",
       "TestData_00107     3\n",
       "TestData_00111     0\n",
       "TestData_00114     0\n",
       "TestData_00115     1\n",
       "TestData_00118     0\n",
       "TestData_00119     0\n",
       "TestData_00121     0\n",
       "TestData_00123     0\n",
       "TestData_00125     0\n",
       "TestData_00127     0\n",
       "TestData_00128     1\n",
       "TestData_00139     1\n",
       "TestData_00140     1\n",
       "TestData_00144     0\n",
       "TestData_00147     2\n",
       "                  ..\n",
       "TestData_07445     0\n",
       "TestData_07456     3\n",
       "TestData_07461     1\n",
       "TestData_07462     4\n",
       "TestData_07465     0\n",
       "TestData_07468     0\n",
       "TestData_07471     1\n",
       "TestData_07475     0\n",
       "TestData_07486    10\n",
       "TestData_07495     1\n",
       "TestData_07509     0\n",
       "TestData_07514     3\n",
       "TestData_07515     1\n",
       "TestData_07523     0\n",
       "TestData_07533     2\n",
       "TestData_07534     2\n",
       "TestData_07542     1\n",
       "TestData_07544     2\n",
       "TestData_07545     0\n",
       "TestData_07552     2\n",
       "TestData_07556     5\n",
       "TestData_07563     1\n",
       "TestData_07565     0\n",
       "TestData_07566     0\n",
       "TestData_07569     0\n",
       "TestData_07571     3\n",
       "TestData_07572     1\n",
       "TestData_07579     6\n",
       "TestData_07580     2\n",
       "TestData_07581     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_submission.set_index('id').ix[q[q == 0].index].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_sub_df.sum(axis=1)\n",
    "q[q==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7581.000000\n",
       "mean        2.160929\n",
       "std         1.739411\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max        13.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = trainingY.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    236286.000000\n",
       "mean          1.392787\n",
       "std           0.762577\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           1.000000\n",
       "75%           2.000000\n",
       "max          15.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bodyText</th>\n",
       "      <th>topics</th>\n",
       "      <th>webPublicationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TestData_03241</th>\n",
       "      <td>A special British police unit was put on stand...</td>\n",
       "      <td>[]</td>\n",
       "      <td>15-11-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_04088</th>\n",
       "      <td>The youngest convict in a fatal gang-rape in N...</td>\n",
       "      <td>[]</td>\n",
       "      <td>20-12-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_06306</th>\n",
       "      <td>Former New York City mayor Rudy Giuliani has s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>28-07-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_06083</th>\n",
       "      <td>John Cantlie, the British journalist who has b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>13-07-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TestData_05896</th>\n",
       "      <td>Lawyers for the companies that manufactured an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>20-06-2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         bodyText topics  \\\n",
       "TestData_03241  A special British police unit was put on stand...     []   \n",
       "TestData_04088  The youngest convict in a fatal gang-rape in N...     []   \n",
       "TestData_06306  Former New York City mayor Rudy Giuliani has s...     []   \n",
       "TestData_06083  John Cantlie, the British journalist who has b...     []   \n",
       "TestData_05896  Lawyers for the companies that manufactured an...     []   \n",
       "\n",
       "               webPublicationDate  \n",
       "TestData_03241         15-11-2015  \n",
       "TestData_04088         20-12-2015  \n",
       "TestData_06306         28-07-2016  \n",
       "TestData_06083         13-07-2016  \n",
       "TestData_05896         20-06-2016  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ix = 'TestData_04088'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights    1.0\n",
       "india          1.0\n",
       "Name: TestData_04088, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_sub_df.ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ukcrime    1\n",
       "Name: TestData_04088, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = top_submission.set_index('id').ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights    1\n",
       "india          1\n",
       "protest        1\n",
       "ukcrime        1\n",
       "Name: TestData_04088, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = some_sub.set_index('id').ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights    1\n",
       "Name: TestData_02924, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = wrong_index_lstm_sub.set_index('id').ix[test_ix]\n",
    "q[q>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter-terrorism policy\n",
    " \n",
    "Foreign policy\n",
    " \n",
    "Defence policy\n",
    " \n",
    "Islamic State\n",
    " \n",
    "Syria\n",
    " \n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = trainingY.sum()\n",
    "unseen_topics = s[s.isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activism',\n",
       " 'bastilledaytruckattack',\n",
       " 'berlinchristmasmarketattack',\n",
       " 'brusselsattacks',\n",
       " 'charliehebdoattack',\n",
       " 'francetrainattack',\n",
       " 'munichshooting',\n",
       " 'orlandoterrorattack',\n",
       " 'parisattacks',\n",
       " 'peaceandreconciliation',\n",
       " 'sanbernardinoshooting',\n",
       " 'tunisiaattack2015',\n",
       " 'turkeycoupattempt',\n",
       " 'zikavirus'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(topics).intersection(unseen_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activism\n",
      "afghanistan\n",
      "aid\n",
      "algerianhostagecrisis\n",
      "alqaida\n",
      "alshabaab\n",
      "antiwar\n",
      "arabandmiddleeastprotests\n",
      "armstrade\n",
      "australianguncontrol\n",
      "australiansecurityandcounterterrorism\n",
      "bastilledaytruckattack\n",
      "belgium\n",
      "berlinchristmasmarketattack\n",
      "bigdata\n",
      "biometrics\n",
      "bokoharam\n",
      "bostonmarathonbombing\n",
      "britisharmy\n",
      "brusselsattacks\n",
      "cameroon\n",
      "carers\n",
      "charliehebdoattack\n",
      "chemicalweapons\n",
      "clusterbombs\n",
      "cobra\n",
      "conflictanddevelopment\n",
      "controversy\n",
      "criminaljustice\n",
      "cybercrime\n",
      "cyberwar\n",
      "darknet\n",
      "dataprotection\n",
      "debate\n",
      "defence\n",
      "deflation\n",
      "drones\n",
      "drugs\n",
      "drugspolicy\n",
      "drugstrade\n",
      "earthquakes\n",
      "ebola\n",
      "economy\n",
      "egypt\n",
      "encryption\n",
      "energy\n",
      "espionage\n",
      "ethics\n",
      "europeanarrestwarrant\n",
      "europeancourtofhumanrights\n",
      "events\n",
      "extradition\n",
      "famine\n",
      "farright\n",
      "firefighters\n",
      "forensicscience\n",
      "france\n",
      "francetrainattack\n",
      "freedomofspeech\n",
      "genevaconventions\n",
      "germany\n",
      "guncrime\n",
      "hacking\n",
      "hashtags\n",
      "helicoptercrashes\n",
      "humanitarianresponse\n",
      "humanrights\n",
      "humanrightsact\n",
      "humantrafficking\n",
      "immigration\n",
      "india\n",
      "indonesia\n",
      "internallydisplacedpeople\n",
      "internationalcourtofjustice\n",
      "internationalcriminaljustice\n",
      "internetsafety\n",
      "iraq\n",
      "isis\n",
      "israel\n",
      "jordan\n",
      "jubilee\n",
      "judiciary\n",
      "july7\n",
      "justiceandsecurity\n",
      "kenya\n",
      "knifecrime\n",
      "lebanon\n",
      "libya\n",
      "localgovernment\n",
      "logistics\n",
      "london\n",
      "londonriots\n",
      "malaysia\n",
      "mali\n",
      "malware\n",
      "metropolitanpolice\n",
      "middleeastpeacetalks\n",
      "migration\n",
      "military\n",
      "ministryofdefence\n",
      "morocco\n",
      "mrsa\n",
      "mumbaiterrorattacks\n",
      "munichshooting\n",
      "naturaldisasters\n",
      "nigeria\n",
      "nuclearweapons\n",
      "occupy\n",
      "organisedcrime\n",
      "orlandoterrorattack\n",
      "osamabinladen\n",
      "paris\n",
      "parisattacks\n",
      "peaceandreconciliation\n",
      "philippines\n",
      "piracy\n",
      "planecrashes\n",
      "police\n",
      "protest\n",
      "refugees\n",
      "religion\n",
      "retirementage\n",
      "rio20earthsummit\n",
      "royalairforce\n",
      "royalnavy\n",
      "russia\n",
      "sanbernardinoshooting\n",
      "saudiarabia\n",
      "september11\n",
      "slavery\n",
      "somalia\n",
      "southafrica\n",
      "southchinasea\n",
      "stopandsearch\n",
      "surveillance\n",
      "sydneysiege\n",
      "syria\n",
      "taliban\n",
      "terrorism\n",
      "thailand\n",
      "torture\n",
      "traincrashes\n",
      "transport\n",
      "tunisiaattack2015\n",
      "turkey\n",
      "turkeycoupattempt\n",
      "ukcrime\n",
      "uksecurity\n",
      "uksupremecourt\n",
      "undercoverpoliceandpolicing\n",
      "unitednations\n",
      "usguncontrol\n",
      "values\n",
      "warcrimes\n",
      "warreporting\n",
      "weaponstechnology\n",
      "womeninbusiness\n",
      "woolwichattack\n",
      "worldmigration\n",
      "zikavirus\n"
     ]
    }
   ],
   "source": [
    "for i in topics:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/ml-ai/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3445929"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(wvmodel['zika'], np.vstack(test_wvec.dropna())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/ml-ai/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38107796869050226"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(fsmodel['zika'], np.vstack(test_fvec.dropna())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyText              The World Health Organisation has convened an ...\n",
       "topics                                                               []\n",
       "webPublicationDate                                           28-01-2016\n",
       "Name: TestData_04490, dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.ix[4488 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyText              The United Nations security council has called...\n",
       "topics                                                               []\n",
       "webPublicationDate                                           17-09-2016\n",
       "Name: TestData_06730, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.ix[6727 + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyText              We are deeply concerned that the counter-terro...\n",
       "topics                                                               []\n",
       "webPublicationDate                                           02-02-2015\n",
       "Name: TestData_00360, dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.ix[359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugstrade    1.0\n",
       "Name: TestData_04490, dtype: float64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_sub_df.iloc[4488 + 1]\n",
    "q[q > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
