{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from growing_instability_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = Word2Vec.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('../data/sampleSubmission.csv')\n",
    "topics = sorted(set(sample_sub.columns.difference(['id'])))\n",
    "\n",
    "topic2actual = {}\n",
    "for i in sample_sub.columns:\n",
    "    if 'id' == i:\n",
    "        continue\n",
    "    topic2actual[i] = segment(i)\n",
    "    \n",
    "target_columns = sorted(topics)\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# word2idx_trainingX = pd.read_hdf('training_data_wv_fs_lsi_no_stopwords.hdf', 'word2idx_trainingX')\n",
    "_word2idx = pd.read_hdf('training_data_wv_fs_lsi_no_stopwords.hdf', '_word2idx')\n",
    "# trainingY = pd.read_hdf('training_data_wv_fs_lsi_no_stopwords.hdf', 'trainingY')\n",
    "\n",
    "# indices = pd.Index(sorted(trainingY.index[trainingY.index.str.contains('^201[2-4]')]))\n",
    "\n",
    "# word2idx_trainingX = word2idx_trainingX.ix[indices]\n",
    "# trainingY = trainingY.ix[indices]\n",
    "\n",
    "# word2idx_trainingX.to_hdf('training_data_word_index_2012-2014.hdf', 'word2idx_trainingX')\n",
    "# trainingY.to_hdf('training_data_word_index_2012-2014.hdf', 'trainingY')\n",
    "\n",
    "word2idx_trainingX = pd.read_hdf('training_data_word_index_2012-2014.hdf', 'word2idx_trainingX')\n",
    "trainingY = pd.read_hdf('training_data_word_index_2012-2014.hdf', 'trainingY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.read_hdf('train_test_df_3.hdf', 'train_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_topics(df, topics):\n",
    "    topics = sorted(topics)\n",
    "#     v = np.zeros(shape=(df.shape[0], len(topics)))\n",
    "    v = []\n",
    "    for ix, tp in enumerate(df.topics):\n",
    "        tt = []\n",
    "        for t in tp:\n",
    "            tt.append(topics.index(t))\n",
    "#             v[ix][topics.index(t)] = 1\n",
    "        v.append(tt)\n",
    "\n",
    "    return pd.Series(v, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2word = {j:i for i, j in _word2idx.iteritems()}\n",
    "ind2class = dict(enumerate(topics))\n",
    "class2ind = {j: i for i, j in ind2class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_vector(term_vecs, vector_size, mean_window=5, sequence_length=200):\n",
    "    # 85 percentile length of docs is (198 * 5)\n",
    "\n",
    "    q = np.arange(0, len(term_vecs), mean_window)\n",
    "    \n",
    "    sequence = np.zeros([1, sequence_length, vector_size])\n",
    "    \n",
    "    for ix, inds in  enumerate(zip(q, q + mean_window)):\n",
    "        if ix < sequence_length:\n",
    "            sequence[0][ix] = np.mean(term_vecs[inds[0]: inds[1]], axis=0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def process_ind2word(term_idx):\n",
    "    return [ind2word.get(idx, -1) for idx in term_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_trainingX.map(len).quantile(0.8) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_mean_average(term_idx, stopwords=[], stack=True):\n",
    "    ret = term_idx.map(process_ind2word).map(\n",
    "        lambda x: [wvmodel[i] for i in x if i in wvmodel.wv.vocab]\n",
    "    ).map(\n",
    "        lambda x: process_vector(x, wvmodel.vector_size)\n",
    "    )\n",
    "    \n",
    "    if stack:\n",
    "        ret = np.vstack(ret)\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "def parallel_generate_word_vectors(samp, transformer, stopwords, batch, num_proc):\n",
    "    with Parallel(n_jobs=num_proc) as parallel:\n",
    "        dataset = np.zeros([samp.shape[0], 200, 300])\n",
    "        is_break = False\n",
    "        i = 0\n",
    "\n",
    "        while not is_break:\n",
    "            payload = []\n",
    "\n",
    "            for j in xrange(num_proc):\n",
    "                t_df = samp[(i + j) * batch: (i + 1 + j) * batch]\n",
    "\n",
    "                if t_df.empty:\n",
    "                    is_break = True\n",
    "                    continue\n",
    "\n",
    "                payload.append(\n",
    "                    delayed(transformer)(\n",
    "                        t_df, stopwords\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            print('Current batch in main thread: {}'.format((i + j) * batch))\n",
    "\n",
    "            if payload:\n",
    "                results = parallel(payload)\n",
    "                dataset.extend(results)\n",
    "                i += num_proc\n",
    "\n",
    "    return np.vstack(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_word_indices = transform_text(train_test_df).map(lambda x: [_word2idx.get(i) for i in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 314 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.zeros(\n",
    "    [word2idx_trainingX.shape[0] + train_test_word_indices.shape[0], 200, 300], dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1140\n",
      "1 1140 2280\n",
      "2 2280 3420\n",
      "3 3420 4560\n",
      "4 4560 5700\n",
      "5 5700 6840\n",
      "6 6840 7980\n",
      "7 7980 9120\n",
      "8 9120 10260\n",
      "9 10260 11400\n",
      "10 11400 12540\n",
      "11 12540 13680\n",
      "12 13680 14820\n",
      "13 14820 15960\n",
      "14 15960 17100\n",
      "15 17100 18240\n",
      "16 18240 19380\n",
      "17 19380 20520\n",
      "18 20520 21660\n",
      "19 21660 22800\n",
      "20 22800 23940\n",
      "21 23940 25080\n",
      "22 25080 26220\n",
      "23 26220 27360\n",
      "24 27360 28500\n",
      "25 28500 29640\n",
      "26 29640 30780\n",
      "27 30780 31920\n",
      "28 31920 33060\n",
      "29 33060 34200\n",
      "30 34200 35340\n",
      "31 35340 36480\n",
      "32 36480 37620\n",
      "33 37620 38760\n",
      "34 38760 39900\n",
      "35 39900 41040\n",
      "36 41040 42180\n",
      "37 42180 43320\n",
      "38 43320 44460\n",
      "39 44460 45600\n",
      "40 45600 46740\n",
      "41 46740 47880\n",
      "42 47880 49020\n",
      "43 49020 50160\n",
      "44 50160 51300\n",
      "45 51300 52440\n",
      "46 52440 53580\n",
      "47 53580 54720\n",
      "48 54720 55860\n",
      "49 55860 56999\n",
      "CPU times: user 2min 33s, sys: 13.3 s, total: 2min 47s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "\n",
    "for ix, chunk in enumerate(np.array_split(pd.concat([word2idx_trainingX, train_test_word_indices]), 50)):\n",
    "    chunk = transform_mean_average(chunk)\n",
    "    j = i + chunk.shape[0]\n",
    "    x_train[i: j] = chunk\n",
    "    print ix, i, j\n",
    "\n",
    "    i = j    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 * 60000 * 300 * 8. / 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_target(y, size):\n",
    "    e = np.zeros(size)\n",
    "    e[y] = 1\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 6853\n",
    "# # 3110\n",
    "# x_dump = parallel_generate_word_vectors(\n",
    "#     pd.concat([word2idx_trainingX, train_test_word_indices]),\n",
    "#     transform_mean_average,\n",
    "#     stopwords=[],\n",
    "#     batch=1000,\n",
    "#     num_proc=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 12 ms, total: 2.09 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_samples = x_train.shape[0]\n",
    "\n",
    "training_Y = pd.DataFrame(zip(*np.where(trainingY.head(num_samples) == 1)), columns=['iloc', 'topics'])\n",
    "training_Y = training_Y.groupby('iloc')['topics'].apply(list)\n",
    "training_Y.index = trainingY.head(num_samples).index\n",
    "\n",
    "train_test_y = transform_topics(train_test_df, topics)\n",
    "y_train = pd.concat([training_Y, train_test_y])\n",
    "y_train = np.vstack(y_train.map(lambda x: build_target(x, len(topics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56999, 160), (56999, 200, 300))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as K\n",
    "import keras.backend as KB\n",
    "\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    TP = K.metrics.true_positives(y_true, K.round(y_pred))\n",
    "    FP = K.metrics.false_positives(y_true, K.round(y_pred))\n",
    "    FN = K.metrics.false_negatives(y_true, K.round(y_pred))\n",
    "    \n",
    "    p = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FP))\n",
    "    r = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FN))\n",
    "    \n",
    "    return (2.0 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    # http://stackoverflow.com/questions/43345909/when-using-mectrics-in-model-compile-in-keras-report-valueerror-unknown-metr\n",
    "    # Count positive samples.\n",
    "    c1 = KB.sum(KB.round(KB.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = KB.sum(KB.round(KB.clip(y_pred, 0, 1)))\n",
    "    c3 = KB.sum(KB.round(KB.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ma_input (InputLayer)        (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 64)          93440     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 160)               20640     \n",
      "=================================================================\n",
      "Total params: 155,424\n",
      "Trainable params: 155,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Convolution1D, MaxPooling1D, Flatten  # , TimeDistributedDense\n",
    "from keras.models import Model\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "ma_input = Input(shape=(None, 300), name='ma_input')\n",
    "\n",
    "ma_x = LSTM(64, return_sequences=True)(ma_input)\n",
    "ma_x = LSTM(64, go_backwards=True)(ma_x)\n",
    "# We stack a deep densely-connected network on top\n",
    "\n",
    "x = Dense(128, activation='relu')(ma_x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(len(class2ind), activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[\n",
    "        ma_input,\n",
    "    ],\n",
    "    outputs=[main_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ma_input (InputLayer)        (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 64)          93440     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 160)               20640     \n",
      "=================================================================\n",
      "Total params: 155,424\n",
      "Trainable params: 155,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # keras.optimizers.RMSprop(lr=0.005),  # , rho=0.9, epsilon=1e-08, decay=0.0, clipnorm=1),\n",
    "    loss={'main_output': 'categorical_crossentropy'},\n",
    "    loss_weights={'main_output': 1.},\n",
    "    metrics=['accuracy', f1_micro]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56999, 160)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_train_data(x, y, batch_size=1000, shuffle=True):\n",
    "    s = x.shape[0]\n",
    "    q = np.arange(0, s, batch_size)\n",
    "    indices = zip(q, q + batch_size)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for i, j in indices:\n",
    "            _y = y[i: j]\n",
    "            _x = x[i: j]\n",
    "            if _y.shape[0] > 0:\n",
    "#                 print('hello')\n",
    "                yield (np.vstack(_x), _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "epochs = 10\n",
    "# steps_per_epoch = (x_dump.shape[0] + batch_size) / batch_size\n",
    "\n",
    "# train_data_generator = generate_batch_train_data(x_dump, y_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model.fit_generator(\n",
    "#     train_data_generator,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     epochs=epochs,\n",
    "#     workers=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_file_name = \"bidirectional-lstm-weights\"\n",
    "filepath = base_file_name + \".{epoch:02d}-{loss:.4f}-{acc:.4f}-{f1_micro:.4f}.hdf5\"\n",
    "filename = base_file_name + \".log\"\n",
    "\n",
    "model_check_point = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='f1_micro', verbose=0,\n",
    "    save_best_only=True, save_weights_only=False,\n",
    "    mode='max', period=1,\n",
    ")\n",
    "\n",
    "remote_monitor = keras.callbacks.RemoteMonitor(\n",
    "    root='http://localhost:9000',\n",
    "    path='/publish/epoch/end/',\n",
    "    field='data', headers=None\n",
    ")\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(filename=filename, separator=',', append=False)\n",
    "\n",
    "callbacks = [model_check_point, remote_monitor, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56999/56999 [==============================] - 30s - loss: 2.0243 - acc: 0.6434 - f1_micro: 0.5626    \n",
      "Epoch 2/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.9847 - acc: 0.6492 - f1_micro: 0.5636    \n",
      "Epoch 3/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.9596 - acc: 0.6498 - f1_micro: 0.5647    \n",
      "Epoch 4/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.9675 - acc: 0.6506 - f1_micro: 0.5659    \n",
      "Epoch 5/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.9278 - acc: 0.6550 - f1_micro: 0.5671    \n",
      "Epoch 6/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.9032 - acc: 0.6580 - f1_micro: 0.5684    \n",
      "Epoch 7/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.9099 - acc: 0.6555 - f1_micro: 0.5700    \n",
      "Epoch 8/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.8922 - acc: 0.6594 - f1_micro: 0.5716    \n",
      "Epoch 9/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.8598 - acc: 0.6668 - f1_micro: 0.5732    \n",
      "Epoch 10/10\n",
      "56999/56999 [==============================] - 31s - loss: 1.8469 - acc: 0.6677 - f1_micro: 0.5747    \n",
      "CPU times: user 4min 43s, sys: 1min 24s, total: 6min 8s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2000\n",
    "epochs = 10\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.4119 - acc: 0.7233 - f1_micro: 0.6582    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.4072 - acc: 0.7240 - f1_micro: 0.6595    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3998 - acc: 0.7284 - f1_micro: 0.6607    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3913 - acc: 0.7279 - f1_micro: 0.6620    \n",
      "Epoch 5/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.4040 - acc: 0.7260 - f1_micro: 0.6632    \n",
      "Epoch 6/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3816 - acc: 0.7282 - f1_micro: 0.6644    \n",
      "Epoch 7/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3949 - acc: 0.7270 - f1_micro: 0.6656    \n",
      "Epoch 8/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3718 - acc: 0.7304 - f1_micro: 0.6669    \n",
      "Epoch 9/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3791 - acc: 0.7300 - f1_micro: 0.6681    \n",
      "Epoch 10/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3705 - acc: 0.7292 - f1_micro: 0.6692    \n",
      "Epoch 11/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3679 - acc: 0.7329 - f1_micro: 0.6704    \n",
      "Epoch 12/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3556 - acc: 0.7326 - f1_micro: 0.6716    \n",
      "Epoch 13/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3540 - acc: 0.7342 - f1_micro: 0.6727    \n",
      "Epoch 14/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3457 - acc: 0.7317 - f1_micro: 0.6738    \n",
      "Epoch 15/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3473 - acc: 0.7322 - f1_micro: 0.6750    \n",
      "Epoch 16/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3541 - acc: 0.7339 - f1_micro: 0.6761    \n",
      "Epoch 17/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3458 - acc: 0.7329 - f1_micro: 0.6772    \n",
      "Epoch 18/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3429 - acc: 0.7353 - f1_micro: 0.6783    \n",
      "Epoch 19/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3266 - acc: 0.7367 - f1_micro: 0.6793    \n",
      "Epoch 20/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3365 - acc: 0.7327 - f1_micro: 0.6804    \n",
      "Epoch 21/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3153 - acc: 0.7388 - f1_micro: 0.6815    \n",
      "Epoch 22/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3223 - acc: 0.7386 - f1_micro: 0.6825    \n",
      "Epoch 23/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3207 - acc: 0.7369 - f1_micro: 0.6836    \n",
      "Epoch 24/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3094 - acc: 0.7394 - f1_micro: 0.6846    \n",
      "Epoch 25/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3198 - acc: 0.7369 - f1_micro: 0.6856    \n",
      "Epoch 26/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.3097 - acc: 0.7383 - f1_micro: 0.6866    \n",
      "Epoch 27/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3138 - acc: 0.7389 - f1_micro: 0.6876    \n",
      "Epoch 28/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2935 - acc: 0.7392 - f1_micro: 0.6886    \n",
      "Epoch 29/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.3110 - acc: 0.7378 - f1_micro: 0.6896    \n",
      "Epoch 30/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2916 - acc: 0.7417 - f1_micro: 0.6906    \n",
      "Epoch 31/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2944 - acc: 0.7405 - f1_micro: 0.6915    \n",
      "Epoch 32/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2885 - acc: 0.7402 - f1_micro: 0.6925    \n",
      "Epoch 33/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2894 - acc: 0.7412 - f1_micro: 0.6935    \n",
      "Epoch 34/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2848 - acc: 0.7420 - f1_micro: 0.6944    \n",
      "Epoch 35/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2872 - acc: 0.7428 - f1_micro: 0.6953    \n",
      "Epoch 36/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2762 - acc: 0.7414 - f1_micro: 0.6963    \n",
      "Epoch 37/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2622 - acc: 0.7457 - f1_micro: 0.6972    \n",
      "Epoch 38/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2783 - acc: 0.7431 - f1_micro: 0.6981    \n",
      "Epoch 39/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2685 - acc: 0.7458 - f1_micro: 0.6990    \n",
      "Epoch 40/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2643 - acc: 0.7468 - f1_micro: 0.7000    \n",
      "Epoch 41/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2678 - acc: 0.7442 - f1_micro: 0.7009    \n",
      "Epoch 42/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2600 - acc: 0.7440 - f1_micro: 0.7018    \n",
      "Epoch 43/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2604 - acc: 0.7473 - f1_micro: 0.7026    \n",
      "Epoch 44/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2498 - acc: 0.7480 - f1_micro: 0.7035    \n",
      "Epoch 45/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2618 - acc: 0.7445 - f1_micro: 0.7044    \n",
      "Epoch 46/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2522 - acc: 0.7471 - f1_micro: 0.7053    \n",
      "Epoch 47/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2503 - acc: 0.7481 - f1_micro: 0.7061    \n",
      "Epoch 48/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2367 - acc: 0.7485 - f1_micro: 0.7070    \n",
      "Epoch 49/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2517 - acc: 0.7462 - f1_micro: 0.7078    \n",
      "Epoch 50/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2417 - acc: 0.7480 - f1_micro: 0.7087    \n",
      "CPU times: user 23min 41s, sys: 6min 54s, total: 30min 35s\n",
      "Wall time: 25min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2000\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.2418 - acc: 0.7488 - f1_micro: 0.7095    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.2268 - acc: 0.7505 - f1_micro: 0.7104    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 30s - loss: 1.2447 - acc: 0.7463 - f1_micro: 0.7112    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 31s - loss: 1.2283 - acc: 0.7492 - f1_micro: 0.7120    \n",
      "Epoch 5/50\n",
      "18000/56999 [========>.....................] - ETA: 21s - loss: 1.2045 - acc: 0.7511 - f1_micro: 0.7126"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2000\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as sk_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = model.predict(\n",
    "    x_train,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 860 ms, sys: 120 ms, total: 980 ms\n",
      "Wall time: 977 ms\n",
      "0.0 (0.0, 0.020931410704883278)\n",
      "CPU times: user 416 ms, sys: 24 ms, total: 440 ms\n",
      "Wall time: 437 ms\n",
      "0.01 (0.01, 0.70378435200480549)\n",
      "CPU times: user 420 ms, sys: 24 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.02 (0.02, 0.75052708710760674)\n",
      "CPU times: user 420 ms, sys: 24 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.03 (0.029999999999999999, 0.77544783884363377)\n",
      "CPU times: user 416 ms, sys: 28 ms, total: 444 ms\n",
      "Wall time: 440 ms\n",
      "0.04 (0.040000000000000001, 0.79182701117830301)\n",
      "CPU times: user 424 ms, sys: 12 ms, total: 436 ms\n",
      "Wall time: 435 ms\n",
      "0.05 (0.050000000000000003, 0.80322496390149667)\n",
      "CPU times: user 408 ms, sys: 32 ms, total: 440 ms\n",
      "Wall time: 438 ms\n",
      "0.06 (0.059999999999999998, 0.81193221698586182)\n",
      "CPU times: user 408 ms, sys: 32 ms, total: 440 ms\n",
      "Wall time: 438 ms\n",
      "0.07 (0.070000000000000007, 0.81876284882655859)\n",
      "CPU times: user 416 ms, sys: 20 ms, total: 436 ms\n",
      "Wall time: 436 ms\n",
      "0.08 (0.080000000000000002, 0.82383438380944052)\n",
      "CPU times: user 424 ms, sys: 16 ms, total: 440 ms\n",
      "Wall time: 440 ms\n",
      "0.09 (0.089999999999999997, 0.82769548311772245)\n",
      "CPU times: user 428 ms, sys: 12 ms, total: 440 ms\n",
      "Wall time: 436 ms\n",
      "0.1 (0.10000000000000001, 0.83109591999030941)\n",
      "CPU times: user 420 ms, sys: 20 ms, total: 440 ms\n",
      "Wall time: 443 ms\n",
      "0.11 (0.11, 0.83365768065082146)\n",
      "CPU times: user 420 ms, sys: 20 ms, total: 440 ms\n",
      "Wall time: 439 ms\n",
      "0.12 (0.12, 0.83585866709559808)\n",
      "CPU times: user 416 ms, sys: 24 ms, total: 440 ms\n",
      "Wall time: 439 ms\n",
      "0.13 (0.13, 0.83767788521302988)\n",
      "CPU times: user 420 ms, sys: 16 ms, total: 436 ms\n",
      "Wall time: 436 ms\n",
      "0.14 (0.14000000000000001, 0.83906963579060823)\n",
      "CPU times: user 420 ms, sys: 20 ms, total: 440 ms\n",
      "Wall time: 439 ms\n",
      "0.15 (0.14999999999999999, 0.84005700504275371)\n",
      "CPU times: user 420 ms, sys: 24 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.16 (0.16, 0.84104929756215996)\n",
      "CPU times: user 436 ms, sys: 12 ms, total: 448 ms\n",
      "Wall time: 445 ms\n",
      "0.17 (0.17000000000000001, 0.8420409681771992)\n",
      "CPU times: user 412 ms, sys: 36 ms, total: 448 ms\n",
      "Wall time: 444 ms\n",
      "0.18 (0.17999999999999999, 0.84229398688575863)\n",
      "CPU times: user 444 ms, sys: 8 ms, total: 452 ms\n",
      "Wall time: 452 ms\n",
      "0.19 (0.19, 0.84275648157513239)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 443 ms\n",
      "0.2 (0.20000000000000001, 0.84280475332116567)\n",
      "CPU times: user 432 ms, sys: 28 ms, total: 460 ms\n",
      "Wall time: 454 ms\n",
      "0.21 (0.20999999999999999, 0.84263717025424756)\n",
      "CPU times: user 428 ms, sys: 20 ms, total: 448 ms\n",
      "Wall time: 447 ms\n",
      "0.22 (0.22, 0.8424984578830732)\n",
      "CPU times: user 452 ms, sys: 12 ms, total: 464 ms\n",
      "Wall time: 460 ms\n",
      "0.23 (0.23000000000000001, 0.84237055578200437)\n",
      "CPU times: user 428 ms, sys: 16 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.24 (0.23999999999999999, 0.84187023297139496)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.25 (0.25, 0.84073818538123923)\n",
      "CPU times: user 420 ms, sys: 28 ms, total: 448 ms\n",
      "Wall time: 443 ms\n",
      "0.26 (0.26000000000000001, 0.84026681424951211)\n",
      "CPU times: user 436 ms, sys: 8 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.27 (0.27000000000000002, 0.83928581878617292)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.28 (0.28000000000000003, 0.83825941757162759)\n",
      "CPU times: user 428 ms, sys: 16 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.29 (0.28999999999999998, 0.83739362047684018)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.3 (0.29999999999999999, 0.8362855481865531)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.31 (0.31, 0.83496580157765532)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.32 (0.32000000000000001, 0.83343307162006897)\n",
      "CPU times: user 424 ms, sys: 16 ms, total: 440 ms\n",
      "Wall time: 440 ms\n",
      "0.33 (0.33000000000000002, 0.83185000450707614)\n",
      "CPU times: user 428 ms, sys: 16 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.34 (0.34000000000000002, 0.83034787436931179)\n",
      "CPU times: user 412 ms, sys: 32 ms, total: 444 ms\n",
      "Wall time: 440 ms\n",
      "0.35 (0.35000000000000003, 0.82863619283125445)\n",
      "CPU times: user 432 ms, sys: 12 ms, total: 444 ms\n",
      "Wall time: 440 ms\n",
      "0.36 (0.35999999999999999, 0.82686127477236215)\n",
      "CPU times: user 436 ms, sys: 8 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.37 (0.37, 0.82485468225687164)\n",
      "CPU times: user 416 ms, sys: 28 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.38 (0.38, 0.82328609916107698)\n",
      "CPU times: user 424 ms, sys: 16 ms, total: 440 ms\n",
      "Wall time: 440 ms\n",
      "0.39 (0.39000000000000001, 0.82115834873690707)\n",
      "CPU times: user 432 ms, sys: 16 ms, total: 448 ms\n",
      "Wall time: 442 ms\n",
      "0.4 (0.40000000000000002, 0.81951961797071671)\n",
      "CPU times: user 492 ms, sys: 8 ms, total: 500 ms\n",
      "Wall time: 497 ms\n",
      "0.41 (0.41000000000000003, 0.81750201850816717)\n",
      "CPU times: user 444 ms, sys: 32 ms, total: 476 ms\n",
      "Wall time: 470 ms\n",
      "0.42 (0.41999999999999998, 0.81524491399633281)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 440 ms\n",
      "0.43 (0.42999999999999999, 0.81262488020895318)\n",
      "CPU times: user 416 ms, sys: 28 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.44 (0.44, 0.81042332167920106)\n",
      "CPU times: user 428 ms, sys: 16 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.45 (0.45000000000000001, 0.8082825668381679)\n",
      "CPU times: user 432 ms, sys: 8 ms, total: 440 ms\n",
      "Wall time: 441 ms\n",
      "0.46 (0.46000000000000002, 0.80598093402971449)\n",
      "CPU times: user 420 ms, sys: 24 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.47 (0.47000000000000003, 0.80373130054658715)\n",
      "CPU times: user 420 ms, sys: 24 ms, total: 444 ms\n",
      "Wall time: 440 ms\n",
      "0.48 (0.47999999999999998, 0.80121636470498037)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 442 ms\n",
      "0.49 (0.48999999999999999, 0.79866077441617334)\n",
      "CPU times: user 424 ms, sys: 20 ms, total: 444 ms\n",
      "Wall time: 441 ms\n",
      "0.5 (0.5, 0.79604059867590615)\n",
      "CPU times: user 22.2 s, sys: 1.13 s, total: 23.3 s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt = 0.01\n",
    "s = 0.0\n",
    "e = 0.5\n",
    "th = np.arange(s, e + dt, dt)\n",
    "mean_fscores = []\n",
    "for t in th:\n",
    "    %time mean_fscores.append((t, sk_f1_score(y_train, 1.0 * (g > t), average='micro')))\n",
    "    print t, mean_fscores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20000000000000001, 0.84280475332116567)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh, thresh_score = sorted(mean_fscores, key=lambda x: x[1], reverse=True)[0]\n",
    "thresh, thresh_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9749945890>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5G5mVkFO7mZmFeTkbmZWQf8f6JOJ5Y7wwMIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f974762e110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ix = 1\n",
    "sd = -20\n",
    "pd.Series(g[sd:][ix]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([145]),), (array([144]),))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thresh = 0.5\n",
    "np.where(y_train[sd:][ix] == 1), np.where(g[sd:][ix] > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'turkeycoupattempt'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(pred, scale_param=0.75, min_thresh=0.05, thresh = 0.5):\n",
    "#     mx = pred.mean() + 3 * pred.std()\n",
    "    return np.where(pred > thresh)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/TestData.json') as fl:\n",
    "    data = json.load(fl)\n",
    "    test_df = pd.DataFrame(data['TestData']).T\n",
    "    del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_word_indices = transform_text(test_df).map(lambda x: [_word2idx.get(i) for i in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_null_index = test_word_indices[test_word_indices.map(len) == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 31.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_test = np.zeros(\n",
    "    [test_word_indices.shape[0], 200, 300], dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 152\n",
      "1 152 304\n",
      "2 304 456\n",
      "3 456 608\n",
      "4 608 760\n",
      "5 760 912\n",
      "6 912 1064\n",
      "7 1064 1216\n",
      "8 1216 1368\n",
      "9 1368 1520\n",
      "10 1520 1672\n",
      "11 1672 1824\n",
      "12 1824 1976\n",
      "13 1976 2128\n",
      "14 2128 2280\n",
      "15 2280 2432\n",
      "16 2432 2584\n",
      "17 2584 2736\n",
      "18 2736 2888\n",
      "19 2888 3040\n",
      "20 3040 3192\n",
      "21 3192 3344\n",
      "22 3344 3496\n",
      "23 3496 3648\n",
      "24 3648 3800\n",
      "25 3800 3952\n",
      "26 3952 4104\n",
      "27 4104 4256\n",
      "28 4256 4408\n",
      "29 4408 4560\n",
      "30 4560 4712\n",
      "31 4712 4863\n",
      "32 4863 5014\n",
      "33 5014 5165\n",
      "34 5165 5316\n",
      "35 5316 5467\n",
      "36 5467 5618\n",
      "37 5618 5769\n",
      "38 5769 5920\n",
      "39 5920 6071\n",
      "40 6071 6222\n",
      "41 6222 6373\n",
      "42 6373 6524\n",
      "43 6524 6675\n",
      "44 6675 6826\n",
      "45 6826 6977\n",
      "46 6977 7128\n",
      "47 7128 7279\n",
      "48 7279 7430\n",
      "49 7430 7581\n",
      "CPU times: user 23.7 s, sys: 2.02 s, total: 25.8 s\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ii = 0\n",
    "\n",
    "for ix, chunk in enumerate(np.array_split(test_word_indices, 50)):\n",
    "    chunk = transform_mean_average(chunk)\n",
    "    jj = ii + chunk.shape[0]\n",
    "    x_test[ii: jj] = chunk\n",
    "    print ix, ii, jj\n",
    "\n",
    "    ii = jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_probas = model.predict(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# valid_test_feature_vec found below!\n",
    "thresh = 0.2\n",
    "test_values = np.zeros([test_probas.shape[0], len(topics)])\n",
    "for ix, pred in enumerate(test_probas):\n",
    "    for v in get_classes(pred, thresh=thresh):\n",
    "        test_values[ix][v] = 1\n",
    "\n",
    "test_sub_df = pd.DataFrame(\n",
    "    test_values,\n",
    "    index=test_df.index,\n",
    "    columns=topics\n",
    ")\n",
    "\n",
    "test_sub_df = test_sub_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12672.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum(axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4095097595951964"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional-lstm-mean_averaged_window-loss_1.4095-f1_micro_0.3921-thresh_0.2.csv\n"
     ]
    }
   ],
   "source": [
    "sub_filename = 'bidirectional-lstm-mean_averaged_window-loss_{:.4f}-f1_micro_{:.4f}-thresh_{}.csv'.format(hist.history['loss'][-1], hist.history['f1_micro'][-1], thresh)\n",
    "print sub_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sub_df.astype(int).reset_index().rename(\n",
    "    columns={'index': 'id'}\n",
    ").sort_values('id').to_csv(\n",
    "    sub_filename, \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
