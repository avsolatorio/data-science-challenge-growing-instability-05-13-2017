{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from growing_instability_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = Word2Vec.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('../data/sampleSubmission.csv')\n",
    "topics = sorted(set(sample_sub.columns.difference(['id'])))\n",
    "\n",
    "topic2actual = {}\n",
    "for i in sample_sub.columns:\n",
    "    if 'id' == i:\n",
    "        continue\n",
    "    topic2actual[i] = segment(i)\n",
    "    \n",
    "target_columns = sorted(topics)\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# word2idx_trainingX = pd.read_hdf('training_data_wv_fs_lsi_no_stopwords.hdf', 'word2idx_trainingX')\n",
    "_word2idx = pd.read_hdf('training_data_wv_fs_lsi_no_stopwords.hdf', '_word2idx')\n",
    "# trainingY = pd.read_hdf('training_data_wv_fs_lsi_no_stopwords.hdf', 'trainingY')\n",
    "\n",
    "# indices = pd.Index(sorted(trainingY.index[trainingY.index.str.contains('^201[2-4]')]))\n",
    "\n",
    "# word2idx_trainingX = word2idx_trainingX.ix[indices]\n",
    "# trainingY = trainingY.ix[indices]\n",
    "\n",
    "# word2idx_trainingX.to_hdf('training_data_word_index_2012-2014.hdf', 'word2idx_trainingX')\n",
    "# trainingY.to_hdf('training_data_word_index_2012-2014.hdf', 'trainingY')\n",
    "\n",
    "word2idx_trainingX = pd.read_hdf('training_data_word_index_2012-2014.hdf', 'word2idx_trainingX')\n",
    "trainingY = pd.read_hdf('training_data_word_index_2012-2014.hdf', 'trainingY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.read_hdf('train_test_df_3.hdf', 'train_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_topics(df, topics):\n",
    "    topics = sorted(topics)\n",
    "#     v = np.zeros(shape=(df.shape[0], len(topics)))\n",
    "    v = []\n",
    "    for ix, tp in enumerate(df.topics):\n",
    "        tt = []\n",
    "        for t in tp:\n",
    "            tt.append(topics.index(t))\n",
    "#             v[ix][topics.index(t)] = 1\n",
    "        v.append(tt)\n",
    "\n",
    "    return pd.Series(v, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2word = {j:i for i, j in _word2idx.iteritems()}\n",
    "ind2class = dict(enumerate(topics))\n",
    "class2ind = {j: i for i, j in ind2class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_vector(term_vecs, vector_size, mean_window=5, sequence_length=200):\n",
    "    # 85 percentile length of docs is (198 * 5)\n",
    "\n",
    "    q = np.arange(0, len(term_vecs), mean_window)\n",
    "    \n",
    "    sequence = np.zeros([1, sequence_length, vector_size])\n",
    "    \n",
    "    for ix, inds in  enumerate(zip(q, q + mean_window)):\n",
    "        if ix < sequence_length:\n",
    "            sequence[0][ix] = np.mean(term_vecs[inds[0]: inds[1]], axis=0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def process_ind2word(term_idx):\n",
    "    return [ind2word.get(idx, -1) for idx in term_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_trainingX.map(len).quantile(0.8) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_mean_average(term_idx, stopwords=[], stack=True):\n",
    "    ret = term_idx.map(process_ind2word).map(\n",
    "        lambda x: [wvmodel[i] for i in x if i in wvmodel.wv.vocab]\n",
    "    ).map(\n",
    "        lambda x: process_vector(x, wvmodel.vector_size)\n",
    "    )\n",
    "    \n",
    "    if stack:\n",
    "        ret = np.vstack(ret)\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "def parallel_generate_word_vectors(samp, transformer, stopwords, batch, num_proc):\n",
    "    with Parallel(n_jobs=num_proc) as parallel:\n",
    "        dataset = np.zeros([samp.shape[0], 200, 300])\n",
    "        is_break = False\n",
    "        i = 0\n",
    "\n",
    "        while not is_break:\n",
    "            payload = []\n",
    "\n",
    "            for j in xrange(num_proc):\n",
    "                t_df = samp[(i + j) * batch: (i + 1 + j) * batch]\n",
    "\n",
    "                if t_df.empty:\n",
    "                    is_break = True\n",
    "                    continue\n",
    "\n",
    "                payload.append(\n",
    "                    delayed(transformer)(\n",
    "                        t_df, stopwords\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            print('Current batch in main thread: {}'.format((i + j) * batch))\n",
    "\n",
    "            if payload:\n",
    "                results = parallel(payload)\n",
    "                dataset.extend(results)\n",
    "                i += num_proc\n",
    "\n",
    "    return np.vstack(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_word_indices = transform_text(train_test_df).map(lambda x: [_word2idx.get(i) for i in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 26.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.zeros(\n",
    "    [word2idx_trainingX.shape[0] + train_test_word_indices.shape[0], 200, 300], dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2280\n",
      "1 2280 4560\n",
      "2 4560 6840\n",
      "3 6840 9120\n",
      "4 9120 11400\n",
      "5 11400 13680\n",
      "6 13680 15960\n",
      "7 15960 18240\n",
      "8 18240 20520\n",
      "9 20520 22800\n",
      "10 22800 25080\n",
      "11 25080 27360\n",
      "12 27360 29640\n",
      "13 29640 31920\n",
      "14 31920 34200\n",
      "15 34200 36480\n",
      "16 36480 38760\n",
      "17 38760 41040\n",
      "18 41040 43320\n",
      "19 43320 45600\n",
      "20 45600 47880\n",
      "21 47880 50160\n",
      "22 50160 52440\n",
      "23 52440 54720\n",
      "24 54720 56999\n",
      "CPU times: user 2min 28s, sys: 9.46 s, total: 2min 38s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "\n",
    "for ix, chunk in enumerate(np.array_split(pd.concat([word2idx_trainingX, train_test_word_indices]), 25)):\n",
    "    chunk = transform_mean_average(chunk)\n",
    "    j = i + chunk.shape[0]\n",
    "    x_train[i: j] = chunk\n",
    "    print ix, i, j\n",
    "\n",
    "    i = j    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 * 60000 * 300 * 8. / 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_target(y, size):\n",
    "    e = np.zeros(size)\n",
    "    e[y] = 1\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 6853\n",
    "# # 3110\n",
    "# x_dump = parallel_generate_word_vectors(\n",
    "#     pd.concat([word2idx_trainingX, train_test_word_indices]),\n",
    "#     transform_mean_average,\n",
    "#     stopwords=[],\n",
    "#     batch=1000,\n",
    "#     num_proc=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 28 ms, total: 2.29 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_samples = x_train.shape[0]\n",
    "\n",
    "training_Y = pd.DataFrame(zip(*np.where(trainingY.head(num_samples) == 1)), columns=['iloc', 'topics'])\n",
    "training_Y = training_Y.groupby('iloc')['topics'].apply(list)\n",
    "training_Y.index = trainingY.head(num_samples).index\n",
    "\n",
    "train_test_y = transform_topics(train_test_df, topics)\n",
    "y_train = pd.concat([training_Y, train_test_y])\n",
    "y_train = np.vstack(y_train.map(lambda x: build_target(x, len(topics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56999, 160), (56999, 200, 300))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as K\n",
    "import keras.backend as KB\n",
    "\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    TP = K.metrics.true_positives(y_true, K.round(y_pred))\n",
    "    FP = K.metrics.false_positives(y_true, K.round(y_pred))\n",
    "    FN = K.metrics.false_negatives(y_true, K.round(y_pred))\n",
    "    \n",
    "    p = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FP))\n",
    "    r = K.reduce_sum(TP) / (K.reduce_sum(TP) + K.reduce_sum(FN))\n",
    "    \n",
    "    return (2.0 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    # http://stackoverflow.com/questions/43345909/when-using-mectrics-in-model-compile-in-keras-report-valueerror-unknown-metr\n",
    "    # Count positive samples.\n",
    "    c1 = KB.sum(KB.round(KB.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = KB.sum(KB.round(KB.clip(y_pred, 0, 1)))\n",
    "    c3 = KB.sum(KB.round(KB.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Convolution1D, MaxPooling1D, Flatten, TimeDistributed\n",
    "from keras.models import Model\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "fma_input = Input(shape=(None, 300), name='fma_input')\n",
    "bma_input = Input(shape=(None, 300), name='bma_input')\n",
    "\n",
    "fma_x = LSTM(32, return_sequences=False)(fma_input)\n",
    "bma_x = LSTM(32, return_sequences=False, go_backwards=True)(bma_input)\n",
    "# We stack a deep densely-connected network on top\n",
    "\n",
    "merge_layer = keras.layers.concatenate([fma_x, bma_x])\n",
    "\n",
    "# x = TimeDistributed(Dense(128, activation='relu'))(merge_layer)\n",
    "x = Dense(128, activation='relu')(merge_layer)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(len(class2ind), activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[\n",
    "        fma_input,\n",
    "        bma_input\n",
    "    ],\n",
    "    outputs=[main_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "fma_input (InputLayer)           (None, None, 300)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bma_input (InputLayer)           (None, None, 300)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                   (None, 32)            42624                                        \n",
      "____________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                   (None, 32)            42624                                        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 128)           8320                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 160)           20640                                        \n",
      "====================================================================================================\n",
      "Total params: 114,208\n",
      "Trainable params: 114,208\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.003)\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # keras.optimizers.RMSprop(lr=0.005),  # , rho=0.9, epsilon=1e-08, decay=0.0, clipnorm=1),\n",
    "    loss={'main_output': 'categorical_crossentropy'},\n",
    "    loss_weights={'main_output': 1.},\n",
    "    metrics=['accuracy', f1_micro]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56999, 160)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_train_data(x, y, batch_size=1000, shuffle=True):\n",
    "    s = x.shape[0]\n",
    "    q = np.arange(0, s, batch_size)\n",
    "    indices = zip(q, q + batch_size)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for i, j in indices:\n",
    "            _y = y[i: j]\n",
    "            _x = x[i: j]\n",
    "            if _y.shape[0] > 0:\n",
    "#                 print('hello')\n",
    "                yield (np.vstack(_x), _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "epochs = 10\n",
    "# steps_per_epoch = (x_dump.shape[0] + batch_size) / batch_size\n",
    "\n",
    "# train_data_generator = generate_batch_train_data(x_dump, y_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model.fit_generator(\n",
    "#     train_data_generator,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     epochs=epochs,\n",
    "#     workers=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_file_name = \"actual-bidirectional-lstm-weights\"\n",
    "filepath = base_file_name + \".{epoch:02d}-{loss:.4f}-{acc:.4f}-{f1_micro:.4f}.hdf5\"\n",
    "filename = base_file_name + \".log\"\n",
    "\n",
    "model_check_point = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='f1_micro', verbose=0,\n",
    "    save_best_only=True, save_weights_only=False,\n",
    "    mode='max', period=1,\n",
    ")\n",
    "\n",
    "remote_monitor = keras.callbacks.RemoteMonitor(\n",
    "    root='http://localhost:9000',\n",
    "    path='/publish/epoch/end/',\n",
    "    field='data', headers=None\n",
    ")\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(filename=filename, separator=',', append=False)\n",
    "\n",
    "callbacks = [model_check_point, remote_monitor, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56999/56999 [==============================] - 32s - loss: 6.4130 - acc: 0.0748 - f1_micro: 0.0636    \n",
      "Epoch 2/10\n",
      "56999/56999 [==============================] - 32s - loss: 5.5858 - acc: 0.1288 - f1_micro: 0.0915    \n",
      "Epoch 3/10\n",
      "56999/56999 [==============================] - 31s - loss: 4.9795 - acc: 0.1935 - f1_micro: 0.1075    \n",
      "Epoch 4/10\n",
      "56999/56999 [==============================] - 31s - loss: 4.5148 - acc: 0.2622 - f1_micro: 0.1158    \n",
      "Epoch 5/10\n",
      "56999/56999 [==============================] - 31s - loss: 4.1135 - acc: 0.3273 - f1_micro: 0.1203    \n",
      "Epoch 6/10\n",
      "56999/56999 [==============================] - 31s - loss: 3.7717 - acc: 0.3886 - f1_micro: 0.1232    \n",
      "Epoch 7/10\n",
      "56999/56999 [==============================] - 31s - loss: 3.4776 - acc: 0.4406 - f1_micro: 0.1257    \n",
      "Epoch 8/10\n",
      "56999/56999 [==============================] - 31s - loss: 3.2444 - acc: 0.4815 - f1_micro: 0.1292    \n",
      "Epoch 9/10\n",
      "56999/56999 [==============================] - 32s - loss: 3.0688 - acc: 0.5078 - f1_micro: 0.1343    \n",
      "Epoch 10/10\n",
      "56999/56999 [==============================] - 32s - loss: 2.9287 - acc: 0.5307 - f1_micro: 0.1400    \n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "batch_size = 2000\n",
    "epochs = 10\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.8313 - acc: 0.5465 - f1_micro: 0.1462    \n",
      "Epoch 2/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.7784 - acc: 0.5526 - f1_micro: 0.1524    \n",
      "Epoch 3/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.7184 - acc: 0.5621 - f1_micro: 0.1590    \n",
      "Epoch 4/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.6853 - acc: 0.5647 - f1_micro: 0.1656    \n",
      "Epoch 5/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.6365 - acc: 0.5739 - f1_micro: 0.1726    \n",
      "Epoch 6/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.5846 - acc: 0.5822 - f1_micro: 0.1796    \n",
      "Epoch 7/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.5587 - acc: 0.5843 - f1_micro: 0.1866    \n",
      "Epoch 8/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.5226 - acc: 0.5885 - f1_micro: 0.1938    \n",
      "Epoch 9/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.4886 - acc: 0.5931 - f1_micro: 0.2008    \n",
      "Epoch 10/10\n",
      "56999/56999 [==============================] - 24s - loss: 2.4612 - acc: 0.5958 - f1_micro: 0.2079    \n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "batch_size = 2000\n",
    "epochs = 10\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.4305 - acc: 0.6010 - f1_micro: 0.2150    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.4084 - acc: 0.6044 - f1_micro: 0.2220    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.3710 - acc: 0.6076 - f1_micro: 0.2289    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.3591 - acc: 0.6093 - f1_micro: 0.2356    \n",
      "Epoch 5/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.3307 - acc: 0.6148 - f1_micro: 0.2423    \n",
      "Epoch 6/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.3068 - acc: 0.6165 - f1_micro: 0.2487    \n",
      "Epoch 7/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.2888 - acc: 0.6185 - f1_micro: 0.2552    \n",
      "Epoch 8/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.2671 - acc: 0.6204 - f1_micro: 0.2615    \n",
      "Epoch 9/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.2459 - acc: 0.6258 - f1_micro: 0.2676    \n",
      "Epoch 10/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.2360 - acc: 0.6266 - f1_micro: 0.2736    \n",
      "Epoch 11/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.2137 - acc: 0.6289 - f1_micro: 0.2795    \n",
      "Epoch 12/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1926 - acc: 0.6333 - f1_micro: 0.2852    \n",
      "Epoch 13/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1849 - acc: 0.6332 - f1_micro: 0.2907    \n",
      "Epoch 14/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1607 - acc: 0.6356 - f1_micro: 0.2961    \n",
      "Epoch 15/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1493 - acc: 0.6359 - f1_micro: 0.3016    \n",
      "Epoch 16/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1390 - acc: 0.6375 - f1_micro: 0.3069    \n",
      "Epoch 17/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1159 - acc: 0.6403 - f1_micro: 0.3122    \n",
      "Epoch 18/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.1104 - acc: 0.6402 - f1_micro: 0.3172    \n",
      "Epoch 19/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0962 - acc: 0.6432 - f1_micro: 0.3222    \n",
      "Epoch 20/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0810 - acc: 0.6452 - f1_micro: 0.3270    \n",
      "Epoch 21/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0677 - acc: 0.6470 - f1_micro: 0.3316    \n",
      "Epoch 22/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0629 - acc: 0.6454 - f1_micro: 0.3363    \n",
      "Epoch 23/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0396 - acc: 0.6482 - f1_micro: 0.3408    \n",
      "Epoch 24/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0398 - acc: 0.6481 - f1_micro: 0.3452    \n",
      "Epoch 25/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0189 - acc: 0.6519 - f1_micro: 0.3495    \n",
      "Epoch 26/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0098 - acc: 0.6532 - f1_micro: 0.3538    \n",
      "Epoch 27/50\n",
      "56999/56999 [==============================] - 24s - loss: 2.0025 - acc: 0.6519 - f1_micro: 0.3579    \n",
      "Epoch 28/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9845 - acc: 0.6567 - f1_micro: 0.3618    \n",
      "Epoch 29/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9761 - acc: 0.6546 - f1_micro: 0.3658    \n",
      "Epoch 30/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9700 - acc: 0.6581 - f1_micro: 0.3697    \n",
      "Epoch 31/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9623 - acc: 0.6588 - f1_micro: 0.3735    \n",
      "Epoch 32/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9487 - acc: 0.6610 - f1_micro: 0.3772    \n",
      "Epoch 33/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9380 - acc: 0.6603 - f1_micro: 0.3809    \n",
      "Epoch 34/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9347 - acc: 0.6613 - f1_micro: 0.3845    \n",
      "Epoch 35/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9200 - acc: 0.6658 - f1_micro: 0.3879    \n",
      "Epoch 36/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9141 - acc: 0.6618 - f1_micro: 0.3913    \n",
      "Epoch 37/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.9063 - acc: 0.6635 - f1_micro: 0.3946    \n",
      "Epoch 38/50\n",
      "56999/56999 [==============================] - 25s - loss: 1.8924 - acc: 0.6664 - f1_micro: 0.3979    \n",
      "Epoch 39/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8871 - acc: 0.6660 - f1_micro: 0.4011    \n",
      "Epoch 40/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8774 - acc: 0.6684 - f1_micro: 0.4043    \n",
      "Epoch 41/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8701 - acc: 0.6706 - f1_micro: 0.4074    \n",
      "Epoch 42/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8594 - acc: 0.6697 - f1_micro: 0.4104    \n",
      "Epoch 43/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8584 - acc: 0.6696 - f1_micro: 0.4134    \n",
      "Epoch 44/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8450 - acc: 0.6725 - f1_micro: 0.4163    \n",
      "Epoch 45/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8416 - acc: 0.6750 - f1_micro: 0.4192    \n",
      "Epoch 46/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8348 - acc: 0.6735 - f1_micro: 0.4221    \n",
      "Epoch 47/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8228 - acc: 0.6746 - f1_micro: 0.4249    \n",
      "Epoch 48/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8155 - acc: 0.6760 - f1_micro: 0.4276    \n",
      "Epoch 49/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8144 - acc: 0.6752 - f1_micro: 0.4303    \n",
      "Epoch 50/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.8030 - acc: 0.6766 - f1_micro: 0.4329    \n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "batch_size = 2000\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7898 - acc: 0.6784 - f1_micro: 0.4354    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7916 - acc: 0.6782 - f1_micro: 0.4379    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7758 - acc: 0.6799 - f1_micro: 0.4404    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7792 - acc: 0.6803 - f1_micro: 0.4428    \n",
      "Epoch 5/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7691 - acc: 0.6820 - f1_micro: 0.4452    \n",
      "Epoch 6/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7668 - acc: 0.6799 - f1_micro: 0.4476    \n",
      "Epoch 7/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7595 - acc: 0.6823 - f1_micro: 0.4499    \n",
      "Epoch 8/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7527 - acc: 0.6835 - f1_micro: 0.4522    \n",
      "Epoch 9/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7532 - acc: 0.6842 - f1_micro: 0.4545    \n",
      "Epoch 10/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7453 - acc: 0.6832 - f1_micro: 0.4567    \n",
      "Epoch 11/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7378 - acc: 0.6855 - f1_micro: 0.4589    \n",
      "Epoch 12/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7370 - acc: 0.6835 - f1_micro: 0.4610    \n",
      "Epoch 13/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7260 - acc: 0.6884 - f1_micro: 0.4631    \n",
      "Epoch 14/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7283 - acc: 0.6850 - f1_micro: 0.4652    \n",
      "Epoch 15/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7193 - acc: 0.6872 - f1_micro: 0.4673    \n",
      "Epoch 16/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7159 - acc: 0.6874 - f1_micro: 0.4693    \n",
      "Epoch 17/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7086 - acc: 0.6871 - f1_micro: 0.4713    \n",
      "Epoch 18/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.7060 - acc: 0.6880 - f1_micro: 0.4732    \n",
      "Epoch 19/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6963 - acc: 0.6901 - f1_micro: 0.4752    \n",
      "Epoch 20/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6994 - acc: 0.6913 - f1_micro: 0.4771    \n",
      "Epoch 21/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6895 - acc: 0.6919 - f1_micro: 0.4789    \n",
      "Epoch 22/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6867 - acc: 0.6917 - f1_micro: 0.4808    \n",
      "Epoch 23/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6844 - acc: 0.6918 - f1_micro: 0.4826    \n",
      "Epoch 24/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6821 - acc: 0.6921 - f1_micro: 0.4843    \n",
      "Epoch 25/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6749 - acc: 0.6928 - f1_micro: 0.4861    \n",
      "Epoch 26/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6689 - acc: 0.6938 - f1_micro: 0.4878    \n",
      "Epoch 27/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6668 - acc: 0.6957 - f1_micro: 0.4895    \n",
      "Epoch 28/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6610 - acc: 0.6949 - f1_micro: 0.4912    \n",
      "Epoch 29/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6573 - acc: 0.6942 - f1_micro: 0.4929    \n",
      "Epoch 30/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6545 - acc: 0.6965 - f1_micro: 0.4945    \n",
      "Epoch 31/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6508 - acc: 0.6971 - f1_micro: 0.4962    \n",
      "Epoch 32/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6457 - acc: 0.6991 - f1_micro: 0.4978    \n",
      "Epoch 33/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6410 - acc: 0.6967 - f1_micro: 0.4993    \n",
      "Epoch 34/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6381 - acc: 0.6972 - f1_micro: 0.5009    \n",
      "Epoch 35/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6387 - acc: 0.6982 - f1_micro: 0.5024    \n",
      "Epoch 36/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6276 - acc: 0.6980 - f1_micro: 0.5039    \n",
      "Epoch 37/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6230 - acc: 0.6999 - f1_micro: 0.5054    \n",
      "Epoch 38/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6264 - acc: 0.6972 - f1_micro: 0.5069    \n",
      "Epoch 39/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6205 - acc: 0.7013 - f1_micro: 0.5084    \n",
      "Epoch 40/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6112 - acc: 0.7021 - f1_micro: 0.5098    \n",
      "Epoch 41/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6070 - acc: 0.7026 - f1_micro: 0.5112    \n",
      "Epoch 42/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6114 - acc: 0.7013 - f1_micro: 0.5127    \n",
      "Epoch 43/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6037 - acc: 0.7030 - f1_micro: 0.5140    \n",
      "Epoch 44/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.6028 - acc: 0.7018 - f1_micro: 0.5154    \n",
      "Epoch 45/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.5995 - acc: 0.7040 - f1_micro: 0.5168    \n",
      "Epoch 46/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.5944 - acc: 0.7034 - f1_micro: 0.5181    \n",
      "Epoch 47/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.5930 - acc: 0.7028 - f1_micro: 0.5194    \n",
      "Epoch 48/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.5893 - acc: 0.7045 - f1_micro: 0.5208    \n",
      "Epoch 49/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.5794 - acc: 0.7047 - f1_micro: 0.5221    \n",
      "Epoch 50/50\n",
      "56999/56999 [==============================] - 23s - loss: 1.5848 - acc: 0.7043 - f1_micro: 0.5233    \n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "batch_size = 2500\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5817 - acc: 0.7032 - f1_micro: 0.5256    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5705 - acc: 0.7061 - f1_micro: 0.5268    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5694 - acc: 0.7058 - f1_micro: 0.5280    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5663 - acc: 0.7067 - f1_micro: 0.5292    \n",
      "Epoch 5/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5642 - acc: 0.7071 - f1_micro: 0.5304    \n",
      "Epoch 6/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5607 - acc: 0.7105 - f1_micro: 0.5316    \n",
      "Epoch 7/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5590 - acc: 0.7081 - f1_micro: 0.5328    \n",
      "Epoch 8/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5576 - acc: 0.7091 - f1_micro: 0.5339    \n",
      "Epoch 9/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5505 - acc: 0.7088 - f1_micro: 0.5351    \n",
      "Epoch 10/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5499 - acc: 0.7070 - f1_micro: 0.5362    \n",
      "Epoch 11/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5439 - acc: 0.7097 - f1_micro: 0.5373    \n",
      "Epoch 12/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5399 - acc: 0.7103 - f1_micro: 0.5384    \n",
      "Epoch 13/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5356 - acc: 0.7106 - f1_micro: 0.5396    \n",
      "Epoch 14/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5287 - acc: 0.7115 - f1_micro: 0.5407    \n",
      "Epoch 15/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5314 - acc: 0.7111 - f1_micro: 0.5417    \n",
      "Epoch 16/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5301 - acc: 0.7111 - f1_micro: 0.5428    \n",
      "Epoch 17/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5202 - acc: 0.7134 - f1_micro: 0.5439    \n",
      "Epoch 18/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5182 - acc: 0.7126 - f1_micro: 0.5450    \n",
      "Epoch 19/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5206 - acc: 0.7136 - f1_micro: 0.5460    \n",
      "Epoch 20/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5116 - acc: 0.7155 - f1_micro: 0.5470    \n",
      "Epoch 21/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5130 - acc: 0.7138 - f1_micro: 0.5481    \n",
      "Epoch 22/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4988 - acc: 0.7149 - f1_micro: 0.5491    \n",
      "Epoch 23/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5119 - acc: 0.7123 - f1_micro: 0.5501    \n",
      "Epoch 24/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4965 - acc: 0.7157 - f1_micro: 0.5511    \n",
      "Epoch 25/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.5011 - acc: 0.7135 - f1_micro: 0.5521    \n",
      "Epoch 26/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4938 - acc: 0.7161 - f1_micro: 0.5531    \n",
      "Epoch 27/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4945 - acc: 0.7148 - f1_micro: 0.5540    \n",
      "Epoch 28/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4933 - acc: 0.7140 - f1_micro: 0.5550    \n",
      "Epoch 29/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4851 - acc: 0.7173 - f1_micro: 0.5560    \n",
      "Epoch 30/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4869 - acc: 0.7174 - f1_micro: 0.5570    \n",
      "Epoch 31/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4831 - acc: 0.7195 - f1_micro: 0.5579    \n",
      "Epoch 32/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4727 - acc: 0.7171 - f1_micro: 0.5588    \n",
      "Epoch 33/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4716 - acc: 0.7181 - f1_micro: 0.5598    \n",
      "Epoch 34/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4712 - acc: 0.7174 - f1_micro: 0.5607    \n",
      "Epoch 35/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4664 - acc: 0.7196 - f1_micro: 0.5616    \n",
      "Epoch 36/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4712 - acc: 0.7191 - f1_micro: 0.5625    \n",
      "Epoch 37/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4609 - acc: 0.7198 - f1_micro: 0.5634    \n",
      "Epoch 38/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4651 - acc: 0.7175 - f1_micro: 0.5643    \n",
      "Epoch 39/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4566 - acc: 0.7208 - f1_micro: 0.5652    \n",
      "Epoch 40/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4585 - acc: 0.7183 - f1_micro: 0.5661    \n",
      "Epoch 41/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4503 - acc: 0.7215 - f1_micro: 0.5670    \n",
      "Epoch 42/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4508 - acc: 0.7222 - f1_micro: 0.5679    \n",
      "Epoch 43/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4568 - acc: 0.7215 - f1_micro: 0.5688    \n",
      "Epoch 44/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4405 - acc: 0.7232 - f1_micro: 0.5696    \n",
      "Epoch 45/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4418 - acc: 0.7231 - f1_micro: 0.5705    \n",
      "Epoch 46/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4388 - acc: 0.7229 - f1_micro: 0.5713    \n",
      "Epoch 47/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4380 - acc: 0.7234 - f1_micro: 0.5722    \n",
      "Epoch 48/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4353 - acc: 0.7216 - f1_micro: 0.5730    \n",
      "Epoch 49/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4359 - acc: 0.7230 - f1_micro: 0.5738    \n",
      "Epoch 50/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4311 - acc: 0.7233 - f1_micro: 0.5747    \n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "batch_size = 2000\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4266 - acc: 0.7215 - f1_micro: 0.5755    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4278 - acc: 0.7235 - f1_micro: 0.5763    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4270 - acc: 0.7235 - f1_micro: 0.5772    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4191 - acc: 0.7268 - f1_micro: 0.5780    \n",
      "Epoch 5/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4164 - acc: 0.7259 - f1_micro: 0.5788    \n",
      "Epoch 6/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4233 - acc: 0.7232 - f1_micro: 0.5796    \n",
      "Epoch 7/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4140 - acc: 0.7255 - f1_micro: 0.5804    \n",
      "Epoch 8/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4124 - acc: 0.7253 - f1_micro: 0.5812    \n",
      "Epoch 9/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4129 - acc: 0.7258 - f1_micro: 0.5820    \n",
      "Epoch 10/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4111 - acc: 0.7262 - f1_micro: 0.5828    \n",
      "Epoch 11/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4062 - acc: 0.7262 - f1_micro: 0.5835    \n",
      "Epoch 12/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3952 - acc: 0.7285 - f1_micro: 0.5843    \n",
      "Epoch 13/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3948 - acc: 0.7285 - f1_micro: 0.5851    \n",
      "Epoch 14/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.4025 - acc: 0.7277 - f1_micro: 0.5859    \n",
      "Epoch 15/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3940 - acc: 0.7270 - f1_micro: 0.5866    \n",
      "Epoch 16/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3964 - acc: 0.7255 - f1_micro: 0.5874    \n",
      "Epoch 17/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3902 - acc: 0.7273 - f1_micro: 0.5881    \n",
      "Epoch 18/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3866 - acc: 0.7273 - f1_micro: 0.5889    \n",
      "Epoch 19/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3898 - acc: 0.7288 - f1_micro: 0.5896    \n",
      "Epoch 20/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3911 - acc: 0.7282 - f1_micro: 0.5904    \n",
      "Epoch 21/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3818 - acc: 0.7290 - f1_micro: 0.5911    \n",
      "Epoch 22/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3816 - acc: 0.7292 - f1_micro: 0.5918    \n",
      "Epoch 23/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3814 - acc: 0.7289 - f1_micro: 0.5925    \n",
      "Epoch 24/50\n",
      "56999/56999 [==============================] - 25s - loss: 1.3785 - acc: 0.7309 - f1_micro: 0.5932    \n",
      "Epoch 25/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3849 - acc: 0.7283 - f1_micro: 0.5939    \n",
      "Epoch 26/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3722 - acc: 0.7311 - f1_micro: 0.5947    \n",
      "Epoch 27/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3713 - acc: 0.7279 - f1_micro: 0.5954    \n",
      "Epoch 28/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3755 - acc: 0.7295 - f1_micro: 0.5961    \n",
      "Epoch 29/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3696 - acc: 0.7307 - f1_micro: 0.5967    \n",
      "Epoch 30/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3718 - acc: 0.7313 - f1_micro: 0.5974    \n",
      "Epoch 31/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3621 - acc: 0.7318 - f1_micro: 0.5981    \n",
      "Epoch 32/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3666 - acc: 0.7300 - f1_micro: 0.5988    \n",
      "Epoch 33/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3578 - acc: 0.7305 - f1_micro: 0.5995    \n",
      "Epoch 34/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3613 - acc: 0.7318 - f1_micro: 0.6001    \n",
      "Epoch 35/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3561 - acc: 0.7314 - f1_micro: 0.6008    \n",
      "Epoch 36/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3565 - acc: 0.7321 - f1_micro: 0.6015    \n",
      "Epoch 37/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3579 - acc: 0.7323 - f1_micro: 0.6022    \n",
      "Epoch 38/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3528 - acc: 0.7325 - f1_micro: 0.6028    \n",
      "Epoch 39/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3509 - acc: 0.7306 - f1_micro: 0.6035    \n",
      "Epoch 40/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3551 - acc: 0.7321 - f1_micro: 0.6041    \n",
      "Epoch 41/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3471 - acc: 0.7303 - f1_micro: 0.6048    \n",
      "Epoch 42/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3469 - acc: 0.7318 - f1_micro: 0.6054    \n",
      "Epoch 43/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3401 - acc: 0.7349 - f1_micro: 0.6061    \n",
      "Epoch 44/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3461 - acc: 0.7342 - f1_micro: 0.6067    \n",
      "Epoch 45/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3414 - acc: 0.7343 - f1_micro: 0.6074    \n",
      "Epoch 46/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3407 - acc: 0.7327 - f1_micro: 0.6080    \n",
      "Epoch 47/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3347 - acc: 0.7367 - f1_micro: 0.6086    \n",
      "Epoch 48/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3370 - acc: 0.7355 - f1_micro: 0.6093    \n",
      "Epoch 49/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3357 - acc: 0.7351 - f1_micro: 0.6099    \n",
      "Epoch 50/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3337 - acc: 0.7363 - f1_micro: 0.6105    \n",
      "CPU times: user 22min 35s, sys: 3min 47s, total: 26min 23s\n",
      "Wall time: 20min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2000\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3363 - acc: 0.7342 - f1_micro: 0.6111    \n",
      "Epoch 2/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3243 - acc: 0.7360 - f1_micro: 0.6117    \n",
      "Epoch 3/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3341 - acc: 0.7337 - f1_micro: 0.6123    \n",
      "Epoch 4/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3248 - acc: 0.7349 - f1_micro: 0.6129    \n",
      "Epoch 5/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3244 - acc: 0.7352 - f1_micro: 0.6135    \n",
      "Epoch 6/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3218 - acc: 0.7378 - f1_micro: 0.6142    \n",
      "Epoch 7/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3262 - acc: 0.7358 - f1_micro: 0.6148    \n",
      "Epoch 8/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3198 - acc: 0.7376 - f1_micro: 0.6154    \n",
      "Epoch 9/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3202 - acc: 0.7381 - f1_micro: 0.6160    \n",
      "Epoch 10/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3160 - acc: 0.7376 - f1_micro: 0.6166    \n",
      "Epoch 11/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3137 - acc: 0.7379 - f1_micro: 0.6172    \n",
      "Epoch 12/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3150 - acc: 0.7346 - f1_micro: 0.6177    \n",
      "Epoch 13/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3125 - acc: 0.7359 - f1_micro: 0.6183    \n",
      "Epoch 14/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3172 - acc: 0.7360 - f1_micro: 0.6189    \n",
      "Epoch 15/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3112 - acc: 0.7361 - f1_micro: 0.6195    \n",
      "Epoch 16/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3050 - acc: 0.7367 - f1_micro: 0.6201    \n",
      "Epoch 17/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3076 - acc: 0.7377 - f1_micro: 0.6206    \n",
      "Epoch 18/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3116 - acc: 0.7373 - f1_micro: 0.6212    \n",
      "Epoch 19/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3002 - acc: 0.7372 - f1_micro: 0.6218    \n",
      "Epoch 20/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3088 - acc: 0.7374 - f1_micro: 0.6223    \n",
      "Epoch 21/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3033 - acc: 0.7370 - f1_micro: 0.6229    \n",
      "Epoch 22/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2956 - acc: 0.7392 - f1_micro: 0.6234    \n",
      "Epoch 23/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.3041 - acc: 0.7372 - f1_micro: 0.6240    \n",
      "Epoch 24/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2967 - acc: 0.7393 - f1_micro: 0.6245    \n",
      "Epoch 25/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2994 - acc: 0.7382 - f1_micro: 0.6251    \n",
      "Epoch 26/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2947 - acc: 0.7386 - f1_micro: 0.6256    \n",
      "Epoch 27/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2942 - acc: 0.7405 - f1_micro: 0.6262    \n",
      "Epoch 28/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2921 - acc: 0.7392 - f1_micro: 0.6267    \n",
      "Epoch 29/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2872 - acc: 0.7412 - f1_micro: 0.6273    \n",
      "Epoch 30/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2915 - acc: 0.7391 - f1_micro: 0.6278    \n",
      "Epoch 31/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2984 - acc: 0.7388 - f1_micro: 0.6283    \n",
      "Epoch 32/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2881 - acc: 0.7395 - f1_micro: 0.6289    \n",
      "Epoch 33/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2860 - acc: 0.7394 - f1_micro: 0.6294    \n",
      "Epoch 34/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2863 - acc: 0.7381 - f1_micro: 0.6299    \n",
      "Epoch 35/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2843 - acc: 0.7415 - f1_micro: 0.6304    \n",
      "Epoch 36/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2832 - acc: 0.7385 - f1_micro: 0.6310    \n",
      "Epoch 37/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2792 - acc: 0.7400 - f1_micro: 0.6315    \n",
      "Epoch 38/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2803 - acc: 0.7416 - f1_micro: 0.6320    \n",
      "Epoch 39/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2811 - acc: 0.7397 - f1_micro: 0.6325    \n",
      "Epoch 40/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2733 - acc: 0.7424 - f1_micro: 0.6330    \n",
      "Epoch 41/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2780 - acc: 0.7388 - f1_micro: 0.6335    \n",
      "Epoch 42/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2721 - acc: 0.7427 - f1_micro: 0.6340    \n",
      "Epoch 43/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2775 - acc: 0.7422 - f1_micro: 0.6345    \n",
      "Epoch 44/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2718 - acc: 0.7423 - f1_micro: 0.6350    \n",
      "Epoch 45/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2722 - acc: 0.7395 - f1_micro: 0.6355    \n",
      "Epoch 46/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2719 - acc: 0.7407 - f1_micro: 0.6360    \n",
      "Epoch 47/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2721 - acc: 0.7409 - f1_micro: 0.6365    \n",
      "Epoch 48/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2599 - acc: 0.7417 - f1_micro: 0.6370    \n",
      "Epoch 49/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2692 - acc: 0.7413 - f1_micro: 0.6375    \n",
      "Epoch 50/50\n",
      "56999/56999 [==============================] - 24s - loss: 1.2697 - acc: 0.7420 - f1_micro: 0.6380    \n",
      "CPU times: user 22min 34s, sys: 3min 46s, total: 26min 20s\n",
      "Wall time: 20min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2000\n",
    "epochs = 50\n",
    "hist = model.fit(\n",
    "    [x_train, x_train],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as sk_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = model.predict(\n",
    "    [x_train, x_train],\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 (0.0, 0.020880745251391636)\n",
      "0.01 (0.01, 0.6726014644899474)\n",
      "0.02 (0.02, 0.72101077588096552)\n",
      "0.03 (0.029999999999999999, 0.74744159104074137)\n",
      "0.04 (0.040000000000000001, 0.76474864236947504)\n",
      "0.05 (0.050000000000000003, 0.77706702263968941)\n",
      "0.06 (0.059999999999999998, 0.78639995230145476)\n",
      "0.07 (0.070000000000000007, 0.79346799408018032)\n",
      "0.08 (0.080000000000000002, 0.79917763832492528)\n",
      "0.09 (0.089999999999999997, 0.80371706683971444)\n",
      "0.1 (0.10000000000000001, 0.80712421698197423)\n",
      "0.11 (0.11, 0.80978243574375997)\n",
      "0.12 (0.12, 0.81227444555102213)\n",
      "0.13 (0.13, 0.81385917257542784)\n",
      "0.14 (0.14000000000000001, 0.81558336873522652)\n",
      "0.15 (0.14999999999999999, 0.81655641863146056)\n",
      "0.16 (0.16, 0.81742265874727638)\n",
      "0.17 (0.17000000000000001, 0.8181007976471637)\n",
      "0.18 (0.17999999999999999, 0.81834903544190218)\n",
      "0.19 (0.19, 0.81845056755353951)\n",
      "0.2 (0.20000000000000001, 0.81850533807829184)\n",
      "0.21 (0.20999999999999999, 0.8180380198607804)\n",
      "0.22 (0.22, 0.81798080623022185)\n",
      "0.23 (0.23000000000000001, 0.81773404703749253)\n",
      "0.24 (0.23999999999999999, 0.81718258528036025)\n",
      "0.25 (0.25, 0.81627771295215856)\n",
      "0.26 (0.26000000000000001, 0.81559763771677241)\n",
      "0.27 (0.27000000000000002, 0.81441919415215025)\n",
      "0.28 (0.28000000000000003, 0.81311793942194355)\n",
      "0.29 (0.28999999999999998, 0.81186162234232528)\n",
      "0.3 (0.29999999999999999, 0.81040554356469385)\n",
      "0.31 (0.31, 0.80867788461538459)\n",
      "0.32 (0.32000000000000001, 0.80707665368475079)\n",
      "0.33 (0.33000000000000002, 0.80570600258371294)\n",
      "0.34 (0.34000000000000002, 0.80421262936049165)\n",
      "0.35 (0.35000000000000003, 0.80245191601820587)\n",
      "0.36 (0.35999999999999999, 0.80056288521688901)\n",
      "0.37 (0.37, 0.79846200371538423)\n",
      "0.38 (0.38, 0.79659598733102355)\n",
      "0.39 (0.39000000000000001, 0.79422315223077511)\n",
      "0.4 (0.40000000000000002, 0.79198593848713628)\n",
      "0.41 (0.41000000000000003, 0.78983108341551977)\n",
      "0.42 (0.41999999999999998, 0.78776601783754474)\n",
      "0.43 (0.42999999999999999, 0.7850784660318425)\n",
      "0.44 (0.44, 0.78265461949605497)\n",
      "0.45 (0.45000000000000001, 0.78026991795107725)\n",
      "0.46 (0.46000000000000002, 0.77787472093202281)\n",
      "0.47 (0.47000000000000003, 0.77505702834018531)\n",
      "0.48 (0.47999999999999998, 0.77197524790927674)\n",
      "0.49 (0.48999999999999999, 0.76899369965084763)\n",
      "0.5 (0.5, 0.76603689824909582)\n",
      "0.51 (0.51000000000000001, 0.76303314428089042)\n",
      "0.52 (0.52000000000000002, 0.76047384007897334)\n",
      "0.53 (0.53000000000000003, 0.75734725572481254)\n",
      "0.54 (0.54000000000000004, 0.75438596491228072)\n",
      "0.55 (0.55000000000000004, 0.7516438712084369)\n",
      "0.56 (0.56000000000000005, 0.74824168690985271)\n",
      "0.57 (0.57000000000000006, 0.74492457447094573)\n",
      "0.58 (0.57999999999999996, 0.7410764949066444)\n",
      "0.59 (0.58999999999999997, 0.73785920271416461)\n",
      "0.6 (0.59999999999999998, 0.7340527790465412)\n",
      "CPU times: user 27.7 s, sys: 724 ms, total: 28.4 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt = 0.01\n",
    "s = 0.0\n",
    "e = 0.6\n",
    "th = np.arange(s, e + dt, dt)\n",
    "mean_fscores = []\n",
    "for t in th:\n",
    "    # %time \n",
    "    mean_fscores.append((t, sk_f1_score(y_train, 1.0 * (g > t), average='micro')))\n",
    "    print t, mean_fscores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20000000000000001, 0.81850533807829184)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh, thresh_score = sorted(mean_fscores, key=lambda x: x[1], reverse=True)[0]\n",
    "thresh, thresh_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd0c870cf10>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSMFdRGQKKbiLiEwhBXcRkSn0/wGsGQgcDFxCdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0c86f4590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ix = 13\n",
    "sd = -0\n",
    "pd.Series(g[sd:][ix]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([  7, 136, 150]),), (array([  7, 125, 136, 150]),))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thresh = 0.5\n",
    "np.where(y_train[sd:][ix] == 1), np.where(g[sd:][ix] > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unitednations'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(pred, scale_param=0.75, min_thresh=0.05, thresh = 0.5):\n",
    "#     mx = pred.mean() + 3 * pred.std()\n",
    "    return np.where(pred > thresh)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/TestData.json') as fl:\n",
    "    data = json.load(fl)\n",
    "    test_df = pd.DataFrame(data['TestData']).T\n",
    "    del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 s, sys: 204 ms, total: 37.3 s\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_word_indices = transform_text(test_df).map(lambda x: [_word2idx.get(i) for i in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_null_index = test_word_indices[test_word_indices.map(len) == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 41 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_test = np.zeros(\n",
    "    [test_word_indices.shape[0], 200, 300], dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 759\n",
      "1 759 1517\n",
      "2 1517 2275\n",
      "3 2275 3033\n",
      "4 3033 3791\n",
      "5 3791 4549\n",
      "6 4549 5307\n",
      "7 5307 6065\n",
      "8 6065 6823\n",
      "9 6823 7581\n",
      "CPU times: user 23.9 s, sys: 1.79 s, total: 25.7 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ii = 0\n",
    "\n",
    "for ix, chunk in enumerate(np.array_split(test_word_indices, 10)):\n",
    "    chunk = transform_mean_average(chunk)\n",
    "    jj = ii + chunk.shape[0]\n",
    "    x_test[ii: jj] = chunk\n",
    "    print ix, ii, jj\n",
    "\n",
    "    ii = jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_probas = model.predict([x_test, x_test], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12762.0\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# valid_test_feature_vec found below!\n",
    "thresh = 0.3\n",
    "# pos_count = []\n",
    "# dt = 0.01\n",
    "# for thresh in  np.arange(0, 0.5 + dt, dt):\n",
    "test_values = np.zeros([test_probas.shape[0], len(topics)])\n",
    "for ix, pred in enumerate(test_probas):\n",
    "    for v in get_classes(pred, thresh=thresh):\n",
    "        test_values[ix][v] = 1\n",
    "\n",
    "test_sub_df = pd.DataFrame(\n",
    "    test_values,\n",
    "    index=test_df.index,\n",
    "    columns=topics\n",
    ")\n",
    "\n",
    "test_sub_df = test_sub_df.sort_index()\n",
    "a = test_sub_df.sum(axis=0).sum()\n",
    "#     pos_count.append((thresh, a))\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pos_count = pd.DataFrame(pos_count, columns=['thresh', 'counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12762.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2696791664818157"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual-bidirectional-lstm-mean_averaged_window-loss_1.2697-f1_micro_0.6380-thresh_0.3.csv\n"
     ]
    }
   ],
   "source": [
    "sub_filename = 'actual-bidirectional-lstm-mean_averaged_window-loss_{:.4f}-f1_micro_{:.4f}-thresh_{}.csv'.format(hist.history['loss'][-1], hist.history['f1_micro'][-1], thresh)\n",
    "print sub_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sub_df.astype(int).reset_index().rename(\n",
    "    columns={'index': 'id'}\n",
    ").sort_values('id').to_csv(\n",
    "    sub_filename, \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     160.00000\n",
       "mean       69.68125\n",
       "std       154.89779\n",
       "min         0.00000\n",
       "25%         4.00000\n",
       "50%        16.50000\n",
       "75%        59.25000\n",
       "max      1390.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activism                                    0.0\n",
       "afghanistan                               173.0\n",
       "aid                                        89.0\n",
       "algerianhostagecrisis                      19.0\n",
       "alqaida                                   177.0\n",
       "alshabaab                                  40.0\n",
       "antiwar                                     0.0\n",
       "arabandmiddleeastprotests                 434.0\n",
       "armstrade                                 107.0\n",
       "australianguncontrol                        0.0\n",
       "australiansecurityandcounterterrorism      42.0\n",
       "bastilledaytruckattack                     15.0\n",
       "belgium                                    24.0\n",
       "berlinchristmasmarketattack                10.0\n",
       "bigdata                                     1.0\n",
       "biometrics                                  0.0\n",
       "bokoharam                                  41.0\n",
       "bostonmarathonbombing                      52.0\n",
       "britisharmy                                15.0\n",
       "brusselsattacks                            34.0\n",
       "cameroon                                    2.0\n",
       "carers                                      2.0\n",
       "charliehebdoattack                         74.0\n",
       "chemicalweapons                            20.0\n",
       "clusterbombs                                0.0\n",
       "cobra                                       9.0\n",
       "conflictanddevelopment                     78.0\n",
       "controversy                                 3.0\n",
       "criminaljustice                            31.0\n",
       "cybercrime                                 90.0\n",
       "                                          ...  \n",
       "somalia                                    72.0\n",
       "southafrica                                39.0\n",
       "southchinasea                               1.0\n",
       "stopandsearch                               1.0\n",
       "surveillance                              131.0\n",
       "sydneysiege                                18.0\n",
       "syria                                    1532.0\n",
       "taliban                                    53.0\n",
       "terrorism                                 231.0\n",
       "thailand                                   25.0\n",
       "torture                                    22.0\n",
       "traincrashes                                3.0\n",
       "transport                                  91.0\n",
       "tunisiaattack2015                          31.0\n",
       "turkey                                    218.0\n",
       "turkeycoupattempt                          25.0\n",
       "ukcrime                                   402.0\n",
       "uksecurity                                492.0\n",
       "uksupremecourt                              4.0\n",
       "undercoverpoliceandpolicing                 2.0\n",
       "unitednations                             296.0\n",
       "usguncontrol                              227.0\n",
       "values                                      0.0\n",
       "warcrimes                                  41.0\n",
       "warreporting                               11.0\n",
       "weaponstechnology                           8.0\n",
       "womeninbusiness                             0.0\n",
       "woolwichattack                             92.0\n",
       "worldmigration                             31.0\n",
       "zikavirus                                   4.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activism                                    0.0\n",
       "afghanistan                               147.0\n",
       "aid                                       127.0\n",
       "algerianhostagecrisis                      34.0\n",
       "alqaida                                   246.0\n",
       "alshabaab                                  46.0\n",
       "antiwar                                     0.0\n",
       "arabandmiddleeastprotests                 376.0\n",
       "armstrade                                  78.0\n",
       "australianguncontrol                        0.0\n",
       "australiansecurityandcounterterrorism      64.0\n",
       "bastilledaytruckattack                      9.0\n",
       "belgium                                    12.0\n",
       "berlinchristmasmarketattack                12.0\n",
       "bigdata                                     3.0\n",
       "biometrics                                  0.0\n",
       "bokoharam                                  65.0\n",
       "bostonmarathonbombing                      77.0\n",
       "britisharmy                                12.0\n",
       "brusselsattacks                            41.0\n",
       "cameroon                                    4.0\n",
       "carers                                      1.0\n",
       "charliehebdoattack                         51.0\n",
       "chemicalweapons                            20.0\n",
       "clusterbombs                                0.0\n",
       "cobra                                       9.0\n",
       "conflictanddevelopment                     97.0\n",
       "controversy                                 7.0\n",
       "criminaljustice                            16.0\n",
       "cybercrime                                103.0\n",
       "                                          ...  \n",
       "somalia                                    85.0\n",
       "southafrica                                51.0\n",
       "southchinasea                               5.0\n",
       "stopandsearch                               3.0\n",
       "surveillance                              112.0\n",
       "sydneysiege                                40.0\n",
       "syria                                    1749.0\n",
       "taliban                                    66.0\n",
       "terrorism                                 190.0\n",
       "thailand                                   28.0\n",
       "torture                                    29.0\n",
       "traincrashes                                3.0\n",
       "transport                                  75.0\n",
       "tunisiaattack2015                          36.0\n",
       "turkey                                    271.0\n",
       "turkeycoupattempt                          25.0\n",
       "ukcrime                                   335.0\n",
       "uksecurity                                409.0\n",
       "uksupremecourt                              5.0\n",
       "undercoverpoliceandpolicing                 2.0\n",
       "unitednations                             268.0\n",
       "usguncontrol                              169.0\n",
       "values                                      0.0\n",
       "warcrimes                                  40.0\n",
       "warreporting                               17.0\n",
       "weaponstechnology                           7.0\n",
       "womeninbusiness                             0.0\n",
       "woolwichattack                             81.0\n",
       "worldmigration                             13.0\n",
       "zikavirus                                   3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activism                                    0.0\n",
       "afghanistan                               129.0\n",
       "aid                                        87.0\n",
       "algerianhostagecrisis                      22.0\n",
       "alqaida                                   224.0\n",
       "alshabaab                                  29.0\n",
       "antiwar                                     1.0\n",
       "arabandmiddleeastprotests                 429.0\n",
       "armstrade                                  67.0\n",
       "australianguncontrol                        0.0\n",
       "australiansecurityandcounterterrorism      84.0\n",
       "bastilledaytruckattack                     19.0\n",
       "belgium                                    53.0\n",
       "berlinchristmasmarketattack                15.0\n",
       "bigdata                                     7.0\n",
       "biometrics                                  1.0\n",
       "bokoharam                                  40.0\n",
       "bostonmarathonbombing                      79.0\n",
       "britisharmy                                16.0\n",
       "brusselsattacks                           108.0\n",
       "cameroon                                    2.0\n",
       "carers                                      0.0\n",
       "charliehebdoattack                         65.0\n",
       "chemicalweapons                            24.0\n",
       "clusterbombs                                1.0\n",
       "cobra                                       0.0\n",
       "conflictanddevelopment                     86.0\n",
       "controversy                                 1.0\n",
       "criminaljustice                            64.0\n",
       "cybercrime                                 78.0\n",
       "                                          ...  \n",
       "somalia                                    75.0\n",
       "southafrica                                32.0\n",
       "southchinasea                               0.0\n",
       "stopandsearch                               3.0\n",
       "surveillance                              166.0\n",
       "sydneysiege                                19.0\n",
       "syria                                    1635.0\n",
       "taliban                                    43.0\n",
       "terrorism                                 374.0\n",
       "thailand                                   28.0\n",
       "torture                                    23.0\n",
       "traincrashes                                3.0\n",
       "transport                                  88.0\n",
       "tunisiaattack2015                          38.0\n",
       "turkey                                    341.0\n",
       "turkeycoupattempt                          56.0\n",
       "ukcrime                                   384.0\n",
       "uksecurity                                740.0\n",
       "uksupremecourt                              4.0\n",
       "undercoverpoliceandpolicing                 4.0\n",
       "unitednations                             359.0\n",
       "usguncontrol                              196.0\n",
       "values                                      0.0\n",
       "warcrimes                                  33.0\n",
       "warreporting                                6.0\n",
       "weaponstechnology                          12.0\n",
       "womeninbusiness                             0.0\n",
       "woolwichattack                             61.0\n",
       "worldmigration                             17.0\n",
       "zikavirus                                   3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activism                                    0.0\n",
       "afghanistan                               117.0\n",
       "aid                                        57.0\n",
       "algerianhostagecrisis                       8.0\n",
       "alqaida                                   168.0\n",
       "alshabaab                                  36.0\n",
       "antiwar                                     0.0\n",
       "arabandmiddleeastprotests                 166.0\n",
       "armstrade                                  42.0\n",
       "australianguncontrol                        0.0\n",
       "australiansecurityandcounterterrorism      51.0\n",
       "bastilledaytruckattack                     22.0\n",
       "belgium                                    24.0\n",
       "berlinchristmasmarketattack                13.0\n",
       "bigdata                                     3.0\n",
       "biometrics                                  1.0\n",
       "bokoharam                                  35.0\n",
       "bostonmarathonbombing                      68.0\n",
       "britisharmy                                 2.0\n",
       "brusselsattacks                            63.0\n",
       "cameroon                                    3.0\n",
       "carers                                      0.0\n",
       "charliehebdoattack                         49.0\n",
       "chemicalweapons                            15.0\n",
       "clusterbombs                                0.0\n",
       "cobra                                       0.0\n",
       "conflictanddevelopment                     89.0\n",
       "controversy                                 0.0\n",
       "criminaljustice                            22.0\n",
       "cybercrime                                 50.0\n",
       "                                          ...  \n",
       "somalia                                    61.0\n",
       "southafrica                                27.0\n",
       "southchinasea                               1.0\n",
       "stopandsearch                               3.0\n",
       "surveillance                              100.0\n",
       "sydneysiege                                17.0\n",
       "syria                                    1530.0\n",
       "taliban                                    42.0\n",
       "terrorism                                 190.0\n",
       "thailand                                   27.0\n",
       "torture                                    14.0\n",
       "traincrashes                                1.0\n",
       "transport                                  61.0\n",
       "tunisiaattack2015                          26.0\n",
       "turkey                                    305.0\n",
       "turkeycoupattempt                          36.0\n",
       "ukcrime                                   169.0\n",
       "uksecurity                                471.0\n",
       "uksupremecourt                              7.0\n",
       "undercoverpoliceandpolicing                 3.0\n",
       "unitednations                             298.0\n",
       "usguncontrol                              209.0\n",
       "values                                      0.0\n",
       "warcrimes                                  17.0\n",
       "warreporting                                7.0\n",
       "weaponstechnology                          15.0\n",
       "womeninbusiness                             0.0\n",
       "woolwichattack                             18.0\n",
       "worldmigration                              9.0\n",
       "zikavirus                                   2.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
