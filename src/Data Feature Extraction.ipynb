{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from growing_instability_lib import *\n",
    "# Start mem 476 MB\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import tfidfmodel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsn = LineSentence('./corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dictionary = Dictionary(lsn)\n",
    "# dictionary.id2token = {j: i for i, j in dictionary.token2id.items()}\n",
    "# dictionary.save(lsn.source + '.dictionary')\n",
    "\n",
    "# CPU times: user 10min 31s, sys: 2.18 s, total: 10min 34s\n",
    "# Wall time: 10min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# corpus = [dictionary.doc2bow(i) for i in lsn]\n",
    "\n",
    "# CPU times: user 10min 59s, sys: 3min 26s, total: 14min 26s\n",
    "# Wall time: 31min 6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpora.MmCorpus.serialize(lsn.source + '.corpora', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load(lsn.source + '.dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _corpus_generator(lsn):\n",
    "    for l in lsn:\n",
    "        yield dictionary.doc2bow(l)\n",
    "\n",
    "corpus_generator = _corpus_generator(lsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tfidf = tfidfmodel.TfidfModel(corpus_generator)\n",
    "# tfidf.save(lsn.source + '.tfidf')\n",
    "\n",
    "# CPU times: user 2min 8s, sys: 26.1 s, total: 2min 34s\n",
    "# Wall time: 7min 28s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidfmodel.TfidfModel.load(lsn.source + '.tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = lsimodel.Lsimodel.load(lsn.source + '.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(215720, 1.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[dictionary.doc2bow(['zika'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_tfidf_word2vec(tokens, stopwords=[]):\n",
    "#     global wvmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    wv_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords and w in wvmodel.wv.vocab)]\n",
    "    ).map(\n",
    "        lambda x: tfidf[dictionary.doc2bow(x)]\n",
    "    ).map(\n",
    "        lambda x: np.array([wvmodel[dictionary.id2token[id]] * w for id, w in x]).mean(axis=0) if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return wv_feature_vec\n",
    "\n",
    "\n",
    "def transform_tfidf_fasttext(tokens, stopwords=[]):\n",
    "#     global fsmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    fs_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(\n",
    "        lambda x: tfidf[dictionary.doc2bow(x)]\n",
    "    ).map(\n",
    "        lambda x: np.array([np.array(fsmodel[dictionary.id2token[id]]) * w for id, w in x]).mean(axis=0) if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return fs_feature_vec\n",
    "\n",
    "\n",
    "def transform_tfidf_lsi(tokens, stopwords=[]):\n",
    "#     global fsmodel\n",
    "#     global tfidf\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    lsi_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(\n",
    "        lambda x: lsi[tfidf[dictionary.doc2bow(x)]] if len(x) > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    return lsi_feature_vec\n",
    "\n",
    "\n",
    "def transform_fasttext(tokens, stopwords=[]):\n",
    "#     global fsmodel\n",
    "    # This requires fsmodel to be present in the namespace.\n",
    "    fs_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    ).map(lambda x: np.array([fsmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return fs_feature_vec\n",
    "\n",
    "\n",
    "def transform_unsupervised_sentiment_neuron(tokens, stopwords=[]):\n",
    "    # This requires fsmodel to be present in the namespace.\n",
    "    \n",
    "    usn_feature_vec = usnmodel.transform(tokens)\n",
    "\n",
    "    # usn_feature_vec = tokens.map(\n",
    "    #     lambda x: [w for w in x.split() if (w not in stopwords)]\n",
    "    # ).map(lambda x: np.array([usnmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return usn_feature_vec\n",
    "\n",
    "\n",
    "def transform_word2vec(tokens, stopwords=[]):\n",
    "#     global wvmodel\n",
    "    # This requires wvmodel to be present in the namespace.\n",
    "    wv_feature_vec = tokens.map(\n",
    "        lambda x: [w for w in x.split() if (w not in stopwords and w in wvmodel.wv.vocab)]\n",
    "    ).map(lambda x: np.array([wvmodel[w] for w in x]).mean(axis=0) if len(x) > 0 else np.nan)\n",
    "\n",
    "    return wv_feature_vec\n",
    "\n",
    "\n",
    "def parallel_generate_word_vectors(samp, transformer, stopwords, batch, num_proc):\n",
    "    with Parallel(n_jobs=num_proc) as parallel:\n",
    "        dataset = []\n",
    "        is_break = False\n",
    "        i = 0\n",
    "\n",
    "        while not is_break:\n",
    "            payload = []\n",
    "\n",
    "            for j in xrange(num_proc):\n",
    "                t_df = samp[(i + j) * batch: (i + 1 + j) * batch]\n",
    "\n",
    "                if t_df.empty:\n",
    "                    is_break = True\n",
    "                    continue\n",
    "\n",
    "                payload.append(\n",
    "                    delayed(transformer)(\n",
    "                        t_df, stopwords\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            print('Current batch in main thread: {}'.format((i + j) * batch))\n",
    "\n",
    "            if payload:\n",
    "                results = parallel(payload)\n",
    "                dataset.extend(results)\n",
    "                i += num_proc\n",
    "\n",
    "    return pd.concat(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = Word2Vec.load('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsmodel = fasttext.load_model('corpus/train_body_data-with_labels_False-retain_special_chars_False.with_test_data.csv.fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.87 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_word2idx = {}\n",
    "\n",
    "def word2idx(word):\n",
    "    ix = len(_word2idx)\n",
    "    _word2idx[word] = _word2idx.get(word, ix)\n",
    "    return _word2idx[word]\n",
    "\n",
    "def generate_training_word_vectors(stopwords=set()):\n",
    "    processed_tname = set()\n",
    "    nan_rows = set()\n",
    "    target_columns = sorted(topics)\n",
    "    wvec_trainingX = pd.DataFrame()\n",
    "    fvec_trainingX = pd.DataFrame()\n",
    "    tfidf_wvec_trainingX = pd.DataFrame()\n",
    "    tfidf_fvec_trainingX = pd.DataFrame()\n",
    "    tfidf_lsi_trainingX = pd.DataFrame()\n",
    "    word2idx_trainingX = pd.DataFrame()\n",
    "    trainingY = pd.DataFrame()\n",
    "    num_proc = 8\n",
    "\n",
    "    for ix, tname in enumerate(glob.iglob('../data/TrainingData/*_TrainingData.json')):\n",
    "        print('Processing {}. {}...'.format(ix + 1, tname))\n",
    "\n",
    "        if tname in processed_tname or ix > 100:\n",
    "            continue\n",
    "\n",
    "        ds = parse_training_data_with_valid_topics(tname, topics=topics)\n",
    "        ds = pd.DataFrame(ds).T\n",
    "\n",
    "        for t in set(topics).difference(ds.columns):\n",
    "            ds[t] = np.nan\n",
    "\n",
    "        print('Done parsing {} docs with valid topics...'.format(ds.shape[0]))\n",
    "\n",
    "        df_tokens = transform_text(ds)\n",
    "        batch = min(df_tokens.shape[0] / num_proc, 2000)\n",
    "\n",
    "        print('Computing fs features...')\n",
    "        fvec = parallel_generate_word_vectors(df_tokens, transform_fasttext, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "        print('Computing wv features...')\n",
    "        wvec = parallel_generate_word_vectors(df_tokens, transform_word2vec, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "        print('Computing tfidf fs features...')\n",
    "        tfidf_fvec = parallel_generate_word_vectors(df_tokens, transform_tfidf_fasttext, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "        print('Computing tfidf wv features...')\n",
    "        tfidf_wvec = parallel_generate_word_vectors(df_tokens, transform_tfidf_word2vec, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "\n",
    "        print('Computing tfidf lsi features...')\n",
    "        tfidf_lsi = parallel_generate_word_vectors(df_tokens, transform_tfidf_lsi, stopwords=stopwords, batch=batch, num_proc=num_proc)\n",
    "        \n",
    "        nulls = set()\n",
    "\n",
    "        nulls.update(fvec[fvec.isnull()].index.values)\n",
    "        nulls.update(wvec[wvec.isnull()].index.values)\n",
    "        nulls.update(tfidf_fvec[tfidf_fvec.isnull()].index.values)\n",
    "        nulls.update(tfidf_wvec[tfidf_wvec.isnull()].index.values)\n",
    "        nulls.update(tfidf_lsi[tfidf_lsi.isnull()].index.values)\n",
    "\n",
    "        \n",
    "        print 'Droping {} NaN samples...'.format(len(nulls))\n",
    "\n",
    "        index = wvec.index.difference(nulls)\n",
    "        wvec = wvec.ix[index]\n",
    "        fvec = fvec.ix[index]\n",
    "        tfidf_wvec = tfidf_wvec.ix[index]\n",
    "        tfidf_fvec = tfidf_fvec.ix[index]\n",
    "        tfidf_lsi = tfidf_lsi.ix[index]\n",
    "        \n",
    "        if wvec_trainingX.empty:\n",
    "            wvec_trainingX = wvec\n",
    "        else:\n",
    "            wvec_trainingX = wvec_trainingX.append(wvec)\n",
    "\n",
    "        if fvec_trainingX.empty:\n",
    "            fvec_trainingX = fvec\n",
    "        else:\n",
    "            fvec_trainingX = fvec_trainingX.append(fvec)\n",
    "\n",
    "        if tfidf_wvec_trainingX.empty:\n",
    "            tfidf_wvec_trainingX = tfidf_wvec\n",
    "        else:\n",
    "            tfidf_wvec_trainingX = tfidf_wvec_trainingX.append(tfidf_wvec)\n",
    "\n",
    "        if tfidf_fvec_trainingX.empty:\n",
    "            tfidf_fvec_trainingX = tfidf_fvec\n",
    "        else:\n",
    "            tfidf_fvec_trainingX = tfidf_fvec_trainingX.append(tfidf_fvec)\n",
    "\n",
    "        if tfidf_lsi_trainingX.empty:\n",
    "            tfidf_lsi_trainingX = tfidf_lsi\n",
    "        else:\n",
    "            tfidf_lsi_trainingX = tfidf_lsi_trainingX.append(tfidf_lsi)\n",
    "\n",
    "        print('Mapping word indices...')\n",
    "        word_indices = df_tokens.ix[index].map(lambda x: [word2idx(i) for i in x.split()])\n",
    "\n",
    "        if word2idx_trainingX.empty:\n",
    "            word2idx_trainingX = word_indices\n",
    "        else:\n",
    "            word2idx_trainingX = word2idx_trainingX.append(word_indices)\n",
    "\n",
    "        print('Current training size is {}...'.format(word2idx_trainingX.shape[0]))\n",
    "\n",
    "        targets = ds[target_columns].ix[index]  # .fillna(0).as_matrix()\n",
    "\n",
    "        if trainingY.empty:\n",
    "            trainingY = targets\n",
    "        else:\n",
    "            trainingY = trainingY.append(targets)\n",
    "\n",
    "        print('Completed extracting data from {}...'.format(tname))\n",
    "        print\n",
    "        processed_tname.add(tname)\n",
    "\n",
    "    wvec_trainingX.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'wvec_trainingX')\n",
    "    fvec_trainingX.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'fvec_trainingX')\n",
    "    \n",
    "    tfidf_wvec_trainingX.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_wvec_trainingX')\n",
    "    tfidf_fvec_trainingX.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_fvec_trainingX')\n",
    "    tfidf_lsi_trainingX.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'tfidf_lsi_trainingX')\n",
    "\n",
    "    word2idx_trainingX.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'word2idx_trainingX')\n",
    "\n",
    "    pd.Series(_word2idx).to_hdf('training_data_wv_fs_no_stopwords.hdf', '_word2idx')\n",
    "    trainingY.to_hdf('training_data_wv_fs_no_stopwords.hdf', 'trainingY')\n",
    "\n",
    "# CPU times: user 3h 11min 14s, sys: 2min 32s, total: 3h 13min 47s\n",
    "# Wall time: 3h 12min 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1. ../data/TrainingData/2003b_TrainingData.json...\n",
      "Done parsing 6786 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 6786...\n",
      "Completed extracting data from ../data/TrainingData/2003b_TrainingData.json...\n",
      "\n",
      "Processing 2. ../data/TrainingData/2004b_TrainingData.json...\n",
      "Done parsing 7863 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6874\n",
      "Current batch in main thread: 14730\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6874\n",
      "Current batch in main thread: 14730\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 14649...\n",
      "Completed extracting data from ../data/TrainingData/2004b_TrainingData.json...\n",
      "\n",
      "Processing 3. ../data/TrainingData/2005a_TrainingData.json...\n",
      "Done parsing 6756 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5908\n",
      "Current batch in main thread: 12660\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5908\n",
      "Current batch in main thread: 12660\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 21405...\n",
      "Completed extracting data from ../data/TrainingData/2005a_TrainingData.json...\n",
      "\n",
      "Processing 4. ../data/TrainingData/1999b_TrainingData.json...\n",
      "Done parsing 1505 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 1316\n",
      "Current batch in main thread: 2820\n",
      "Computing wv features...\n",
      "Current batch in main thread: 1316\n",
      "Current batch in main thread: 2820\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 22910...\n",
      "Completed extracting data from ../data/TrainingData/1999b_TrainingData.json...\n",
      "\n",
      "Processing 5. ../data/TrainingData/2003a_TrainingData.json...\n",
      "Done parsing 9130 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7987\n",
      "Current batch in main thread: 17115\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7987\n",
      "Current batch in main thread: 17115\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 32040...\n",
      "Completed extracting data from ../data/TrainingData/2003a_TrainingData.json...\n",
      "\n",
      "Processing 6. ../data/TrainingData/2008b_TrainingData.json...\n",
      "Done parsing 10798 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 9443\n",
      "Current batch in main thread: 20235\n",
      "Computing wv features...\n",
      "Current batch in main thread: 9443\n",
      "Current batch in main thread: 20235\n",
      "Droping 4 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 42834...\n",
      "Completed extracting data from ../data/TrainingData/2008b_TrainingData.json...\n",
      "\n",
      "Processing 7. ../data/TrainingData/2009a_TrainingData.json...\n",
      "Done parsing 10803 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 9450\n",
      "Current batch in main thread: 20250\n",
      "Computing wv features...\n",
      "Current batch in main thread: 9450\n",
      "Current batch in main thread: 20250\n",
      "Droping 10 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 53627...\n",
      "Completed extracting data from ../data/TrainingData/2009a_TrainingData.json...\n",
      "\n",
      "Processing 8. ../data/TrainingData/1999a_TrainingData.json...\n",
      "Done parsing 969 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 847\n",
      "Current batch in main thread: 1815\n",
      "Computing wv features...\n",
      "Current batch in main thread: 847\n",
      "Current batch in main thread: 1815\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 54596...\n",
      "Completed extracting data from ../data/TrainingData/1999a_TrainingData.json...\n",
      "\n",
      "Processing 9. ../data/TrainingData/2002a_TrainingData.json...\n",
      "Done parsing 5573 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 4872\n",
      "Current batch in main thread: 10440\n",
      "Computing wv features...\n",
      "Current batch in main thread: 4872\n",
      "Current batch in main thread: 10440\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 60169...\n",
      "Completed extracting data from ../data/TrainingData/2002a_TrainingData.json...\n",
      "\n",
      "Processing 10. ../data/TrainingData/2012a_TrainingData.json...\n",
      "Done parsing 9236 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8078\n",
      "Current batch in main thread: 17310\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8078\n",
      "Current batch in main thread: 17310\n",
      "Droping 5 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 69400...\n",
      "Completed extracting data from ../data/TrainingData/2012a_TrainingData.json...\n",
      "\n",
      "Processing 11. ../data/TrainingData/2004a_TrainingData.json...\n",
      "Done parsing 7482 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6545\n",
      "Current batch in main thread: 14025\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6545\n",
      "Current batch in main thread: 14025\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 76882...\n",
      "Completed extracting data from ../data/TrainingData/2004a_TrainingData.json...\n",
      "\n",
      "Processing 12. ../data/TrainingData/2014a_TrainingData.json...\n",
      "Done parsing 9978 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8729\n",
      "Current batch in main thread: 18705\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8729\n",
      "Current batch in main thread: 18705\n",
      "Droping 15 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 86845...\n",
      "Completed extracting data from ../data/TrainingData/2014a_TrainingData.json...\n",
      "\n",
      "Processing 13. ../data/TrainingData/2008a_TrainingData.json...\n",
      "Done parsing 9032 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7903\n",
      "Current batch in main thread: 16935\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7903\n",
      "Current batch in main thread: 16935\n",
      "Droping 2 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 95875...\n",
      "Completed extracting data from ../data/TrainingData/2008a_TrainingData.json...\n",
      "\n",
      "Processing 14. ../data/TrainingData/2013a_TrainingData.json...\n",
      "Done parsing 8955 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7833\n",
      "Current batch in main thread: 16785\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7833\n",
      "Current batch in main thread: 16785\n",
      "Droping 3 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 104827...\n",
      "Completed extracting data from ../data/TrainingData/2013a_TrainingData.json...\n",
      "\n",
      "Processing 15. ../data/TrainingData/2000b_TrainingData.json...\n",
      "Done parsing 2178 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 1904\n",
      "Current batch in main thread: 4080\n",
      "Computing wv features...\n",
      "Current batch in main thread: 1904\n",
      "Current batch in main thread: 4080\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 107005...\n",
      "Completed extracting data from ../data/TrainingData/2000b_TrainingData.json...\n",
      "\n",
      "Processing 16. ../data/TrainingData/2001a_TrainingData.json...\n",
      "Done parsing 2825 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 2471\n",
      "Current batch in main thread: 5295\n",
      "Computing wv features...\n",
      "Current batch in main thread: 2471\n",
      "Current batch in main thread: 5295\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 109830...\n",
      "Completed extracting data from ../data/TrainingData/2001a_TrainingData.json...\n",
      "\n",
      "Processing 17. ../data/TrainingData/2005b_TrainingData.json...\n",
      "Done parsing 6713 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5873\n",
      "Current batch in main thread: 12585\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5873\n",
      "Current batch in main thread: 12585\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 116543...\n",
      "Completed extracting data from ../data/TrainingData/2005b_TrainingData.json...\n",
      "\n",
      "Processing 18. ../data/TrainingData/2001b_TrainingData.json...\n",
      "Done parsing 6478 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5663\n",
      "Current batch in main thread: 12135\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5663\n",
      "Current batch in main thread: 12135\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 123021...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed extracting data from ../data/TrainingData/2001b_TrainingData.json...\n",
      "\n",
      "Processing 19. ../data/TrainingData/2010b_TrainingData.json...\n",
      "Done parsing 9561 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8365\n",
      "Current batch in main thread: 17925\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8365\n",
      "Current batch in main thread: 17925\n",
      "Droping 4 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 132578...\n",
      "Completed extracting data from ../data/TrainingData/2010b_TrainingData.json...\n",
      "\n",
      "Processing 20. ../data/TrainingData/2002b_TrainingData.json...\n",
      "Done parsing 6180 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5404\n",
      "Current batch in main thread: 11580\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5404\n",
      "Current batch in main thread: 11580\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 138758...\n",
      "Completed extracting data from ../data/TrainingData/2002b_TrainingData.json...\n",
      "\n",
      "Processing 21. ../data/TrainingData/2006a_TrainingData.json...\n",
      "Done parsing 6784 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 145542...\n",
      "Completed extracting data from ../data/TrainingData/2006a_TrainingData.json...\n",
      "\n",
      "Processing 22. ../data/TrainingData/2009b_TrainingData.json...\n",
      "Done parsing 8126 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7105\n",
      "Current batch in main thread: 15225\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7105\n",
      "Current batch in main thread: 15225\n",
      "Droping 1 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 153667...\n",
      "Completed extracting data from ../data/TrainingData/2009b_TrainingData.json...\n",
      "\n",
      "Processing 23. ../data/TrainingData/2014b_TrainingData.json...\n",
      "Done parsing 9436 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8253\n",
      "Current batch in main thread: 17685\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8253\n",
      "Current batch in main thread: 17685\n",
      "Droping 12 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 163091...\n",
      "Completed extracting data from ../data/TrainingData/2014b_TrainingData.json...\n",
      "\n",
      "Processing 24. ../data/TrainingData/2006b_TrainingData.json...\n",
      "Done parsing 7425 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6496\n",
      "Current batch in main thread: 13920\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6496\n",
      "Current batch in main thread: 13920\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 170516...\n",
      "Completed extracting data from ../data/TrainingData/2006b_TrainingData.json...\n",
      "\n",
      "Processing 25. ../data/TrainingData/2007b_TrainingData.json...\n",
      "Done parsing 8010 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7007\n",
      "Current batch in main thread: 15015\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7007\n",
      "Current batch in main thread: 15015\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 178526...\n",
      "Completed extracting data from ../data/TrainingData/2007b_TrainingData.json...\n",
      "\n",
      "Processing 26. ../data/TrainingData/2000a_TrainingData.json...\n",
      "Done parsing 2044 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 1785\n",
      "Current batch in main thread: 3825\n",
      "Computing wv features...\n",
      "Current batch in main thread: 1785\n",
      "Current batch in main thread: 3825\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 180570...\n",
      "Completed extracting data from ../data/TrainingData/2000a_TrainingData.json...\n",
      "\n",
      "Processing 27. ../data/TrainingData/2011a_TrainingData.json...\n",
      "Done parsing 10358 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 9058\n",
      "Current batch in main thread: 19410\n",
      "Computing wv features...\n",
      "Current batch in main thread: 9058\n",
      "Current batch in main thread: 19410\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 190928...\n",
      "Completed extracting data from ../data/TrainingData/2011a_TrainingData.json...\n",
      "\n",
      "Processing 28. ../data/TrainingData/2012b_TrainingData.json...\n",
      "Done parsing 9148 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8001\n",
      "Current batch in main thread: 17145\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8001\n",
      "Current batch in main thread: 17145\n",
      "Droping 3 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 200073...\n",
      "Completed extracting data from ../data/TrainingData/2012b_TrainingData.json...\n",
      "\n",
      "Processing 29. ../data/TrainingData/2013b_TrainingData.json...\n",
      "Done parsing 10170 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8897\n",
      "Current batch in main thread: 19065\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8897\n",
      "Current batch in main thread: 19065\n",
      "Droping 8 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 210235...\n",
      "Completed extracting data from ../data/TrainingData/2013b_TrainingData.json...\n",
      "\n",
      "Processing 30. ../data/TrainingData/2007a_TrainingData.json...\n",
      "Done parsing 8112 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7098\n",
      "Current batch in main thread: 15210\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7098\n",
      "Current batch in main thread: 15210\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 218347...\n",
      "Completed extracting data from ../data/TrainingData/2007a_TrainingData.json...\n",
      "\n",
      "Processing 31. ../data/TrainingData/2010a_TrainingData.json...\n",
      "Done parsing 7855 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6867\n",
      "Current batch in main thread: 14715\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6867\n",
      "Current batch in main thread: 14715\n",
      "Droping 1 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 226201...\n",
      "Completed extracting data from ../data/TrainingData/2010a_TrainingData.json...\n",
      "\n",
      "Processing 32. ../data/TrainingData/2011b_TrainingData.json...\n",
      "Done parsing 10094 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8827\n",
      "Current batch in main thread: 18915\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8827\n",
      "Current batch in main thread: 18915\n",
      "Droping 9 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 236286...\n",
      "Completed extracting data from ../data/TrainingData/2011b_TrainingData.json...\n",
      "\n",
      "CPU times: user 9min 43s, sys: 42.1 s, total: 10min 25s\n",
      "Wall time: 41min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# start: 5395 MB\n",
    "generate_training_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1. ../data/TrainingData/2003b_TrainingData.json...\n",
      "Done parsing 6786 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 6786...\n",
      "Completed extracting data from ../data/TrainingData/2003b_TrainingData.json...\n",
      "\n",
      "Processing 2. ../data/TrainingData/2004b_TrainingData.json...\n",
      "Done parsing 7863 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6874\n",
      "Current batch in main thread: 14730\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6874\n",
      "Current batch in main thread: 14730\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 6874\n",
      "Current batch in main thread: 14730\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 6874\n",
      "Current batch in main thread: 14730\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 14649...\n",
      "Completed extracting data from ../data/TrainingData/2004b_TrainingData.json...\n",
      "\n",
      "Processing 3. ../data/TrainingData/2005a_TrainingData.json...\n",
      "Done parsing 6756 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5908\n",
      "Current batch in main thread: 12660\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5908\n",
      "Current batch in main thread: 12660\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 5908\n",
      "Current batch in main thread: 12660\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 5908\n",
      "Current batch in main thread: 12660\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 21405...\n",
      "Completed extracting data from ../data/TrainingData/2005a_TrainingData.json...\n",
      "\n",
      "Processing 4. ../data/TrainingData/1999b_TrainingData.json...\n",
      "Done parsing 1505 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 1316\n",
      "Current batch in main thread: 2820\n",
      "Computing wv features...\n",
      "Current batch in main thread: 1316\n",
      "Current batch in main thread: 2820\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 1316\n",
      "Current batch in main thread: 2820\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 1316\n",
      "Current batch in main thread: 2820\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 22910...\n",
      "Completed extracting data from ../data/TrainingData/1999b_TrainingData.json...\n",
      "\n",
      "Processing 5. ../data/TrainingData/2003a_TrainingData.json...\n",
      "Done parsing 9130 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7987\n",
      "Current batch in main thread: 17115\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7987\n",
      "Current batch in main thread: 17115\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 7987\n",
      "Current batch in main thread: 17115\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 7987\n",
      "Current batch in main thread: 17115\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 32040...\n",
      "Completed extracting data from ../data/TrainingData/2003a_TrainingData.json...\n",
      "\n",
      "Processing 6. ../data/TrainingData/2008b_TrainingData.json...\n",
      "Done parsing 10798 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 9443\n",
      "Current batch in main thread: 20235\n",
      "Computing wv features...\n",
      "Current batch in main thread: 9443\n",
      "Current batch in main thread: 20235\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 9443\n",
      "Current batch in main thread: 20235\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 9443\n",
      "Current batch in main thread: 20235\n",
      "Droping 4 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 42834...\n",
      "Completed extracting data from ../data/TrainingData/2008b_TrainingData.json...\n",
      "\n",
      "Processing 7. ../data/TrainingData/2009a_TrainingData.json...\n",
      "Done parsing 10803 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 9450\n",
      "Current batch in main thread: 20250\n",
      "Computing wv features...\n",
      "Current batch in main thread: 9450\n",
      "Current batch in main thread: 20250\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 9450\n",
      "Current batch in main thread: 20250\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 9450\n",
      "Current batch in main thread: 20250\n",
      "Droping 10 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 53627...\n",
      "Completed extracting data from ../data/TrainingData/2009a_TrainingData.json...\n",
      "\n",
      "Processing 8. ../data/TrainingData/1999a_TrainingData.json...\n",
      "Done parsing 969 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 847\n",
      "Current batch in main thread: 1815\n",
      "Computing wv features...\n",
      "Current batch in main thread: 847\n",
      "Current batch in main thread: 1815\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 847\n",
      "Current batch in main thread: 1815\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 847\n",
      "Current batch in main thread: 1815\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 54596...\n",
      "Completed extracting data from ../data/TrainingData/1999a_TrainingData.json...\n",
      "\n",
      "Processing 9. ../data/TrainingData/2002a_TrainingData.json...\n",
      "Done parsing 5573 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 4872\n",
      "Current batch in main thread: 10440\n",
      "Computing wv features...\n",
      "Current batch in main thread: 4872\n",
      "Current batch in main thread: 10440\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 4872\n",
      "Current batch in main thread: 10440\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 4872\n",
      "Current batch in main thread: 10440\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 60169...\n",
      "Completed extracting data from ../data/TrainingData/2002a_TrainingData.json...\n",
      "\n",
      "Processing 10. ../data/TrainingData/2012a_TrainingData.json...\n",
      "Done parsing 9236 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8078\n",
      "Current batch in main thread: 17310\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8078\n",
      "Current batch in main thread: 17310\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8078\n",
      "Current batch in main thread: 17310\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8078\n",
      "Current batch in main thread: 17310\n",
      "Droping 5 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 69400...\n",
      "Completed extracting data from ../data/TrainingData/2012a_TrainingData.json...\n",
      "\n",
      "Processing 11. ../data/TrainingData/2004a_TrainingData.json...\n",
      "Done parsing 7482 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6545\n",
      "Current batch in main thread: 14025\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6545\n",
      "Current batch in main thread: 14025\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 6545\n",
      "Current batch in main thread: 14025\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 6545\n",
      "Current batch in main thread: 14025\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 76882...\n",
      "Completed extracting data from ../data/TrainingData/2004a_TrainingData.json...\n",
      "\n",
      "Processing 12. ../data/TrainingData/2014a_TrainingData.json...\n",
      "Done parsing 9978 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8729\n",
      "Current batch in main thread: 18705\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8729\n",
      "Current batch in main thread: 18705\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8729\n",
      "Current batch in main thread: 18705\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8729\n",
      "Current batch in main thread: 18705\n",
      "Droping 15 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 86845...\n",
      "Completed extracting data from ../data/TrainingData/2014a_TrainingData.json...\n",
      "\n",
      "Processing 13. ../data/TrainingData/2008a_TrainingData.json...\n",
      "Done parsing 9032 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7903\n",
      "Current batch in main thread: 16935\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7903\n",
      "Current batch in main thread: 16935\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 7903\n",
      "Current batch in main thread: 16935\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 7903\n",
      "Current batch in main thread: 16935\n",
      "Droping 2 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 95875...\n",
      "Completed extracting data from ../data/TrainingData/2008a_TrainingData.json...\n",
      "\n",
      "Processing 14. ../data/TrainingData/2013a_TrainingData.json...\n",
      "Done parsing 8955 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7833\n",
      "Current batch in main thread: 16785\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7833\n",
      "Current batch in main thread: 16785\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 7833\n",
      "Current batch in main thread: 16785\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 7833\n",
      "Current batch in main thread: 16785\n",
      "Droping 3 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 104827...\n",
      "Completed extracting data from ../data/TrainingData/2013a_TrainingData.json...\n",
      "\n",
      "Processing 15. ../data/TrainingData/2000b_TrainingData.json...\n",
      "Done parsing 2178 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 1904\n",
      "Current batch in main thread: 4080\n",
      "Computing wv features...\n",
      "Current batch in main thread: 1904\n",
      "Current batch in main thread: 4080\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 1904\n",
      "Current batch in main thread: 4080\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 1904\n",
      "Current batch in main thread: 4080\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 107005...\n",
      "Completed extracting data from ../data/TrainingData/2000b_TrainingData.json...\n",
      "\n",
      "Processing 16. ../data/TrainingData/2001a_TrainingData.json...\n",
      "Done parsing 2825 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 2471\n",
      "Current batch in main thread: 5295\n",
      "Computing wv features...\n",
      "Current batch in main thread: 2471\n",
      "Current batch in main thread: 5295\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 2471\n",
      "Current batch in main thread: 5295\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 2471\n",
      "Current batch in main thread: 5295\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 109830...\n",
      "Completed extracting data from ../data/TrainingData/2001a_TrainingData.json...\n",
      "\n",
      "Processing 17. ../data/TrainingData/2005b_TrainingData.json...\n",
      "Done parsing 6713 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5873\n",
      "Current batch in main thread: 12585\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5873\n",
      "Current batch in main thread: 12585\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 5873\n",
      "Current batch in main thread: 12585\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 5873\n",
      "Current batch in main thread: 12585\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 116543...\n",
      "Completed extracting data from ../data/TrainingData/2005b_TrainingData.json...\n",
      "\n",
      "Processing 18. ../data/TrainingData/2001b_TrainingData.json...\n",
      "Done parsing 6478 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5663\n",
      "Current batch in main thread: 12135\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5663\n",
      "Current batch in main thread: 12135\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 5663\n",
      "Current batch in main thread: 12135\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 5663\n",
      "Current batch in main thread: 12135\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 123021...\n",
      "Completed extracting data from ../data/TrainingData/2001b_TrainingData.json...\n",
      "\n",
      "Processing 19. ../data/TrainingData/2010b_TrainingData.json...\n",
      "Done parsing 9561 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8365\n",
      "Current batch in main thread: 17925\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8365\n",
      "Current batch in main thread: 17925\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8365\n",
      "Current batch in main thread: 17925\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8365\n",
      "Current batch in main thread: 17925\n",
      "Droping 4 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 132578...\n",
      "Completed extracting data from ../data/TrainingData/2010b_TrainingData.json...\n",
      "\n",
      "Processing 20. ../data/TrainingData/2002b_TrainingData.json...\n",
      "Done parsing 6180 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5404\n",
      "Current batch in main thread: 11580\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5404\n",
      "Current batch in main thread: 11580\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 5404\n",
      "Current batch in main thread: 11580\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 5404\n",
      "Current batch in main thread: 11580\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 138758...\n",
      "Completed extracting data from ../data/TrainingData/2002b_TrainingData.json...\n",
      "\n",
      "Processing 21. ../data/TrainingData/2006a_TrainingData.json...\n",
      "Done parsing 6784 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing wv features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 5936\n",
      "Current batch in main thread: 12720\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 145542...\n",
      "Completed extracting data from ../data/TrainingData/2006a_TrainingData.json...\n",
      "\n",
      "Processing 22. ../data/TrainingData/2009b_TrainingData.json...\n",
      "Done parsing 8126 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7105\n",
      "Current batch in main thread: 15225\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7105\n",
      "Current batch in main thread: 15225\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 7105\n",
      "Current batch in main thread: 15225\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 7105\n",
      "Current batch in main thread: 15225\n",
      "Droping 1 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 153667...\n",
      "Completed extracting data from ../data/TrainingData/2009b_TrainingData.json...\n",
      "\n",
      "Processing 23. ../data/TrainingData/2014b_TrainingData.json...\n",
      "Done parsing 9436 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8253\n",
      "Current batch in main thread: 17685\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8253\n",
      "Current batch in main thread: 17685\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8253\n",
      "Current batch in main thread: 17685\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8253\n",
      "Current batch in main thread: 17685\n",
      "Droping 12 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 163091...\n",
      "Completed extracting data from ../data/TrainingData/2014b_TrainingData.json...\n",
      "\n",
      "Processing 24. ../data/TrainingData/2006b_TrainingData.json...\n",
      "Done parsing 7425 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6496\n",
      "Current batch in main thread: 13920\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6496\n",
      "Current batch in main thread: 13920\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 6496\n",
      "Current batch in main thread: 13920\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 6496\n",
      "Current batch in main thread: 13920\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 170516...\n",
      "Completed extracting data from ../data/TrainingData/2006b_TrainingData.json...\n",
      "\n",
      "Processing 25. ../data/TrainingData/2007b_TrainingData.json...\n",
      "Done parsing 8010 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7007\n",
      "Current batch in main thread: 15015\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7007\n",
      "Current batch in main thread: 15015\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 7007\n",
      "Current batch in main thread: 15015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 7007\n",
      "Current batch in main thread: 15015\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 178526...\n",
      "Completed extracting data from ../data/TrainingData/2007b_TrainingData.json...\n",
      "\n",
      "Processing 26. ../data/TrainingData/2000a_TrainingData.json...\n",
      "Done parsing 2044 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 1785\n",
      "Current batch in main thread: 3825\n",
      "Computing wv features...\n",
      "Current batch in main thread: 1785\n",
      "Current batch in main thread: 3825\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 1785\n",
      "Current batch in main thread: 3825\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 1785\n",
      "Current batch in main thread: 3825\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 180570...\n",
      "Completed extracting data from ../data/TrainingData/2000a_TrainingData.json...\n",
      "\n",
      "Processing 27. ../data/TrainingData/2011a_TrainingData.json...\n",
      "Done parsing 10358 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 9058\n",
      "Current batch in main thread: 19410\n",
      "Computing wv features...\n",
      "Current batch in main thread: 9058\n",
      "Current batch in main thread: 19410\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 9058\n",
      "Current batch in main thread: 19410\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 9058\n",
      "Current batch in main thread: 19410\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 190928...\n",
      "Completed extracting data from ../data/TrainingData/2011a_TrainingData.json...\n",
      "\n",
      "Processing 28. ../data/TrainingData/2012b_TrainingData.json...\n",
      "Done parsing 9148 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8001\n",
      "Current batch in main thread: 17145\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8001\n",
      "Current batch in main thread: 17145\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8001\n",
      "Current batch in main thread: 17145\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8001\n",
      "Current batch in main thread: 17145\n",
      "Droping 3 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 200073...\n",
      "Completed extracting data from ../data/TrainingData/2012b_TrainingData.json...\n",
      "\n",
      "Processing 29. ../data/TrainingData/2013b_TrainingData.json...\n",
      "Done parsing 10170 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8897\n",
      "Current batch in main thread: 19065\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8897\n",
      "Current batch in main thread: 19065\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8897\n",
      "Current batch in main thread: 19065\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8897\n",
      "Current batch in main thread: 19065\n",
      "Droping 8 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 210235...\n",
      "Completed extracting data from ../data/TrainingData/2013b_TrainingData.json...\n",
      "\n",
      "Processing 30. ../data/TrainingData/2007a_TrainingData.json...\n",
      "Done parsing 8112 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 7098\n",
      "Current batch in main thread: 15210\n",
      "Computing wv features...\n",
      "Current batch in main thread: 7098\n",
      "Current batch in main thread: 15210\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 7098\n",
      "Current batch in main thread: 15210\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 7098\n",
      "Current batch in main thread: 15210\n",
      "Droping 0 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 218347...\n",
      "Completed extracting data from ../data/TrainingData/2007a_TrainingData.json...\n",
      "\n",
      "Processing 31. ../data/TrainingData/2010a_TrainingData.json...\n",
      "Done parsing 7855 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 6867\n",
      "Current batch in main thread: 14715\n",
      "Computing wv features...\n",
      "Current batch in main thread: 6867\n",
      "Current batch in main thread: 14715\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 6867\n",
      "Current batch in main thread: 14715\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 6867\n",
      "Current batch in main thread: 14715\n",
      "Droping 1 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 226201...\n",
      "Completed extracting data from ../data/TrainingData/2010a_TrainingData.json...\n",
      "\n",
      "Processing 32. ../data/TrainingData/2011b_TrainingData.json...\n",
      "Done parsing 10094 docs with valid topics...\n",
      "Computing fs features...\n",
      "Current batch in main thread: 8827\n",
      "Current batch in main thread: 18915\n",
      "Computing wv features...\n",
      "Current batch in main thread: 8827\n",
      "Current batch in main thread: 18915\n",
      "Computing tfidf fs features...\n",
      "Current batch in main thread: 8827\n",
      "Current batch in main thread: 18915\n",
      "Computing tfidf wv features...\n",
      "Current batch in main thread: 8827\n",
      "Current batch in main thread: 18915\n",
      "Droping 9 NaN samples...\n",
      "Mapping word indices...\n",
      "Current training size is 236286...\n",
      "Completed extracting data from ../data/TrainingData/2011b_TrainingData.json...\n",
      "\n",
      "CPU times: user 9min 56s, sys: 53.3 s, total: 10min 49s\n",
      "Wall time: 38min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# start: 5395 MB\n",
    "generate_training_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingY = pd.read_hdf('training_data_wv_fs_no_stopwords', 'trainingY_fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tty = trainingY.copy()\n",
    "freqs = tty.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ukcrime        20513.0\n",
       "iraq           20344.0\n",
       "religion       16951.0\n",
       "immigration    15296.0\n",
       "london         15129.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      146.000000\n",
       "mean      2254.082192\n",
       "std       3955.853032\n",
       "min          3.000000\n",
       "25%        212.750000\n",
       "50%        630.000000\n",
       "75%       2052.750000\n",
       "max      20513.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = resample_data(tty, upsample=100, maxsample=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humanrights                    2203.0\n",
       "military                       2060.0\n",
       "iraq                           1986.0\n",
       "ukcrime                        1962.0\n",
       "uksecurity                     1682.0\n",
       "afghanistan                    1663.0\n",
       "syria                          1595.0\n",
       "immigration                    1546.0\n",
       "police                         1506.0\n",
       "london                         1435.0\n",
       "unitednations                  1331.0\n",
       "israel                         1311.0\n",
       "religion                       1219.0\n",
       "protest                        1198.0\n",
       "terrorism                      1176.0\n",
       "france                         1133.0\n",
       "alqaida                        1131.0\n",
       "defence                        1056.0\n",
       "arabandmiddleeastprotests      1056.0\n",
       "criminaljustice                 980.0\n",
       "russia                          902.0\n",
       "refugees                        897.0\n",
       "aid                             893.0\n",
       "india                           892.0\n",
       "hacking                         876.0\n",
       "surveillance                    869.0\n",
       "naturaldisasters                867.0\n",
       "somalia                         850.0\n",
       "libya                           793.0\n",
       "warcrimes                       737.0\n",
       "                                ...  \n",
       "internallydisplacedpeople       127.0\n",
       "europeanarrestwarrant           124.0\n",
       "algerianhostagecrisis           121.0\n",
       "cameroon                        119.0\n",
       "cobra                           114.0\n",
       "encryption                      114.0\n",
       "genevaconventions               114.0\n",
       "internationalcourtofjustice     111.0\n",
       "darknet                         104.0\n",
       "southchinasea                   104.0\n",
       "traincrashes                    104.0\n",
       "australianguncontrol            103.0\n",
       "deflation                       102.0\n",
       "debate                          102.0\n",
       "hashtags                        101.0\n",
       "values                          100.0\n",
       "activism                          NaN\n",
       "bastilledaytruckattack            NaN\n",
       "berlinchristmasmarketattack       NaN\n",
       "brusselsattacks                   NaN\n",
       "charliehebdoattack                NaN\n",
       "francetrainattack                 NaN\n",
       "munichshooting                    NaN\n",
       "orlandoterrorattack               NaN\n",
       "parisattacks                      NaN\n",
       "peaceandreconciliation            NaN\n",
       "sanbernardinoshooting             NaN\n",
       "tunisiaattack2015                 NaN\n",
       "turkeycoupattempt                 NaN\n",
       "zikavirus                         NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tty.ix[indices].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tty['year'] = tty.index.map(lambda x: re.findall('^([0-9]{4}).*', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_distribution = tty[freqs.head(len(freqs) - 14).index.union(['year'])].groupby('year').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ukcrime</th>\n",
       "      <th>iraq</th>\n",
       "      <th>religion</th>\n",
       "      <th>immigration</th>\n",
       "      <th>london</th>\n",
       "      <th>economy</th>\n",
       "      <th>israel</th>\n",
       "      <th>transport</th>\n",
       "      <th>afghanistan</th>\n",
       "      <th>france</th>\n",
       "      <th>...</th>\n",
       "      <th>internallydisplacedpeople</th>\n",
       "      <th>sydneysiege</th>\n",
       "      <th>cobra</th>\n",
       "      <th>encryption</th>\n",
       "      <th>southchinasea</th>\n",
       "      <th>darknet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>values</th>\n",
       "      <th>debate</th>\n",
       "      <th>australianguncontrol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>246</td>\n",
       "      <td>59</td>\n",
       "      <td>172</td>\n",
       "      <td>42</td>\n",
       "      <td>177</td>\n",
       "      <td>18</td>\n",
       "      <td>91</td>\n",
       "      <td>182</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>335</td>\n",
       "      <td>67</td>\n",
       "      <td>316</td>\n",
       "      <td>263</td>\n",
       "      <td>374</td>\n",
       "      <td>37</td>\n",
       "      <td>595</td>\n",
       "      <td>538</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>481</td>\n",
       "      <td>122</td>\n",
       "      <td>329</td>\n",
       "      <td>738</td>\n",
       "      <td>347</td>\n",
       "      <td>326</td>\n",
       "      <td>868</td>\n",
       "      <td>770</td>\n",
       "      <td>2620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>1008</td>\n",
       "      <td>1168</td>\n",
       "      <td>594</td>\n",
       "      <td>1309</td>\n",
       "      <td>391</td>\n",
       "      <td>678</td>\n",
       "      <td>1243</td>\n",
       "      <td>914</td>\n",
       "      <td>687</td>\n",
       "      <td>408.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>1346</td>\n",
       "      <td>6098</td>\n",
       "      <td>593</td>\n",
       "      <td>1677</td>\n",
       "      <td>441</td>\n",
       "      <td>902</td>\n",
       "      <td>797</td>\n",
       "      <td>716</td>\n",
       "      <td>167</td>\n",
       "      <td>592.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>1650</td>\n",
       "      <td>3066</td>\n",
       "      <td>777</td>\n",
       "      <td>2472</td>\n",
       "      <td>275</td>\n",
       "      <td>1206</td>\n",
       "      <td>882</td>\n",
       "      <td>771</td>\n",
       "      <td>199</td>\n",
       "      <td>659.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1495</td>\n",
       "      <td>1769</td>\n",
       "      <td>1072</td>\n",
       "      <td>1815</td>\n",
       "      <td>252</td>\n",
       "      <td>960</td>\n",
       "      <td>497</td>\n",
       "      <td>499</td>\n",
       "      <td>156</td>\n",
       "      <td>626.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1731</td>\n",
       "      <td>1576</td>\n",
       "      <td>1211</td>\n",
       "      <td>1709</td>\n",
       "      <td>345</td>\n",
       "      <td>837</td>\n",
       "      <td>1133</td>\n",
       "      <td>767</td>\n",
       "      <td>374</td>\n",
       "      <td>541.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2024</td>\n",
       "      <td>2039</td>\n",
       "      <td>1175</td>\n",
       "      <td>2034</td>\n",
       "      <td>448</td>\n",
       "      <td>967</td>\n",
       "      <td>849</td>\n",
       "      <td>1049</td>\n",
       "      <td>452</td>\n",
       "      <td>805.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2249</td>\n",
       "      <td>1016</td>\n",
       "      <td>1560</td>\n",
       "      <td>524</td>\n",
       "      <td>1299</td>\n",
       "      <td>1918</td>\n",
       "      <td>1049</td>\n",
       "      <td>1234</td>\n",
       "      <td>869</td>\n",
       "      <td>841.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>1631</td>\n",
       "      <td>841</td>\n",
       "      <td>1931</td>\n",
       "      <td>399</td>\n",
       "      <td>1781</td>\n",
       "      <td>1497</td>\n",
       "      <td>1271</td>\n",
       "      <td>890</td>\n",
       "      <td>1184</td>\n",
       "      <td>773.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1404</td>\n",
       "      <td>740</td>\n",
       "      <td>1771</td>\n",
       "      <td>428</td>\n",
       "      <td>1401</td>\n",
       "      <td>1114</td>\n",
       "      <td>868</td>\n",
       "      <td>677</td>\n",
       "      <td>1123</td>\n",
       "      <td>744.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>1667</td>\n",
       "      <td>391</td>\n",
       "      <td>1524</td>\n",
       "      <td>358</td>\n",
       "      <td>1945</td>\n",
       "      <td>1143</td>\n",
       "      <td>718</td>\n",
       "      <td>604</td>\n",
       "      <td>728</td>\n",
       "      <td>985.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>1306</td>\n",
       "      <td>188</td>\n",
       "      <td>1362</td>\n",
       "      <td>399</td>\n",
       "      <td>2189</td>\n",
       "      <td>1064</td>\n",
       "      <td>686</td>\n",
       "      <td>619</td>\n",
       "      <td>573</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1203</td>\n",
       "      <td>268</td>\n",
       "      <td>1291</td>\n",
       "      <td>569</td>\n",
       "      <td>1740</td>\n",
       "      <td>1165</td>\n",
       "      <td>504</td>\n",
       "      <td>535</td>\n",
       "      <td>458</td>\n",
       "      <td>891.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>737</td>\n",
       "      <td>936</td>\n",
       "      <td>1273</td>\n",
       "      <td>560</td>\n",
       "      <td>1724</td>\n",
       "      <td>857</td>\n",
       "      <td>740</td>\n",
       "      <td>533</td>\n",
       "      <td>399</td>\n",
       "      <td>805.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ukcrime  iraq  religion  immigration  london  economy  israel  \\\n",
       "year                                                                  \n",
       "1999      246    59       172           42     177       18      91   \n",
       "2000      335    67       316          263     374       37     595   \n",
       "2001      481   122       329          738     347      326     868   \n",
       "2002     1008  1168       594         1309     391      678    1243   \n",
       "2003     1346  6098       593         1677     441      902     797   \n",
       "2004     1650  3066       777         2472     275     1206     882   \n",
       "2005     1495  1769      1072         1815     252      960     497   \n",
       "2006     1731  1576      1211         1709     345      837    1133   \n",
       "2007     2024  2039      1175         2034     448      967     849   \n",
       "2008     2249  1016      1560          524    1299     1918    1049   \n",
       "2009     1631   841      1931          399    1781     1497    1271   \n",
       "2010     1404   740      1771          428    1401     1114     868   \n",
       "2011     1667   391      1524          358    1945     1143     718   \n",
       "2012     1306   188      1362          399    2189     1064     686   \n",
       "2013     1203   268      1291          569    1740     1165     504   \n",
       "2014      737   936      1273          560    1724      857     740   \n",
       "\n",
       "      transport  afghanistan  france          ...           \\\n",
       "year                                          ...            \n",
       "1999        182           35     NaN          ...            \n",
       "2000        538            8     1.0          ...            \n",
       "2001        770         2620     NaN          ...            \n",
       "2002        914          687   408.0          ...            \n",
       "2003        716          167   592.0          ...            \n",
       "2004        771          199   659.0          ...            \n",
       "2005        499          156   626.0          ...            \n",
       "2006        767          374   541.0          ...            \n",
       "2007       1049          452   805.0          ...            \n",
       "2008       1234          869   841.0          ...            \n",
       "2009        890         1184   773.0          ...            \n",
       "2010        677         1123   744.0          ...            \n",
       "2011        604          728   985.0          ...            \n",
       "2012        619          573  1085.0          ...            \n",
       "2013        535          458   891.0          ...            \n",
       "2014        533          399   805.0          ...            \n",
       "\n",
       "      internallydisplacedpeople  sydneysiege  cobra  encryption  \\\n",
       "year                                                              \n",
       "1999                        3.0          NaN    NaN         NaN   \n",
       "2000                        NaN          NaN    NaN         NaN   \n",
       "2001                        NaN          NaN    NaN         NaN   \n",
       "2002                        NaN          NaN    2.0         NaN   \n",
       "2003                        NaN          NaN    NaN         NaN   \n",
       "2004                        NaN          NaN    NaN         NaN   \n",
       "2005                        1.0          NaN    NaN         NaN   \n",
       "2006                        NaN          NaN    NaN         NaN   \n",
       "2007                        NaN          NaN    6.0         NaN   \n",
       "2008                        1.0          NaN    NaN         NaN   \n",
       "2009                        1.0          NaN    NaN         NaN   \n",
       "2010                        1.0          NaN    1.0         NaN   \n",
       "2011                        3.0          NaN    3.0         NaN   \n",
       "2012                        1.0          NaN    1.0         NaN   \n",
       "2013                       18.0          NaN    8.0         NaN   \n",
       "2014                       15.0         42.0   14.0        34.0   \n",
       "\n",
       "      southchinasea  darknet  hashtags  values  debate  australianguncontrol  \n",
       "year                                                                          \n",
       "1999            4.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2000            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2001            4.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2002            NaN      NaN       NaN     NaN     NaN                   NaN  \n",
       "2003            NaN      NaN       NaN     NaN     NaN                   NaN  \n",
       "2004            4.0      1.0       NaN     NaN     NaN                   NaN  \n",
       "2005            1.0      1.0       NaN     NaN     NaN                   NaN  \n",
       "2006            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2007            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2008            NaN      NaN       NaN     NaN     NaN                   NaN  \n",
       "2009            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2010            1.0      NaN       5.0     NaN     NaN                   NaN  \n",
       "2011            3.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2012            8.0      NaN       6.0    18.0     NaN                   NaN  \n",
       "2013            2.0      8.0       7.0     NaN     NaN                   2.0  \n",
       "2014            3.0     14.0       2.0     NaN    18.0                   1.0  \n",
       "\n",
       "[16 rows x 146 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "time_distribution #.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_topic_articles = tty[freqs.index].sum(axis=1)\n",
    "unique_topic_articles = unique_topic_articles[unique_topic_articles == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ukcrime</th>\n",
       "      <th>iraq</th>\n",
       "      <th>religion</th>\n",
       "      <th>immigration</th>\n",
       "      <th>london</th>\n",
       "      <th>economy</th>\n",
       "      <th>israel</th>\n",
       "      <th>transport</th>\n",
       "      <th>afghanistan</th>\n",
       "      <th>france</th>\n",
       "      <th>...</th>\n",
       "      <th>internallydisplacedpeople</th>\n",
       "      <th>sydneysiege</th>\n",
       "      <th>cobra</th>\n",
       "      <th>encryption</th>\n",
       "      <th>southchinasea</th>\n",
       "      <th>darknet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>values</th>\n",
       "      <th>debate</th>\n",
       "      <th>australianguncontrol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>214</td>\n",
       "      <td>51</td>\n",
       "      <td>169</td>\n",
       "      <td>42</td>\n",
       "      <td>175</td>\n",
       "      <td>18</td>\n",
       "      <td>88</td>\n",
       "      <td>178</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>310</td>\n",
       "      <td>57</td>\n",
       "      <td>293</td>\n",
       "      <td>60</td>\n",
       "      <td>347</td>\n",
       "      <td>37</td>\n",
       "      <td>509</td>\n",
       "      <td>511</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>383</td>\n",
       "      <td>91</td>\n",
       "      <td>228</td>\n",
       "      <td>338</td>\n",
       "      <td>197</td>\n",
       "      <td>301</td>\n",
       "      <td>728</td>\n",
       "      <td>660</td>\n",
       "      <td>1436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>621</td>\n",
       "      <td>869</td>\n",
       "      <td>467</td>\n",
       "      <td>696</td>\n",
       "      <td>264</td>\n",
       "      <td>633</td>\n",
       "      <td>1093</td>\n",
       "      <td>784</td>\n",
       "      <td>391</td>\n",
       "      <td>282.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>868</td>\n",
       "      <td>4897</td>\n",
       "      <td>469</td>\n",
       "      <td>674</td>\n",
       "      <td>279</td>\n",
       "      <td>827</td>\n",
       "      <td>699</td>\n",
       "      <td>573</td>\n",
       "      <td>109</td>\n",
       "      <td>452.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>1034</td>\n",
       "      <td>2420</td>\n",
       "      <td>578</td>\n",
       "      <td>1534</td>\n",
       "      <td>171</td>\n",
       "      <td>1087</td>\n",
       "      <td>740</td>\n",
       "      <td>587</td>\n",
       "      <td>151</td>\n",
       "      <td>481.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1079</td>\n",
       "      <td>1342</td>\n",
       "      <td>840</td>\n",
       "      <td>1006</td>\n",
       "      <td>166</td>\n",
       "      <td>894</td>\n",
       "      <td>442</td>\n",
       "      <td>406</td>\n",
       "      <td>103</td>\n",
       "      <td>552.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1111</td>\n",
       "      <td>1165</td>\n",
       "      <td>933</td>\n",
       "      <td>858</td>\n",
       "      <td>255</td>\n",
       "      <td>740</td>\n",
       "      <td>528</td>\n",
       "      <td>657</td>\n",
       "      <td>148</td>\n",
       "      <td>448.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1259</td>\n",
       "      <td>1332</td>\n",
       "      <td>912</td>\n",
       "      <td>1079</td>\n",
       "      <td>285</td>\n",
       "      <td>865</td>\n",
       "      <td>652</td>\n",
       "      <td>859</td>\n",
       "      <td>185</td>\n",
       "      <td>672.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1407</td>\n",
       "      <td>577</td>\n",
       "      <td>1214</td>\n",
       "      <td>318</td>\n",
       "      <td>671</td>\n",
       "      <td>1790</td>\n",
       "      <td>739</td>\n",
       "      <td>910</td>\n",
       "      <td>232</td>\n",
       "      <td>663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>935</td>\n",
       "      <td>478</td>\n",
       "      <td>1387</td>\n",
       "      <td>209</td>\n",
       "      <td>1032</td>\n",
       "      <td>1365</td>\n",
       "      <td>791</td>\n",
       "      <td>551</td>\n",
       "      <td>284</td>\n",
       "      <td>505.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>763</td>\n",
       "      <td>378</td>\n",
       "      <td>1302</td>\n",
       "      <td>250</td>\n",
       "      <td>810</td>\n",
       "      <td>1032</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>294</td>\n",
       "      <td>501.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>680</td>\n",
       "      <td>157</td>\n",
       "      <td>1002</td>\n",
       "      <td>196</td>\n",
       "      <td>810</td>\n",
       "      <td>1018</td>\n",
       "      <td>333</td>\n",
       "      <td>383</td>\n",
       "      <td>194</td>\n",
       "      <td>598.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>618</td>\n",
       "      <td>53</td>\n",
       "      <td>983</td>\n",
       "      <td>204</td>\n",
       "      <td>1204</td>\n",
       "      <td>966</td>\n",
       "      <td>358</td>\n",
       "      <td>397</td>\n",
       "      <td>144</td>\n",
       "      <td>705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>574</td>\n",
       "      <td>86</td>\n",
       "      <td>921</td>\n",
       "      <td>348</td>\n",
       "      <td>931</td>\n",
       "      <td>1048</td>\n",
       "      <td>263</td>\n",
       "      <td>324</td>\n",
       "      <td>123</td>\n",
       "      <td>468.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>370</td>\n",
       "      <td>154</td>\n",
       "      <td>834</td>\n",
       "      <td>366</td>\n",
       "      <td>1076</td>\n",
       "      <td>739</td>\n",
       "      <td>453</td>\n",
       "      <td>348</td>\n",
       "      <td>139</td>\n",
       "      <td>480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ukcrime  iraq  religion  immigration  london  economy  israel  \\\n",
       "year                                                                  \n",
       "1999      214    51       169           42     175       18      88   \n",
       "2000      310    57       293           60     347       37     509   \n",
       "2001      383    91       228          338     197      301     728   \n",
       "2002      621   869       467          696     264      633    1093   \n",
       "2003      868  4897       469          674     279      827     699   \n",
       "2004     1034  2420       578         1534     171     1087     740   \n",
       "2005     1079  1342       840         1006     166      894     442   \n",
       "2006     1111  1165       933          858     255      740     528   \n",
       "2007     1259  1332       912         1079     285      865     652   \n",
       "2008     1407   577      1214          318     671     1790     739   \n",
       "2009      935   478      1387          209    1032     1365     791   \n",
       "2010      763   378      1302          250     810     1032     448   \n",
       "2011      680   157      1002          196     810     1018     333   \n",
       "2012      618    53       983          204    1204      966     358   \n",
       "2013      574    86       921          348     931     1048     263   \n",
       "2014      370   154       834          366    1076      739     453   \n",
       "\n",
       "      transport  afghanistan  france          ...           \\\n",
       "year                                          ...            \n",
       "1999        178           28     NaN          ...            \n",
       "2000        511            4     1.0          ...            \n",
       "2001        660         1436     NaN          ...            \n",
       "2002        784          391   282.0          ...            \n",
       "2003        573          109   452.0          ...            \n",
       "2004        587          151   481.0          ...            \n",
       "2005        406          103   552.0          ...            \n",
       "2006        657          148   448.0          ...            \n",
       "2007        859          185   672.0          ...            \n",
       "2008        910          232   663.0          ...            \n",
       "2009        551          284   505.0          ...            \n",
       "2010        448          294   501.0          ...            \n",
       "2011        383          194   598.0          ...            \n",
       "2012        397          144   705.0          ...            \n",
       "2013        324          123   468.0          ...            \n",
       "2014        348          139   480.0          ...            \n",
       "\n",
       "      internallydisplacedpeople  sydneysiege  cobra  encryption  \\\n",
       "year                                                              \n",
       "1999                        2.0          NaN    NaN         NaN   \n",
       "2000                        NaN          NaN    NaN         NaN   \n",
       "2001                        NaN          NaN    NaN         NaN   \n",
       "2002                        NaN          NaN    NaN         NaN   \n",
       "2003                        NaN          NaN    NaN         NaN   \n",
       "2004                        NaN          NaN    NaN         NaN   \n",
       "2005                        NaN          NaN    NaN         NaN   \n",
       "2006                        NaN          NaN    NaN         NaN   \n",
       "2007                        NaN          NaN    1.0         NaN   \n",
       "2008                        1.0          NaN    NaN         NaN   \n",
       "2009                        1.0          NaN    NaN         NaN   \n",
       "2010                        NaN          NaN    NaN         NaN   \n",
       "2011                        NaN          NaN    NaN         NaN   \n",
       "2012                        NaN          NaN    NaN         NaN   \n",
       "2013                        NaN          NaN    NaN         NaN   \n",
       "2014                        NaN         25.0    6.0         8.0   \n",
       "\n",
       "      southchinasea  darknet  hashtags  values  debate  australianguncontrol  \n",
       "year                                                                          \n",
       "1999            4.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2000            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2001            4.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2002            NaN      NaN       NaN     NaN     NaN                   NaN  \n",
       "2003            NaN      NaN       NaN     NaN     NaN                   NaN  \n",
       "2004            4.0      1.0       NaN     NaN     NaN                   NaN  \n",
       "2005            1.0      1.0       NaN     NaN     NaN                   NaN  \n",
       "2006            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2007            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2008            NaN      NaN       NaN     NaN     NaN                   NaN  \n",
       "2009            1.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2010            1.0      NaN       2.0     NaN     NaN                   NaN  \n",
       "2011            3.0      NaN       NaN     NaN     NaN                   NaN  \n",
       "2012            5.0      NaN       6.0    18.0     NaN                   NaN  \n",
       "2013            NaN      NaN       4.0     NaN     NaN                   1.0  \n",
       "2014            1.0     11.0       1.0     NaN    16.0                   NaN  \n",
       "\n",
       "[16 rows x 146 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tty.ix[unique_topic_articles.index][freqs.head(len(freqs) - 14).index.union(['year'])].groupby('year').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = parse_training_data_with_valid_topics('../data/TrainingData/1999b_TrainingData.json', topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014b_TrainingData_52907   NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df[df.columns.intersection(topics)].sum(axis=1)\n",
    "x[x.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_word2idx = pd.read_hdf('training_data_wv_fs_no_stopwords.hdf', '_word2idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146425"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word2idx['zika']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_tokens = ['hello', 'world', 'united', 'nations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topword2idx = {}\n",
    "for ix, token in enumerate(top_tokens):\n",
    "    topword2idx[_word2idx[token]] = ix + 1  # Start with index 1 since 0 is a special symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topword2idx[_word2idx['united']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
